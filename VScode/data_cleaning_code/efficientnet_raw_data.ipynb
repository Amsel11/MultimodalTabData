{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522a8135",
   "metadata": {},
   "source": [
    "1. same thing for the other 4 data sets\n",
    "- used the updated version Jack sent (w/ demographics)\n",
    "- saved in their respective folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d014242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e71feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Cleaning efficientnet...\n",
      "âœ… Cleaned data saved (grouped by diagnosis): /Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_cleaned_master.csv\n",
      "âœ… Dropped info saved: /Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_dropped_info.csv\n",
      "\n",
      "ðŸ”„ Cleaning imagenet_resnet50...\n",
      "âœ… Cleaned data saved (grouped by diagnosis): /Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_cleaned_master.csv\n",
      "âœ… Dropped info saved: /Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_dropped_info.csv\n",
      "\n",
      "ðŸ”„ Cleaning pretrained_resnet50...\n",
      "âœ… Cleaned data saved (grouped by diagnosis): /Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_cleaned_master.csv\n",
      "âœ… Dropped info saved: /Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_dropped_info.csv\n",
      "\n",
      "ðŸ”„ Cleaning vit_base...\n",
      "âœ… Cleaned data saved (grouped by diagnosis): /Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_cleaned_master.csv\n",
      "âœ… Dropped info saved: /Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_dropped_info.csv\n",
      "\n",
      "âœ… All 4 datasets cleaned!\n",
      "{'efficientnet': {'original_shape': (522, 480), 'cleaned_shape': (510, 244), 'columns_dropped': 236, 'rows_dropped': 12}, 'imagenet_resnet50': {'original_shape': (522, 480), 'cleaned_shape': (510, 244), 'columns_dropped': 236, 'rows_dropped': 12}, 'pretrained_resnet50': {'original_shape': (522, 480), 'cleaned_shape': (510, 244), 'columns_dropped': 236, 'rows_dropped': 12}, 'vit_base': {'original_shape': (522, 480), 'cleaned_shape': (510, 244), 'columns_dropped': 236, 'rows_dropped': 12}}\n"
     ]
    }
   ],
   "source": [
    "# cleaning all the datasets\n",
    "\n",
    "# âœ… 1. Helper function: Standardize ALL text entries\n",
    "def standardize_text(value):\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"\\xa0\", \" \")          # replace non-breaking spaces\n",
    "        value = re.sub(r\"\\s+\", \" \", value).strip()  # collapse multiple spaces & trim\n",
    "        value = value.lower()                       # lowercase everything\n",
    "        value = re.sub(r\"^[-â€“]+\", \"\", value).strip() # remove leading dashes or en-dashes\n",
    "    return value\n",
    "\n",
    "# âœ… 2. Cleaning & Standardizing Function\n",
    "def clean_and_standardize_all(df, filename_prefix, save_path=\"./\"):\n",
    "    original_shape = df.shape\n",
    "\n",
    "    # --- Standardize text in all object columns ---\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = df[col].apply(standardize_text)\n",
    "\n",
    "    # --- Drop rows with no permanent diagnosis (NaN or empty) ---\n",
    "    rows_before = df.shape[0]\n",
    "    df_cleaned = df[df[\"permanent\"].notna() & (df[\"permanent\"].str.strip() != \"\")]\n",
    "\n",
    "    # --- Track dropped rows ---\n",
    "    rows_missing_dx = df[~df.index.isin(df_cleaned.index)].copy()\n",
    "    rows_missing_dx[\"drop_reason\"] = \"No permanent diagnosis\"\n",
    "\n",
    "    # --- Drop columns with <10 non-null entries ---\n",
    "    cols_to_drop = [col for col in df_cleaned.columns if df_cleaned[col].notna().sum() < 10]\n",
    "    dropped_cols_info = pd.DataFrame({\n",
    "        \"column\": cols_to_drop,\n",
    "        \"non_null_count\": [df_cleaned[c].notna().sum() for c in cols_to_drop],\n",
    "        \"drop_reason\": \"Fewer than 10 non-null entries\"\n",
    "    })\n",
    "    df_cleaned = df_cleaned.drop(columns=cols_to_drop)\n",
    "\n",
    "    # âœ… Explicitly drop any unnamed/blank columns (just in case)\n",
    "    unnamed_cols = [c for c in df_cleaned.columns if \"Unnamed\" in c or c.strip() == \"\"]\n",
    "    if unnamed_cols:\n",
    "        df_cleaned = df_cleaned.drop(columns=unnamed_cols)\n",
    "\n",
    "    # --- âœ… Sort/group by permanent diagnosis ---\n",
    "    df_cleaned = df_cleaned.sort_values(by=\"permanent\").reset_index(drop=True)\n",
    "\n",
    "    # --- Save Cleaned Master CSV ---\n",
    "    os.makedirs(save_path, exist_ok=True)  # create folder if not exists\n",
    "    cleaned_file = os.path.join(save_path, f\"{filename_prefix}_cleaned_master.csv\")\n",
    "    df_cleaned.to_csv(cleaned_file, index=False)\n",
    "    print(f\"âœ… Cleaned data saved (grouped by diagnosis): {cleaned_file}\")\n",
    "\n",
    "    # --- Save Dropped Info CSV ---\n",
    "    dropped_cols_info[\"row_index\"] = \"N/A\"\n",
    "    rows_missing_dx[\"column\"] = \"N/A\"\n",
    "    rows_missing_dx[\"non_null_count\"] = \"N/A\"\n",
    "    dropped_info_combined = pd.concat([dropped_cols_info, rows_missing_dx], ignore_index=True)\n",
    "    dropped_file = os.path.join(save_path, f\"{filename_prefix}_dropped_info.csv\")\n",
    "    dropped_info_combined.to_csv(dropped_file, index=False)\n",
    "    print(f\"âœ… Dropped info saved: {dropped_file}\")\n",
    "\n",
    "    return {\n",
    "        \"original_shape\": original_shape,\n",
    "        \"cleaned_shape\": df_cleaned.shape,\n",
    "        \"columns_dropped\": len(cols_to_drop) + len(unnamed_cols),\n",
    "        \"rows_dropped\": rows_before - df_cleaned.shape[0]\n",
    "    }\n",
    "\n",
    "# âœ… 3. Batch Clean 4 Datasets\n",
    "data_folder = \"/Users/joi263/Documents/MultimodalTabData/data/OG_data_csv\"\n",
    "\n",
    "datasets = {\n",
    "    \"efficientnet\": \"efficientnet_new.csv\",\n",
    "    \"imagenet_resnet50\": \"imagenet_resnet50_new.csv\",\n",
    "    \"pretrained_resnet50\": \"pretrained_resnet50_new.csv\",\n",
    "    \"vit_base\": \"vit_base_new.csv\"\n",
    "}\n",
    "\n",
    "summaries = {}\n",
    "\n",
    "for name, filename in datasets.items():\n",
    "    print(f\"\\nðŸ”„ Cleaning {name}...\")\n",
    "    \n",
    "    # Define dataset-specific save folder (e.g., efficientnet_data)\n",
    "    save_folder = f\"/Users/joi263/Documents/MultimodalTabData/data/{name}_data\"\n",
    "    \n",
    "    # Load and clean\n",
    "    df_raw = pd.read_csv(os.path.join(data_folder, filename))\n",
    "    summaries[name] = clean_and_standardize_all(df_raw, name, save_path=save_folder)\n",
    "\n",
    "print(\"\\nâœ… All 4 datasets cleaned!\")\n",
    "print(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a98902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Column Counts per Cleaned Master CSV:\n",
      "efficientnet: 244 columns\n",
      "imagenet_resnet50: 244 columns\n",
      "pretrained_resnet50: 244 columns\n",
      "vit_base: 244 columns\n"
     ]
    }
   ],
   "source": [
    "#counting how many columns were dropped from master csv pre manual drop\n",
    "\n",
    "# âœ… Paths to cleaned master files\n",
    "cleaned_files = {\n",
    "    \"efficientnet\": \"/Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_cleaned_master.csv\",\n",
    "    \"imagenet_resnet50\": \"/Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_cleaned_master.csv\",\n",
    "    \"pretrained_resnet50\": \"/Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_cleaned_master.csv\",\n",
    "    \"vit_base\": \"/Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_cleaned_master.csv\"\n",
    "}\n",
    "\n",
    "print(\"âœ… Column Counts per Cleaned Master CSV:\")\n",
    "for name, path in cleaned_files.items():\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"{name}: {df.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85604eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Column Counts per Cleaned Master CSV:\n",
      "efficientnet: 228 columns\n",
      "imagenet_resnet50: 228 columns\n",
      "pretrained_resnet50: 228 columns\n",
      "vit_base: 228 columns\n"
     ]
    }
   ],
   "source": [
    "#counting how many columns were dropped from master csv post manual drop\n",
    "\n",
    "# âœ… Paths to cleaned master files\n",
    "cleaned_files = {\n",
    "    \"efficientnet\": \"/Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_cleaned_master.csv\",\n",
    "    \"imagenet_resnet50\": \"/Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_cleaned_master.csv\",\n",
    "    \"pretrained_resnet50\": \"/Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_cleaned_master.csv\",\n",
    "    \"vit_base\": \"/Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_cleaned_master.csv\"\n",
    "}\n",
    "\n",
    "print(\"âœ… Column Counts per Cleaned Master CSV:\")\n",
    "for name, path in cleaned_files.items():\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"{name}: {df.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83da4ee",
   "metadata": {},
   "source": [
    "-Each file started w/ 480 columns\n",
    "-Dropped to 244 columns after initial cleaning\n",
    "-Dropped to 228 columns after manual cleaning (16 columns dropped manually)\n",
    "\n",
    "2. now generate files of diagnosis counts for all 4 remaining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8860b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Processing efficientnet...\n",
      "âœ… Diagnosis counts saved: /Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_diagnosis_counts.csv\n",
      "\n",
      "ðŸ”„ Processing imagenet_resnet50...\n",
      "âœ… Diagnosis counts saved: /Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_diagnosis_counts.csv\n",
      "\n",
      "ðŸ”„ Processing pretrained_resnet50...\n",
      "âœ… Diagnosis counts saved: /Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_diagnosis_counts.csv\n",
      "\n",
      "ðŸ”„ Processing vit_base...\n",
      "âœ… Diagnosis counts saved: /Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_diagnosis_counts.csv\n",
      "\n",
      "âœ… Diagnosis counts generated for all 4 datasets!\n"
     ]
    }
   ],
   "source": [
    "#generates diagnosis counts\n",
    "\n",
    "# âœ… Paths to the 4 cleaned master CSVs\n",
    "cleaned_files = {\n",
    "    \"efficientnet\": \"/Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_cleaned_master.csv\",\n",
    "    \"imagenet_resnet50\": \"/Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_cleaned_master.csv\",\n",
    "    \"pretrained_resnet50\": \"/Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_cleaned_master.csv\",\n",
    "    \"vit_base\": \"/Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_cleaned_master.csv\"\n",
    "}\n",
    "\n",
    "# âœ… Helper: Generate Diagnosis Counts\n",
    "def generate_diagnosis_counts(df, filename_prefix, save_path=\"./\"):\n",
    "    diagnosis_counts = (\n",
    "        df.groupby(\"permanent\")\n",
    "        .size()\n",
    "        .reset_index(name=\"row_count\")\n",
    "        .sort_values(by=\"row_count\", ascending=False)\n",
    "    )\n",
    "    counts_file = os.path.join(save_path, f\"{filename_prefix}_diagnosis_counts.csv\")\n",
    "    diagnosis_counts.to_csv(counts_file, index=False)\n",
    "    print(f\"âœ… Diagnosis counts saved: {counts_file}\")\n",
    "    return diagnosis_counts\n",
    "\n",
    "# âœ… Batch Run for All 4 Datasets\n",
    "all_counts = {}\n",
    "\n",
    "for name, path in cleaned_files.items():\n",
    "    print(f\"\\nðŸ”„ Processing {name}...\")\n",
    "    df = pd.read_csv(path)\n",
    "    save_path = os.path.dirname(path)  # save in the same folder as the cleaned master\n",
    "    all_counts[name] = generate_diagnosis_counts(df, name, save_path=save_path)\n",
    "\n",
    "print(\"\\nâœ… Diagnosis counts generated for all 4 datasets!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6b6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Filtering efficientnet for ['glioma', 'glioblastoma', 'meningioma']...\n",
      "âœ… Filtered CSV saved: /Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_omas_only.csv\n",
      "âœ… Shape: (273, 228) (rows, columns)\n",
      "\n",
      "ðŸ”„ Filtering imagenet_resnet50 for ['glioma', 'glioblastoma', 'meningioma']...\n",
      "âœ… Filtered CSV saved: /Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_omas_only.csv\n",
      "âœ… Shape: (273, 228) (rows, columns)\n",
      "\n",
      "ðŸ”„ Filtering pretrained_resnet50 for ['glioma', 'glioblastoma', 'meningioma']...\n",
      "âœ… Filtered CSV saved: /Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_omas_only.csv\n",
      "âœ… Shape: (273, 228) (rows, columns)\n",
      "\n",
      "ðŸ”„ Filtering vit_base for ['glioma', 'glioblastoma', 'meningioma']...\n",
      "âœ… Filtered CSV saved: /Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_omas_only.csv\n",
      "âœ… Shape: (273, 228) (rows, columns)\n",
      "\n",
      "âœ… Filtering complete for all 4 datasets!\n"
     ]
    }
   ],
   "source": [
    "#filters using glioma, glioblastoma, meningioma\n",
    "\n",
    "# âœ… Paths to the 4 cleaned master CSVs\n",
    "cleaned_files = {\n",
    "    \"efficientnet\": \"/Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_cleaned_master.csv\",\n",
    "    \"imagenet_resnet50\": \"/Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_cleaned_master.csv\",\n",
    "    \"pretrained_resnet50\": \"/Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_cleaned_master.csv\",\n",
    "    \"vit_base\": \"/Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_cleaned_master.csv\"\n",
    "}\n",
    "\n",
    "# âœ… Keywords for filtering\n",
    "keywords = [\"glioma\", \"glioblastoma\", \"meningioma\"]\n",
    "pattern = \"|\".join(keywords)  # \"glioma|glioblastoma|meningioma\"\n",
    "\n",
    "# âœ… Batch Filtering\n",
    "for name, path in cleaned_files.items():\n",
    "    print(f\"\\nðŸ”„ Filtering {name} for {keywords}...\")\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    df_filtered = df[df[\"permanent\"].str.contains(pattern, case=False, na=False)]\n",
    "    \n",
    "    save_path = os.path.join(os.path.dirname(path), f\"{name}_omas_only.csv\")\n",
    "    df_filtered.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(f\"âœ… Filtered CSV saved: {save_path}\")\n",
    "    print(f\"âœ… Shape: {df_filtered.shape} (rows, columns)\")\n",
    "\n",
    "print(\"\\nâœ… Filtering complete for all 4 datasets!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurosurgery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
