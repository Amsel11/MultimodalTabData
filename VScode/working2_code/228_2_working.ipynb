{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1daa20b",
   "metadata": {},
   "source": [
    "*tried splitting the code into cells for better division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708511a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ML imports\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             accuracy_score, roc_curve, precision_recall_curve, auc)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import t\n",
    "\n",
    "# Optional dependencies with proper error handling\n",
    "try:\n",
    "    from tabpfn import TabPFNClassifier\n",
    "    TABPFN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TABPFN_AVAILABLE = False\n",
    "    print(\"TabPFN not available. Install with: pip install tabpfn\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "    import torch\n",
    "    TABNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TABNET_AVAILABLE = False\n",
    "    print(\"TabNet not available. Install with: pip install pytorch-tabnet torch\")\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    \"\"\"Configuration for the analysis\"\"\"\n",
    "    # File paths - CHANGE THESE TO YOUR PATHS\n",
    "    data_base_path: str = \"/Users/joi263/Documents/MultimodalTabData/data\"\n",
    "    \n",
    "    # Dataset configurations\n",
    "    datasets: Dict[str, str] = None\n",
    "    \n",
    "    # ML parameters\n",
    "    test_size: float = 0.25\n",
    "    cv_folds: int = 5\n",
    "    random_state: int = 42\n",
    "    max_features: int = 100\n",
    "    missing_threshold: float = 0.5\n",
    "    min_class_size: int = 3\n",
    "    min_sample_size: int = 15\n",
    "    \n",
    "    # Confidence interval\n",
    "    confidence_level: float = 0.95\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.datasets is None:\n",
    "            self.datasets = {\n",
    "                'ConvNext': f'{self.data_base_path}/convnext_data/convnext_cleaned_master.csv',\n",
    "                'ViT': f'{self.data_base_path}/vit_base_data/vit_base_cleaned_master.csv',\n",
    "                'ResNet50_Pretrained': f'{self.data_base_path}/pretrained_resnet50_data/pretrained_resnet50_cleaned_master.csv',\n",
    "                'ResNet50_ImageNet': f'{self.data_base_path}/imagenet_resnet50_data/imagenet_resnet50_cleaned_master.csv',\n",
    "                'EfficientNet': f'{self.data_base_path}/efficientnet_data/efficientnet_cleaned_master.csv'\n",
    "            }\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize configuration\n",
    "config = AnalysisConfig()\n",
    "\n",
    "print(\"Configuration initialized successfully!\")\n",
    "print(f\"TabPFN available: {TABPFN_AVAILABLE}\")\n",
    "print(f\"XGBoost available: {XGBOOST_AVAILABLE}\")\n",
    "print(f\"TabNet available: {TABNET_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: File Validation Functions\n",
    "\n",
    "def validate_file_paths(datasets: Dict[str, str]) -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Validate that all dataset files exist\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dictionary mapping dataset names to file paths\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping dataset names to existence status\n",
    "    \"\"\"\n",
    "    print(\"CHECKING DATA FILE PATHS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    file_status = {}\n",
    "    existing_count = 0\n",
    "    \n",
    "    for name, path in datasets.items():\n",
    "        exists = Path(path).exists()\n",
    "        status = \"EXISTS\" if exists else \"NOT FOUND\"\n",
    "        print(f\"{name:<20}: {status}\")\n",
    "        \n",
    "        file_status[name] = exists\n",
    "        if exists:\n",
    "            existing_count += 1\n",
    "    \n",
    "    print(f\"Found {existing_count}/{len(datasets)} files\")\n",
    "    print(\"=\" * 50, \"\\n\")\n",
    "    \n",
    "    return file_status\n",
    "\n",
    "def load_and_validate_dataset(file_path: str, dataset_name: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and perform basic validation on a dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        dataset_name: Name of the dataset for logging\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame if successful, None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not Path(file_path).exists():\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.error(f\"Dataset {dataset_name} is empty\")\n",
    "            return None\n",
    "            \n",
    "        logger.info(f\"Loaded {dataset_name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load {dataset_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_dataset_summary(df: pd.DataFrame, dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate summary statistics for a dataset\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        dataset_name: Name of the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing summary statistics\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'n_rows': len(df),\n",
    "        'n_columns': len(df.columns),\n",
    "        'missing_data_percent': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,\n",
    "        'numeric_columns': len(df.select_dtypes(include=[np.number]).columns),\n",
    "        'categorical_columns': len(df.select_dtypes(include=['object']).columns),\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Run file validation\n",
    "file_status = validate_file_paths(config.datasets)\n",
    "print(\"File validation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ece5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Target Creation Functions\n",
    "\n",
    "def create_mortality_targets(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create mortality prediction targets at different time points\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with mortality data and targets, or None if insufficient data\n",
    "    \"\"\"\n",
    "    required_cols = ['survival', 'patient_status']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        logger.warning(\"Missing required columns for mortality targets\")\n",
    "        return None\n",
    "    \n",
    "    # Filter for valid survival data\n",
    "    survival_df = df[df['survival'].notna() & df['patient_status'].notna()].copy()\n",
    "    \n",
    "    if len(survival_df) < config.min_sample_size:\n",
    "        logger.warning(f\"Insufficient survival data: {len(survival_df)} samples\")\n",
    "        return None\n",
    "    \n",
    "    # Create mortality targets (patient_status==2 means death)\n",
    "    survival_df['mortality_6mo'] = (\n",
    "        (survival_df['patient_status'] == 2) & \n",
    "        (survival_df['survival'] <= 6)\n",
    "    ).astype(int)\n",
    "    \n",
    "    survival_df['mortality_1yr'] = (\n",
    "        (survival_df['patient_status'] == 2) & \n",
    "        (survival_df['survival'] <= 12)\n",
    "    ).astype(int)\n",
    "    \n",
    "    survival_df['mortality_2yr'] = (\n",
    "        (survival_df['patient_status'] == 2) & \n",
    "        (survival_df['survival'] <= 24)\n",
    "    ).astype(int)\n",
    "    \n",
    "    targets = ['mortality_6mo', 'mortality_1yr', 'mortality_2yr']\n",
    "    descriptions = ['6-month Mortality', '1-year Mortality', '2-year Mortality']\n",
    "    \n",
    "    # Log class distributions\n",
    "    for target, desc in zip(targets, descriptions):\n",
    "        pos_count = survival_df[target].sum()\n",
    "        total_count = len(survival_df)\n",
    "        logger.info(f\"{desc}: {pos_count}/{total_count} ({pos_count/total_count*100:.1f}% positive)\")\n",
    "    \n",
    "    return {\n",
    "        'data': survival_df,\n",
    "        'targets': targets,\n",
    "        'descriptions': descriptions,\n",
    "        'category': 'mortality'\n",
    "    }\n",
    "\n",
    "def create_tumor_grade_targets(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create tumor grade classification targets\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with tumor grade data and targets, or None if insufficient data\n",
    "    \"\"\"\n",
    "    if 'methylation_class' not in df.columns:\n",
    "        logger.warning(\"Missing methylation_class column for tumor grade targets\")\n",
    "        return None\n",
    "    \n",
    "    tumor_df = df[df['methylation_class'].notna()].copy()\n",
    "    \n",
    "    if len(tumor_df) < config.min_sample_size:\n",
    "        logger.warning(f\"Insufficient tumor grade data: {len(tumor_df)} samples\")\n",
    "        return None\n",
    "    \n",
    "    # Define high-grade tumor patterns\n",
    "    high_grade_terms = [\n",
    "        'glioblastoma', 'anaplastic', 'high grade', \n",
    "        'grade iv', 'grade 4', 'gbm'\n",
    "    ]\n",
    "    \n",
    "    pattern = '|'.join(high_grade_terms)\n",
    "    tumor_df['high_grade'] = (\n",
    "        tumor_df['methylation_class']\n",
    "        .str.lower()\n",
    "        .str.contains(pattern, na=False)\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    # Log class distribution\n",
    "    pos_count = tumor_df['high_grade'].sum()\n",
    "    total_count = len(tumor_df)\n",
    "    logger.info(f\"High-grade tumors: {pos_count}/{total_count} ({pos_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'data': tumor_df,\n",
    "        'targets': ['high_grade'],\n",
    "        'descriptions': ['High vs Low Grade Tumor'],\n",
    "        'category': 'tumor_grade'\n",
    "    }\n",
    "\n",
    "def create_idh_targets(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create IDH mutation status targets\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with IDH data and targets, or None if insufficient data\n",
    "    \"\"\"\n",
    "    if 'idh_1_r132h' not in df.columns:\n",
    "        logger.warning(\"Missing idh_1_r132h column for IDH targets\")\n",
    "        return None\n",
    "    \n",
    "    idh_df = df.copy()\n",
    "    idh_df['idh_binary'] = np.nan\n",
    "    \n",
    "    # First, check text-based IDH1 column if available\n",
    "    if 'idh1' in df.columns:\n",
    "        idh_text = df['idh1'].astype(str).str.lower()\n",
    "        mutation_patterns = ['r132h', 'r132s', 'arg132', 'missense', 'p.arg132']\n",
    "        pattern = '|'.join(mutation_patterns)\n",
    "        \n",
    "        mutation_mask = idh_text.str.contains(pattern, na=False)\n",
    "        idh_df.loc[mutation_mask, 'idh_binary'] = 1\n",
    "    \n",
    "    # Then use coded values for remaining cases\n",
    "    remaining_mask = idh_df['idh_binary'].isna() & idh_df['idh_1_r132h'].notna()\n",
    "    \n",
    "    # Assuming: 1=wild-type, 2=mutant, 3=unknown\n",
    "    idh_df.loc[remaining_mask & (idh_df['idh_1_r132h'] == 2), 'idh_binary'] = 1\n",
    "    idh_df.loc[remaining_mask & (idh_df['idh_1_r132h'] == 1), 'idh_binary'] = 0\n",
    "    idh_df.loc[idh_df['idh_1_r132h'] == 3, 'idh_binary'] = np.nan\n",
    "    \n",
    "    # Filter to valid cases\n",
    "    valid_idh_df = idh_df[idh_df['idh_binary'].notna()].copy()\n",
    "    \n",
    "    if len(valid_idh_df) < config.min_sample_size:\n",
    "        logger.warning(f\"Insufficient IDH data: {len(valid_idh_df)} samples\")\n",
    "        return None\n",
    "    \n",
    "    # Log class distribution\n",
    "    pos_count = valid_idh_df['idh_binary'].sum()\n",
    "    total_count = len(valid_idh_df)\n",
    "    logger.info(f\"IDH mutations: {pos_count}/{total_count} ({pos_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'data': valid_idh_df,\n",
    "        'targets': ['idh_binary'],\n",
    "        'descriptions': ['IDH Mutation Status'],\n",
    "        'category': 'idh'\n",
    "    }\n",
    "\n",
    "def create_mgmt_targets(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create MGMT methylation status targets\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with MGMT data and targets, or None if insufficient data\n",
    "    \"\"\"\n",
    "    if 'mgmt' not in df.columns:\n",
    "        logger.warning(\"Missing mgmt column for MGMT targets\")\n",
    "        return None\n",
    "    \n",
    "    mgmt_df = df[df['mgmt'].notna()].copy()\n",
    "    \n",
    "    if len(mgmt_df) < config.min_sample_size:\n",
    "        logger.warning(f\"Insufficient MGMT data: {len(mgmt_df)} samples\")\n",
    "        return None\n",
    "    \n",
    "    mgmt_df['mgmt_binary'] = np.nan\n",
    "    \n",
    "    # Assuming: 1=methylated, 2=unmethylated, 3=unknown\n",
    "    mgmt_df.loc[mgmt_df['mgmt'] == 1, 'mgmt_binary'] = 1\n",
    "    mgmt_df.loc[mgmt_df['mgmt'] == 2, 'mgmt_binary'] = 0\n",
    "    mgmt_df.loc[mgmt_df['mgmt'] == 3, 'mgmt_binary'] = np.nan\n",
    "    \n",
    "    # Filter to valid cases\n",
    "    valid_mgmt_df = mgmt_df[mgmt_df['mgmt_binary'].notna()].copy()\n",
    "    \n",
    "    if len(valid_mgmt_df) < config.min_sample_size:\n",
    "        logger.warning(f\"Insufficient valid MGMT data: {len(valid_mgmt_df)} samples\")\n",
    "        return None\n",
    "    \n",
    "    # Log class distribution\n",
    "    pos_count = valid_mgmt_df['mgmt_binary'].sum()\n",
    "    total_count = len(valid_mgmt_df)\n",
    "    logger.info(f\"MGMT methylation: {pos_count}/{total_count} ({pos_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'data': valid_mgmt_df,\n",
    "        'targets': ['mgmt_binary'],\n",
    "        'descriptions': ['MGMT Methylation Status'],\n",
    "        'category': 'mgmt'\n",
    "    }\n",
    "\n",
    "def create_all_targets(df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create all prediction targets for a dataset\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping target categories to target information\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating prediction targets...\")\n",
    "    \n",
    "    all_targets = {}\n",
    "    \n",
    "    # Create each target type\n",
    "    target_creators = [\n",
    "        ('mortality', create_mortality_targets),\n",
    "        ('tumor_grade', create_tumor_grade_targets),\n",
    "        ('idh', create_idh_targets),\n",
    "        ('mgmt', create_mgmt_targets)\n",
    "    ]\n",
    "    \n",
    "    for target_name, creator_func in target_creators:\n",
    "        try:\n",
    "            target_info = creator_func(df)\n",
    "            if target_info is not None:\n",
    "                all_targets[target_name] = target_info\n",
    "                logger.info(f\"Successfully created {target_name} targets\")\n",
    "            else:\n",
    "                logger.warning(f\"Failed to create {target_name} targets\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating {target_name} targets: {str(e)}\")\n",
    "    \n",
    "    logger.info(f\"Created {len(all_targets)} target categories\")\n",
    "    return all_targets\n",
    "\n",
    "print(\"Target creation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Selection and Preprocessing Functions\n",
    "\n",
    "def get_feature_categories(df: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Categorize features into clinical, molecular, and imaging groups\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping feature categories to lists of column names\n",
    "    \"\"\"\n",
    "    clinical_features = ['age', 'sex', 'race', 'ethnicity', 'gtr']\n",
    "    molecular_features = [\n",
    "        'mgmt_pyro', 'atrx', 'p53', 'braf_v600', \n",
    "        'h3k27m', 'gfap', 'tumor', 'hg_glioma'\n",
    "    ]\n",
    "    \n",
    "    # Find imaging features (those starting with 'feature_')\n",
    "    imaging_features = [col for col in df.columns if col.startswith('feature_')]\n",
    "    \n",
    "    # Filter to only include features that exist in the dataframe\n",
    "    available_features = {\n",
    "        'clinical': [f for f in clinical_features if f in df.columns],\n",
    "        'molecular': [f for f in molecular_features if f in df.columns],\n",
    "        'imaging': imaging_features\n",
    "    }\n",
    "    \n",
    "    # Log feature counts\n",
    "    for category, features in available_features.items():\n",
    "        logger.info(f\"{category.capitalize()} features: {len(features)}\")\n",
    "    \n",
    "    return available_features\n",
    "\n",
    "def select_features_for_target(all_features: List[str], target_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Select appropriate features for a specific target, avoiding data leakage\n",
    "    \n",
    "    Args:\n",
    "        all_features: List of all available features\n",
    "        target_name: Name of the target variable\n",
    "        \n",
    "    Returns:\n",
    "        List of safe features for this target\n",
    "    \"\"\"\n",
    "    # Define features that should be excluded for each target to prevent leakage\n",
    "    unsafe_patterns = {\n",
    "        'idh_binary': ['idh'],\n",
    "        'mgmt_binary': ['mgmt'],\n",
    "        'high_grade': [],  # No specific exclusions for tumor grade\n",
    "        'mortality_6mo': [],\n",
    "        'mortality_1yr': [],\n",
    "        'mortality_2yr': []\n",
    "    }\n",
    "    \n",
    "    patterns_to_exclude = unsafe_patterns.get(target_name, [])\n",
    "    \n",
    "    # Filter out unsafe features\n",
    "    safe_features = []\n",
    "    for feature in all_features:\n",
    "        is_safe = True\n",
    "        for pattern in patterns_to_exclude:\n",
    "            if pattern.lower() in feature.lower():\n",
    "                is_safe = False\n",
    "                break\n",
    "        if is_safe:\n",
    "            safe_features.append(feature)\n",
    "    \n",
    "    excluded_count = len(all_features) - len(safe_features)\n",
    "    if excluded_count > 0:\n",
    "        logger.info(f\"Excluded {excluded_count} features for {target_name} to prevent leakage\")\n",
    "    \n",
    "    return safe_features\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame, features: List[str]) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Handle missing values in features\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        features: List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (processed dataframe, final feature list)\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Calculate missing percentages\n",
    "    missing_percentages = df_processed[features].isnull().mean()\n",
    "    \n",
    "    # Remove features with too much missing data\n",
    "    features_to_keep = missing_percentages[missing_percentages <= config.missing_threshold].index.tolist()\n",
    "    features_removed = len(features) - len(features_to_keep)\n",
    "    \n",
    "    if features_removed > 0:\n",
    "        logger.info(f\"Removed {features_removed} features due to >50% missing data\")\n",
    "    \n",
    "    # Impute remaining missing values\n",
    "    numeric_features = df_processed[features_to_keep].select_dtypes(include=[np.number]).columns\n",
    "    categorical_features = df_processed[features_to_keep].select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Impute numeric features with median\n",
    "    for feature in numeric_features:\n",
    "        if df_processed[feature].isnull().any():\n",
    "            median_value = df_processed[feature].median()\n",
    "            df_processed[feature].fillna(median_value, inplace=True)\n",
    "            logger.debug(f\"Imputed {feature} with median: {median_value}\")\n",
    "    \n",
    "    # Impute categorical features with mode\n",
    "    for feature in categorical_features:\n",
    "        if df_processed[feature].isnull().any():\n",
    "            mode_value = df_processed[feature].mode().iloc[0] if not df_processed[feature].mode().empty else 'Unknown'\n",
    "            df_processed[feature].fillna(mode_value, inplace=True)\n",
    "            logger.debug(f\"Imputed {feature} with mode: {mode_value}\")\n",
    "    \n",
    "    return df_processed, features_to_keep\n",
    "\n",
    "def encode_categorical_features(df: pd.DataFrame, features: List[str], target: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode categorical features using label encoding\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        features: List of feature names\n",
    "        target: Target variable name\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with encoded categorical features\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Find categorical features (excluding target)\n",
    "    categorical_features = df_encoded[features].select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if categorical_features:\n",
    "        logger.info(f\"Encoding {len(categorical_features)} categorical features\")\n",
    "        \n",
    "        for feature in categorical_features:\n",
    "            try:\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[feature] = le.fit_transform(df_encoded[feature].astype(str))\n",
    "                logger.debug(f\"Encoded {feature}: {len(le.classes_)} classes\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to encode {feature}: {str(e)}\")\n",
    "                # Remove problematic feature\n",
    "                if feature in features:\n",
    "                    features.remove(feature)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def perform_feature_selection(X: np.ndarray, y: np.ndarray, max_features: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform feature selection if there are too many features\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        max_features: Maximum number of features to keep\n",
    "        \n",
    "    Returns:\n",
    "        Transformed feature matrix\n",
    "    \"\"\"\n",
    "    if max_features is None:\n",
    "        max_features = config.max_features\n",
    "    \n",
    "    if X.shape[1] <= max_features:\n",
    "        return X\n",
    "    \n",
    "    logger.info(f\"Reducing features from {X.shape[1]} to {max_features} using univariate selection\")\n",
    "    \n",
    "    try:\n",
    "        selector = SelectKBest(score_func=f_classif, k=max_features)\n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "        logger.info(f\"Feature selection completed: {X_selected.shape[1]} features retained\")\n",
    "        return X_selected\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Feature selection failed: {str(e)}, using original features\")\n",
    "        return X\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame, features: List[str], target: str) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for features and target\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        features: List of feature names\n",
    "        target: Target variable name\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (X, y, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter to samples with valid target values\n",
    "        data = df[features + [target]].dropna(subset=[target]).copy()\n",
    "        \n",
    "        if len(data) < config.min_sample_size:\n",
    "            return None, None, f\"Insufficient data: {len(data)} samples (minimum: {config.min_sample_size})\"\n",
    "        \n",
    "        # Handle missing values\n",
    "        data_processed, final_features = handle_missing_values(data, features)\n",
    "        \n",
    "        if len(final_features) == 0:\n",
    "            return None, None, \"No features remaining after missing value handling\"\n",
    "        \n",
    "        # Encode categorical features\n",
    "        data_encoded = encode_categorical_features(data_processed, final_features, target)\n",
    "        \n",
    "        # Extract feature matrix and target vector\n",
    "        X = data_encoded[final_features].values\n",
    "        y = data_encoded[target].values\n",
    "        \n",
    "        # Check class balance\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        min_class_size = min(class_counts)\n",
    "        \n",
    "        if min_class_size < config.min_class_size:\n",
    "            return None, None, f\"Smallest class has only {min_class_size} samples (minimum: {config.min_class_size})\"\n",
    "        \n",
    "        # Log class distribution\n",
    "        class_distribution = dict(zip(unique_classes, class_counts))\n",
    "        logger.info(f\"Class distribution: {class_distribution}\")\n",
    "        \n",
    "        # Perform feature selection if needed\n",
    "        X_selected = perform_feature_selection(X, y)\n",
    "        \n",
    "        logger.info(f\"Preprocessing completed: {X_selected.shape[0]} samples, {X_selected.shape[1]} features\")\n",
    "        return X_selected, y, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Preprocessing failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return None, None, error_msg\n",
    "\n",
    "print(\"Feature selection and preprocessing functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1436e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Machine Learning Algorithm Configuration\n",
    "\n",
    "def get_algorithm_config(algorithm_name: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get configuration for a specific algorithm\n",
    "    \n",
    "    Args:\n",
    "        algorithm_name: Name of the algorithm\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with model and configuration, or None if not available\n",
    "    \"\"\"\n",
    "    configs = {\n",
    "        'TabPFN': {\n",
    "            'available': TABPFN_AVAILABLE,\n",
    "            'model': lambda: TabPFNClassifier(device='cpu', N_ensemble_configurations=4),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Transformer-based Prior-Data Fitted Networks'\n",
    "        },\n",
    "        \n",
    "        'XGBoost': {\n",
    "            'available': XGBOOST_AVAILABLE,\n",
    "            'model': lambda: xgb.XGBClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                min_child_weight=3,\n",
    "                reg_alpha=1,\n",
    "                reg_lambda=1,\n",
    "                random_state=config.random_state,\n",
    "                eval_metric='logloss',\n",
    "                use_label_encoder=False,\n",
    "                verbosity=0\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Gradient Boosting Decision Trees'\n",
    "        },\n",
    "        \n",
    "        'TabNet': {\n",
    "            'available': TABNET_AVAILABLE,\n",
    "            'model': lambda: TabNetClassifier(\n",
    "                n_d=64,\n",
    "                n_a=64,\n",
    "                n_steps=5,\n",
    "                gamma=1.5,\n",
    "                lambda_sparse=1e-4,\n",
    "                optimizer_fn=torch.optim.Adam,\n",
    "                optimizer_params=dict(lr=0.01, weight_decay=1e-5),\n",
    "                mask_type='entmax',\n",
    "                scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                scheduler_params={'step_size': 20, 'gamma': 0.8},\n",
    "                verbose=0,\n",
    "                seed=config.random_state\n",
    "            ),\n",
    "            'needs_scaling': True,\n",
    "            'description': 'Attentive Interpretable Tabular Learning'\n",
    "        },\n",
    "        \n",
    "        'RandomForest': {\n",
    "            'available': True,\n",
    "            'model': lambda: RandomForestClassifier(\n",
    "                n_estimators=500,\n",
    "                max_depth=8,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                max_features='sqrt',\n",
    "                oob_score=True,\n",
    "                class_weight='balanced',\n",
    "                random_state=config.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Ensemble of Decision Trees'\n",
    "        },\n",
    "        \n",
    "        'LogisticRegression': {\n",
    "            'available': True,\n",
    "            'model': lambda: LogisticRegression(\n",
    "                penalty='elasticnet',\n",
    "                l1_ratio=0.5,\n",
    "                C=0.1,\n",
    "                solver='saga',\n",
    "                max_iter=2000,\n",
    "                class_weight='balanced',\n",
    "                random_state=config.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'needs_scaling': True,\n",
    "            'description': 'Linear Classification with Regularization'\n",
    "        },\n",
    "        \n",
    "        'SVM': {\n",
    "            'available': True,\n",
    "            'model': lambda: SVC(\n",
    "                kernel='rbf',\n",
    "                C=1.0,\n",
    "                gamma='scale',\n",
    "                probability=True,\n",
    "                class_weight='balanced',\n",
    "                random_state=config.random_state\n",
    "            ),\n",
    "            'needs_scaling': True,\n",
    "            'description': 'Support Vector Machine with RBF Kernel'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if algorithm_name not in configs:\n",
    "        logger.error(f\"Unknown algorithm: {algorithm_name}\")\n",
    "        return None\n",
    "    \n",
    "    config_dict = configs[algorithm_name]\n",
    "    if not config_dict['available']:\n",
    "        logger.warning(f\"{algorithm_name} is not available (missing dependencies)\")\n",
    "        return None\n",
    "    \n",
    "    # Create model instance\n",
    "    try:\n",
    "        model_instance = config_dict['model']()\n",
    "        return {\n",
    "            'model': model_instance,\n",
    "            'needs_scaling': config_dict['needs_scaling'],\n",
    "            'description': config_dict['description']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize {algorithm_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_available_algorithms() -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get all available algorithms\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping algorithm names to their configurations\n",
    "    \"\"\"\n",
    "    algorithm_names = ['TabPFN', 'XGBoost', 'TabNet', 'RandomForest', 'LogisticRegression', 'SVM']\n",
    "    \n",
    "    available_algorithms = {}\n",
    "    for name in algorithm_names:\n",
    "        config_dict = get_algorithm_config(name)\n",
    "        if config_dict is not None:\n",
    "            available_algorithms[name] = config_dict\n",
    "    \n",
    "    logger.info(f\"Available algorithms: {list(available_algorithms.keys())}\")\n",
    "    return available_algorithms\n",
    "\n",
    "def get_scaler(scaler_type: str = 'robust'):\n",
    "    \"\"\"\n",
    "    Get a data scaler\n",
    "    \n",
    "    Args:\n",
    "        scaler_type: Type of scaler ('robust', 'standard')\n",
    "        \n",
    "    Returns:\n",
    "        Scaler instance\n",
    "    \"\"\"\n",
    "    if scaler_type == 'robust':\n",
    "        return RobustScaler(quantile_range=(10, 90))\n",
    "    elif scaler_type == 'standard':\n",
    "        return StandardScaler()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scaler type: {scaler_type}\")\n",
    "\n",
    "# Initialize available algorithms\n",
    "available_algorithms = get_available_algorithms()\n",
    "print(\"Machine learning algorithm configurations loaded successfully!\")\n",
    "print(f\"Available algorithms: {list(available_algorithms.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d27289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model Training and Evaluation Functions\n",
    "\n",
    "def calculate_classification_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive classification metrics\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        y_prob: Predicted probabilities\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Basic metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # AUC-ROC\n",
    "        try:\n",
    "            auc_score = roc_auc_score(y_true, y_prob)\n",
    "        except ValueError as e:\n",
    "            logger.warning(f\"Could not calculate AUC: {str(e)}\")\n",
    "            auc_score = 0.5\n",
    "        \n",
    "        # Confusion matrix and derived metrics\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            \n",
    "            # Calculate metrics with division by zero protection\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "            ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0  # Positive Predictive Value\n",
    "            npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0  # Negative Predictive Value\n",
    "            \n",
    "            balanced_accuracy = (sensitivity + specificity) / 2\n",
    "            f1_score = 2 * (ppv * sensitivity) / (ppv + sensitivity) if (ppv + sensitivity) > 0 else 0.0\n",
    "            \n",
    "        else:\n",
    "            # Multi-class case (shouldn't happen with binary classification)\n",
    "            sensitivity = specificity = ppv = npv = balanced_accuracy = f1_score = 0.0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc_score,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'ppv': ppv,\n",
    "            'npv': npv,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'f1_score': f1_score,\n",
    "            'confusion_matrix': cm,\n",
    "            'n_test': len(y_true)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating metrics: {str(e)}\")\n",
    "        return {\n",
    "            'accuracy': 0.0, 'auc': 0.5, 'sensitivity': 0.0, 'specificity': 0.0,\n",
    "            'ppv': 0.0, 'npv': 0.0, 'balanced_accuracy': 0.0, 'f1_score': 0.0,\n",
    "            'confusion_matrix': np.array([[0, 0], [0, 0]]), 'n_test': len(y_true)\n",
    "        }\n",
    "\n",
    "def train_single_algorithm(X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray, \n",
    "                          y_test: np.ndarray, algorithm_name: str, algorithm_config: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Train and evaluate a single algorithm\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        X_test: Test features\n",
    "        y_train: Training labels\n",
    "        y_test: Test labels\n",
    "        algorithm_name: Name of the algorithm\n",
    "        algorithm_config: Algorithm configuration\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of results or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Training {algorithm_name}...\")\n",
    "        \n",
    "        model = algorithm_config['model']\n",
    "        needs_scaling = algorithm_config['needs_scaling']\n",
    "        \n",
    "        # Scale data if needed\n",
    "        if needs_scaling:\n",
    "            scaler = get_scaler('robust')\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "        else:\n",
    "            X_train_scaled = X_train\n",
    "            X_test_scaled = X_test\n",
    "        \n",
    "        # Train model with algorithm-specific handling\n",
    "        if algorithm_name == 'TabNet' and TABNET_AVAILABLE:\n",
    "            model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                eval_set=[(X_test_scaled, y_test)],\n",
    "                patience=20,\n",
    "                max_epochs=100,\n",
    "                eval_metric=['auc'],\n",
    "                batch_size=min(256, len(X_train) // 4)\n",
    "            )\n",
    "            y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            y_pred = (y_prob > 0.5).astype(int)\n",
    "            \n",
    "        elif algorithm_name == 'XGBoost' and XGBOOST_AVAILABLE:\n",
    "            try:\n",
    "                model.fit(\n",
    "                    X_train_scaled, y_train,\n",
    "                    eval_set=[(X_test_scaled, y_test)],\n",
    "                    verbose=False\n",
    "                )\n",
    "            except TypeError:\n",
    "                # Fallback if eval_set is not supported\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            \n",
    "        else:\n",
    "            # Standard sklearn interface\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            else:\n",
    "                y_prob = y_pred.astype(float)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_classification_metrics(y_test, y_pred, y_prob)\n",
    "        metrics['scaling_used'] = needs_scaling\n",
    "        metrics['algorithm'] = algorithm_name\n",
    "        \n",
    "        logger.info(f\"{algorithm_name} - AUC: {metrics['auc']:.3f}, Accuracy: {metrics['accuracy']:.3f}\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"{algorithm_name} training failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def perform_cross_validation(X: np.ndarray, y: np.ndarray, algorithm_name: str, \n",
    "                           algorithm_config: Dict[str, Any], cv_folds: int = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform cross-validation for an algorithm\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        algorithm_name: Name of the algorithm\n",
    "        algorithm_config: Algorithm configuration\n",
    "        cv_folds: Number of CV folds\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of cross-validation results\n",
    "    \"\"\"\n",
    "    if cv_folds is None:\n",
    "        cv_folds = config.cv_folds\n",
    "    \n",
    "    try:\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=config.random_state)\n",
    "        \n",
    "        auc_scores = []\n",
    "        accuracy_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "            logger.debug(f\"CV Fold {fold + 1}/{cv_folds} for {algorithm_name}\")\n",
    "            \n",
    "            # Get fresh model instance for each fold\n",
    "            fresh_config = get_algorithm_config(algorithm_name)\n",
    "            if fresh_config is None:\n",
    "                continue\n",
    "            \n",
    "            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "            \n",
    "            fold_results = train_single_algorithm(\n",
    "                X_train_fold, X_val_fold, y_train_fold, y_val_fold,\n",
    "                algorithm_name, fresh_config\n",
    "            )\n",
    "            \n",
    "            if fold_results is not None:\n",
    "                auc_scores.append(fold_results['auc'])\n",
    "                accuracy_scores.append(fold_results['accuracy'])\n",
    "        \n",
    "        if len(auc_scores) == 0:\n",
    "            logger.warning(f\"No successful CV folds for {algorithm_name}\")\n",
    "            return {\n",
    "                'cv_auc_mean': 0.5, 'cv_auc_std': 0.0,\n",
    "                'cv_auc_ci_lower': 0.5, 'cv_auc_ci_upper': 0.5,\n",
    "                'cv_accuracy_mean': 0.5, 'cv_accuracy_std': 0.0,\n",
    "                'cv_folds': 0, 'cv_stability': 'FAILED'\n",
    "            }\n",
    "        \n",
    "        # Calculate statistics\n",
    "        auc_mean = np.mean(auc_scores)\n",
    "        auc_std = np.std(auc_scores)\n",
    "        accuracy_mean = np.mean(accuracy_scores)\n",
    "        accuracy_std = np.std(accuracy_scores)\n",
    "        \n",
    "        # Calculate confidence interval\n",
    "        confidence_alpha = 1 - config.confidence_level\n",
    "        t_critical = t.ppf(1 - confidence_alpha/2, len(auc_scores) - 1)\n",
    "        margin_of_error = t_critical * (auc_std / np.sqrt(len(auc_scores)))\n",
    "        \n",
    "        ci_lower = max(0, auc_mean - margin_of_error)\n",
    "        ci_upper = min(1, auc_mean + margin_of_error)\n",
    "        \n",
    "        # Assess stability\n",
    "        coefficient_of_variation = auc_std / auc_mean if auc_mean > 0 else float('inf')\n",
    "        if coefficient_of_variation < 0.05:\n",
    "            stability = 'HIGH'\n",
    "        elif coefficient_of_variation < 0.10:\n",
    "            stability = 'STABLE'\n",
    "        else:\n",
    "            stability = 'VARIABLE'\n",
    "        \n",
    "        cv_results = {\n",
    "            'cv_auc_mean': auc_mean,\n",
    "            'cv_auc_std': auc_std,\n",
    "            'cv_auc_ci_lower': ci_lower,\n",
    "            'cv_auc_ci_upper': ci_upper,\n",
    "            'cv_accuracy_mean': accuracy_mean,\n",
    "            'cv_accuracy_std': accuracy_std,\n",
    "            'cv_folds': len(auc_scores),\n",
    "            'cv_stability': stability\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"{algorithm_name} CV - AUC: {auc_mean:.3f}Â±{auc_std:.3f} ({stability})\")\n",
    "        return cv_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cross-validation failed for {algorithm_name}: {str(e)}\")\n",
    "        return {\n",
    "            'cv_auc_mean': 0.5, 'cv_auc_std': 0.0,\n",
    "            'cv_auc_ci_lower': 0.5, 'cv_auc_ci_upper': 0.5,\n",
    "            'cv_accuracy_mean': 0.5, 'cv_accuracy_std': 0.0,\n",
    "            'cv_folds': 0, 'cv_stability': 'FAILED'\n",
    "        }\n",
    "\n",
    "print(\"Model training and evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main Analysis Pipeline Functions\n",
    "\n",
    "def run_prediction_task(X: np.ndarray, y: np.ndarray, task_name: str, \n",
    "                       dataset_name: str, algorithms: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run a complete prediction task with multiple algorithms\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        task_name: Name of the prediction task\n",
    "        dataset_name: Name of the dataset\n",
    "        algorithms: Dictionary of available algorithms\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping algorithm names to their results\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n=== {task_name} - {dataset_name} ===\")\n",
    "    logger.info(f\"Data shape: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "    class_ratio = min(class_counts) / max(class_counts)\n",
    "    logger.info(f\"Class distribution: {dict(zip(unique_classes, class_counts))}\")\n",
    "    logger.info(f\"Class ratio: {class_ratio:.3f}\")\n",
    "    \n",
    "    try:\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=config.test_size, \n",
    "            random_state=config.random_state, \n",
    "            stratify=y\n",
    "        )\n",
    "        logger.info(f\"Train/Test split: {len(X_train)}/{len(X_test)} samples\")\n",
    "        \n",
    "    except ValueError:\n",
    "        # Fallback if stratification fails\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=config.test_size, \n",
    "            random_state=config.random_state\n",
    "        )\n",
    "        logger.warning(\"Stratified split failed, using random split\")\n",
    "    \n",
    "    task_results = {}\n",
    "    \n",
    "    # Test each algorithm\n",
    "    for algorithm_name, algorithm_config in algorithms.items():\n",
    "        logger.info(f\"Testing {algorithm_name}...\")\n",
    "        \n",
    "        # Train and evaluate on holdout set\n",
    "        holdout_results = train_single_algorithm(\n",
    "            X_train, X_test, y_train, y_test, \n",
    "            algorithm_name, algorithm_config\n",
    "        )\n",
    "        \n",
    "        if holdout_results is None:\n",
    "            logger.warning(f\"{algorithm_name} failed on holdout set\")\n",
    "            continue\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_results = perform_cross_validation(X, y, algorithm_name, algorithm_config)\n",
    "        \n",
    "        # Combine results\n",
    "        combined_results = {**holdout_results, **cv_results}\n",
    "        combined_results['task_name'] = task_name\n",
    "        combined_results['dataset_name'] = dataset_name\n",
    "        \n",
    "        task_results[algorithm_name] = combined_results\n",
    "        \n",
    "        logger.info(f\"{algorithm_name} completed - Holdout AUC: {holdout_results['auc']:.3f}, \"\n",
    "                   f\"CV AUC: {cv_results['cv_auc_mean']:.3f}Â±{cv_results['cv_auc_std']:.3f}\")\n",
    "    \n",
    "    logger.info(f\"Task completed: {len(task_results)} algorithms successful\")\n",
    "    return task_results\n",
    "\n",
    "def analyze_single_dataset(dataset_name: str, file_path: str, \n",
    "                          algorithms: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Analyze a single dataset with all prediction tasks\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        file_path: Path to the dataset file\n",
    "        algorithms: Dictionary of available algorithms\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping task names to their results\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"ANALYZING DATASET: {dataset_name}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    df = load_and_validate_dataset(file_path, dataset_name)\n",
    "    if df is None:\n",
    "        logger.error(f\"Failed to load {dataset_name}\")\n",
    "        return {}\n",
    "    \n",
    "    # Get dataset summary\n",
    "    summary = get_dataset_summary(df, dataset_name)\n",
    "    logger.info(f\"Dataset summary: {summary['n_rows']} rows, {summary['n_columns']} columns, \"\n",
    "               f\"{summary['missing_data_percent']:.1f}% missing data\")\n",
    "    \n",
    "    # Create all prediction targets\n",
    "    all_targets = create_all_targets(df)\n",
    "    if not all_targets:\n",
    "        logger.warning(f\"No valid targets found for {dataset_name}\")\n",
    "        return {}\n",
    "    \n",
    "    # Get available features\n",
    "    feature_categories = get_feature_categories(df)\n",
    "    all_features = (feature_categories['clinical'] + \n",
    "                   feature_categories['molecular'] + \n",
    "                   feature_categories['imaging'])\n",
    "    \n",
    "    logger.info(f\"Available features: {len(all_features)} total\")\n",
    "    \n",
    "    dataset_results = {}\n",
    "    \n",
    "    # Process each target category\n",
    "    for target_category, target_info in all_targets.items():\n",
    "        logger.info(f\"\\n--- Processing {target_category.upper()} targets ---\")\n",
    "        \n",
    "        target_data = target_info['data']\n",
    "        targets = target_info['targets']\n",
    "        descriptions = target_info['descriptions']\n",
    "        \n",
    "        # Process each specific target\n",
    "        for target, description in zip(targets, descriptions):\n",
    "            logger.info(f\"\\nProcessing target: {description}\")\n",
    "            \n",
    "            # Select appropriate features (avoid data leakage)\n",
    "            safe_features = select_features_for_target(all_features, target)\n",
    "            logger.info(f\"Using {len(safe_features)} features for {target}\")\n",
    "            \n",
    "            # Preprocess data\n",
    "            X, y, error_msg = preprocess_data(target_data, safe_features, target)\n",
    "            \n",
    "            if X is None:\n",
    "                logger.warning(f\"Preprocessing failed for {target}: {error_msg}\")\n",
    "                continue\n",
    "            \n",
    "            # Run prediction task\n",
    "            task_results = run_prediction_task(X, y, description, dataset_name, algorithms)\n",
    "            \n",
    "            if task_results:\n",
    "                task_key = f\"{target_category}_{target}\"\n",
    "                dataset_results[task_key] = {\n",
    "                    'task_name': description,\n",
    "                    'target_category': target_category,\n",
    "                    'target_name': target,\n",
    "                    'results': task_results,\n",
    "                    'n_samples': len(X),\n",
    "                    'n_features': X.shape[1]\n",
    "                }\n",
    "                logger.info(f\"Task {description} completed with {len(task_results)} algorithms\")\n",
    "    \n",
    "    logger.info(f\"\\nDataset {dataset_name} analysis completed: {len(dataset_results)} tasks successful\")\n",
    "    return dataset_results\n",
    "\n",
    "def run_comprehensive_analysis(datasets_to_analyze: List[str] = None) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Run comprehensive analysis across all or specified datasets\n",
    "    \n",
    "    Args:\n",
    "        datasets_to_analyze: List of dataset names to analyze, or None for all\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping dataset names to their analysis results\n",
    "    \"\"\"\n",
    "    logger.info(\"STARTING COMPREHENSIVE NEUROSURGICAL AI ANALYSIS\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Get available algorithms\n",
    "    algorithms = get_available_algorithms()\n",
    "    if not algorithms:\n",
    "        logger.error(\"No algorithms available for analysis\")\n",
    "        return {}\n",
    "    \n",
    "    # Determine which datasets to analyze\n",
    "    if datasets_to_analyze is None:\n",
    "        datasets_to_analyze = list(config.datasets.keys())\n",
    "    \n",
    "    # Filter to existing files\n",
    "    datasets_to_analyze = [ds for ds in datasets_to_analyze if file_status.get(ds, False)]\n",
    "    \n",
    "    logger.info(f\"Analyzing {len(datasets_to_analyze)} datasets with {len(algorithms)} algorithms\")\n",
    "    logger.info(f\"Datasets: {datasets_to_analyze}\")\n",
    "    logger.info(f\"Algorithms: {list(algorithms.keys())}\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    # Analyze each dataset\n",
    "    for dataset_name in datasets_to_analyze:\n",
    "        file_path = config.datasets[dataset_name]\n",
    "        \n",
    "        try:\n",
    "            dataset_results = analyze_single_dataset(dataset_name, file_path, algorithms)\n",
    "            \n",
    "            if dataset_results:\n",
    "                all_results[dataset_name] = dataset_results\n",
    "                logger.info(f\"â {dataset_name} analysis completed successfully\")\n",
    "            else:\n",
    "                logger.warning(f\"â {dataset_name} analysis produced no results\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"â {dataset_name} analysis failed: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"\\nCOMPREHENSIVE ANALYSIS COMPLETED\")\n",
    "    logger.info(f\"Successfully analyzed: {len(all_results)}/{len(datasets_to_analyze)} datasets\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"Main analysis pipeline functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Results Analysis and Reporting Functions\n",
    "\n",
    "def summarize_results(all_results: Dict[str, Dict[str, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary of all results\n",
    "    \n",
    "    Args:\n",
    "        all_results: Complete results from all datasets\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing summary statistics\n",
    "    \"\"\"\n",
    "    if not all_results:\n",
    "        return {'error': 'No results to summarize'}\n",
    "    \n",
    "    summary = {\n",
    "        'total_datasets': len(all_results),\n",
    "        'total_tasks': 0,\n",
    "        'total_algorithm_runs': 0,\n",
    "        'best_performers': {},\n",
    "        'algorithm_performance': {},\n",
    "        'task_performance': {},\n",
    "        'dataset_performance': {}\n",
    "    }\n",
    "    \n",
    "    all_algorithm_results = []\n",
    "    \n",
    "    # Collect all results\n",
    "    for dataset_name, dataset_results in all_results.items():\n",
    "        dataset_aucs = []\n",
    "        \n",
    "        for task_name, task_info in dataset_results.items():\n",
    "            summary['total_tasks'] += 1\n",
    "            task_results = task_info['results']\n",
    "            \n",
    "            for algorithm_name, algorithm_results in task_results.items():\n",
    "                summary['total_algorithm_runs'] += 1\n",
    "                \n",
    "                result_entry = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'task': task_info['task_name'],\n",
    "                    'algorithm': algorithm_name,\n",
    "                    'auc': algorithm_results['auc'],\n",
    "                    'accuracy': algorithm_results['accuracy'],\n",
    "                    'cv_auc_mean': algorithm_results['cv_auc_mean'],\n",
    "                    'cv_auc_std': algorithm_results['cv_auc_std'],\n",
    "                    'cv_stability': algorithm_results['cv_stability']\n",
    "                }\n",
    "                all_algorithm_results.append(result_entry)\n",
    "                dataset_aucs.append(algorithm_results['auc'])\n",
    "        \n",
    "        # Dataset-level performance\n",
    "        if dataset_aucs:\n",
    "            summary['dataset_performance'][dataset_name] = {\n",
    "                'mean_auc': np.mean(dataset_aucs),\n",
    "                'std_auc': np.std(dataset_aucs),\n",
    "                'max_auc': np.max(dataset_aucs),\n",
    "                'n_results': len(dataset_aucs)\n",
    "            }\n",
    "    \n",
    "    if not all_algorithm_results:\n",
    "        return summary\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(all_algorithm_results)\n",
    "    \n",
    "    # Algorithm performance summary\n",
    "    algorithm_stats = results_df.groupby('algorithm').agg({\n",
    "        'auc': ['mean', 'std', 'max', 'count'],\n",
    "        'cv_auc_mean': ['mean', 'std'],\n",
    "        'accuracy': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    summary['algorithm_performance'] = algorithm_stats.to_dict()\n",
    "    \n",
    "    # Task performance summary\n",
    "    task_stats = results_df.groupby('task').agg({\n",
    "        'auc': ['mean', 'std', 'max', 'count']\n",
    "    }).round(4)\n",
    "    \n",
    "    summary['task_performance'] = task_stats.to_dict()\n",
    "    \n",
    "    # Find best performers\n",
    "    summary['best_performers'] = {\n",
    "        'highest_auc': results_df.loc[results_df['auc'].idxmax()].to_dict(),\n",
    "        'most_stable': results_df.loc[results_df['cv_auc_std'].idxmin()].to_dict(),\n",
    "        'best_cv_performance': results_df.loc[results_df['cv_auc_mean'].idxmax()].to_dict()\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_results_table(all_results: Dict[str, Dict[str, Dict[str, Any]]], \n",
    "                          sort_by: str = 'auc') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive results table\n",
    "    \n",
    "    Args:\n",
    "        all_results: Complete results from all datasets\n",
    "        sort_by: Column to sort by ('auc', 'accuracy', 'cv_auc_mean')\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all results\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for dataset_name, dataset_results in all_results.items():\n",
    "        for task_name, task_info in dataset_results.items():\n",
    "            task_results = task_info['results']\n",
    "            \n",
    "            for algorithm_name, algorithm_results in task_results.items():\n",
    "                row = {\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Task': task_info['task_name'],\n",
    "                    'Algorithm': algorithm_name,\n",
    "                    'AUC': algorithm_results['auc'],\n",
    "                    'Accuracy': algorithm_results['accuracy'],\n",
    "                    'Sensitivity': algorithm_results['sensitivity'],\n",
    "                    'Specificity': algorithm_results['specificity'],\n",
    "                    'PPV': algorithm_results['ppv'],\n",
    "                    'NPV': algorithm_results['npv'],\n",
    "                    'F1-Score': algorithm_results['f1_score'],\n",
    "                    'CV_AUC_Mean': algorithm_results['cv_auc_mean'],\n",
    "                    'CV_AUC_Std': algorithm_results['cv_auc_std'],\n",
    "                    'CV_Stability': algorithm_results['cv_stability'],\n",
    "                    'N_Samples': task_info['n_samples'],\n",
    "                    'N_Features': task_info['n_features'],\n",
    "                    'Scaling_Used': algorithm_results['scaling_used']\n",
    "                }\n",
    "                rows.append(row)\n",
    "    \n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Sort by specified column\n",
    "    if sort_by in df.columns:\n",
    "        df = df.sort_values(sort_by, ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_summary_report(all_results: Dict[str, Dict[str, Dict[str, Any]]]):\n",
    "    \"\"\"\n",
    "    Print a comprehensive summary report to console\n",
    "    \n",
    "    Args:\n",
    "        all_results: Complete results from all datasets\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE NEUROSURGICAL AI ANALYSIS REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"No results to report.\")\n",
    "        return\n",
    "    \n",
    "    summary = summarize_results(all_results)\n",
    "    \n",
    "    # Executive Summary\n",
    "    print(f\"\\nEXECUTIVE SUMMARY:\")\n",
    "    print(f\"â¢ Analyzed {summary['total_datasets']} datasets\")\n",
    "    print(f\"â¢ Completed {summary['total_tasks']} prediction tasks\")\n",
    "    print(f\"â¢ Ran {summary['total_algorithm_runs']} algorithm evaluations\")\n",
    "    \n",
    "    # Best Performers\n",
    "    print(f\"\\nBEST PERFORMERS:\")\n",
    "    if 'highest_auc' in summary['best_performers']:\n",
    "        best = summary['best_performers']['highest_auc']\n",
    "        print(f\"â¢ Highest AUC: {best['algorithm']} on {best['task']} ({best['dataset']}) - AUC: {best['auc']:.3f}\")\n",
    "    \n",
    "    if 'most_stable' in summary['best_performers']:\n",
    "        stable = summary['best_performers']['most_stable']\n",
    "        print(f\"â¢ Most Stable: {stable['algorithm']} on {stable['task']} ({stable['dataset']}) - CV Std: {stable['cv_auc_std']:.3f}\")\n",
    "    \n",
    "    if 'best_cv_performance' in summary['best_performers']:\n",
    "        cv_best = summary['best_performers']['best_cv_performance']\n",
    "        print(f\"â¢ Best CV Performance: {cv_best['algorithm']} on {cv_best['task']} ({cv_best['dataset']}) - CV AUC: {cv_best['cv_auc_mean']:.3f}\")\n",
    "    \n",
    "    # Dataset Performance\n",
    "    print(f\"\\nDATASET PERFORMANCE:\")\n",
    "    for dataset, performance in summary['dataset_performance'].items():\n",
    "        print(f\"â¢ {dataset}: Mean AUC {performance['mean_auc']:.3f} Â± {performance['std_auc']:.3f} \"\n",
    "              f\"(Max: {performance['max_auc']:.3f}, N={performance['n_results']})\")\n",
    "    \n",
    "    # Algorithm Rankings\n",
    "    print(f\"\\nALGORITHM RANKINGS (by mean AUC):\")\n",
    "    results_df = generate_results_table(all_results)\n",
    "    if not results_df.empty:\n",
    "        algo_rankings = results_df.groupby('Algorithm')['AUC'].agg(['mean', 'std', 'count']).sort_values('mean', ascending=False)\n",
    "        for i, (algorithm, stats) in enumerate(algo_rankings.iterrows(), 1):\n",
    "            print(f\"{i}. {algorithm}: {stats['mean']:.3f} Â± {stats['std']:.3f} (N={int(stats['count'])})\")\n",
    "    \n",
    "    # Task Difficulty\n",
    "    print(f\"\\nTASK DIFFICULTY (by mean AUC, lower = harder):\")\n",
    "    if not results_df.empty:\n",
    "        task_difficulty = results_df.groupby('Task')['AUC'].agg(['mean', 'std', 'count']).sort_values('mean', ascending=True)\n",
    "        for i, (task, stats) in enumerate(task_difficulty.iterrows(), 1):\n",
    "            print(f\"{i}. {task}: {stats['mean']:.3f} Â± {stats['std']:.3f} (N={int(stats['count'])})\")\n",
    "\n",
    "def save_results_to_files(all_results: Dict[str, Dict[str, Dict[str, Any]]], output_dir: str = \".\"):\n",
    "    \"\"\"\n",
    "    Save results to CSV and text files\n",
    "    \n",
    "    Args:\n",
    "        all_results: Complete results from all datasets\n",
    "        output_dir: Directory to save files\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    if not all_results:\n",
    "        logger.warning(\"No results to save\")\n",
    "        return\n",
    "    \n",
    "    # Save comprehensive results table\n",
    "    results_df = generate_results_table(all_results)\n",
    "    if not results_df.empty:\n",
    "        csv_path = output_path / \"neurosurgical_ai_comprehensive_results.csv\"\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"Comprehensive results saved to: {csv_path}\")\n",
    "    \n",
    "    # Save summary report\n",
    "    summary = summarize_results(all_results)\n",
    "    txt_path = output_path / \"neurosurgical_ai_summary_report.txt\"\n",
    "    \n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"NEUROSURGICAL AI ANALYSIS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Analysis Overview:\\n\")\n",
    "        f.write(f\"â¢ Total datasets analyzed: {summary['total_datasets']}\\n\")\n",
    "        f.write(f\"â¢ Total prediction tasks: {summary['total_tasks']}\\n\")\n",
    "        f.write(f\"â¢ Total algorithm evaluations: {summary['total_algorithm_runs']}\\n\\n\")\n",
    "        \n",
    "        if 'best_performers' in summary:\n",
    "            f.write(\"Best Performers:\\n\")\n",
    "            for category, performer in summary['best_performers'].items():\n",
    "                f.write(f\"â¢ {category}: {performer['algorithm']} - {performer.get('auc', 'N/A')}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"Dataset Performance Summary:\\n\")\n",
    "        for dataset, perf in summary['dataset_performance'].items():\n",
    "            f.write(f\"â¢ {dataset}: {perf['mean_auc']:.3f} Â± {perf['std_auc']:.3f}\\n\")\n",
    "    \n",
    "    logger.info(f\"Summary report saved to: {txt_path}\")\n",
    "    \n",
    "    # Save individual dataset reports\n",
    "    for dataset_name, dataset_results in all_results.items():\n",
    "        dataset_df = results_df[results_df['Dataset'] == dataset_name]\n",
    "        if not dataset_df.empty:\n",
    "            dataset_csv_path = output_path / f\"{dataset_name}_detailed_results.csv\"\n",
    "            dataset_df.to_csv(dataset_csv_path, index=False)\n",
    "            logger.info(f\"{dataset_name} detailed results saved to: {dataset_csv_path}\")\n",
    "\n",
    "def analyze_algorithm_strengths(all_results: Dict[str, Dict[str, Dict[str, Any]]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Analyze which algorithms perform best on which types of tasks\n",
    "    \n",
    "    Args:\n",
    "        all_results: Complete results from all datasets\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with algorithm strength analysis\n",
    "    \"\"\"\n",
    "    results_df = generate_results_table(all_results)\n",
    "    if results_df.empty:\n",
    "        return {}\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    # Performance by task type\n",
    "    for algorithm in results_df['Algorithm'].unique():\n",
    "        algo_data = results_df[results_df['Algorithm'] == algorithm]\n",
    "        \n",
    "        task_performance = algo_data.groupby('Task').agg({\n",
    "            'AUC': ['mean', 'std', 'count'],\n",
    "            'CV_AUC_Mean': ['mean', 'std'],\n",
    "            'CV_Stability': lambda x: (x == 'HIGH').sum() / len(x)\n",
    "        }).round(4)\n",
    "        \n",
    "        # Find best and worst tasks for this algorithm\n",
    "        task_means = algo_data.groupby('Task')['AUC'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        analysis[algorithm] = {\n",
    "            'overall_mean_auc': algo_data['AUC'].mean(),\n",
    "            'overall_std_auc': algo_data['AUC'].std(),\n",
    "            'best_task': task_means.index[0] if len(task_means) > 0 else None,\n",
    "            'best_task_auc': task_means.iloc[0] if len(task_means) > 0 else None,\n",
    "            'worst_task': task_means.index[-1] if len(task_means) > 0 else None,\n",
    "            'worst_task_auc': task_means.iloc[-1] if len(task_means) > 0 else None,\n",
    "            'stability_rate': (algo_data['CV_Stability'] == 'HIGH').sum() / len(algo_data),\n",
    "            'task_performance': task_performance.to_dict() if not task_performance.empty else {}\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def generate_clinical_recommendations(all_results: Dict[str, Dict[str, Dict[str, Any]]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate clinical recommendations based on analysis results\n",
    "    \n",
    "    Args:\n",
    "        all_results: Complete results from all datasets\n",
    "        \n",
    "    Returns:\n",
    "        List of clinical recommendation strings\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    if not all_results:\n",
    "        return [\"No results available for generating recommendations.\"]\n",
    "    \n",
    "    results_df = generate_results_table(all_results)\n",
    "    if results_df.empty:\n",
    "        return [\"No valid results for generating recommendations.\"]\n",
    "    \n",
    "    # Find consistently high-performing algorithms\n",
    "    algo_performance = results_df.groupby('Algorithm')['AUC'].agg(['mean', 'std', 'count'])\n",
    "    top_algorithms = algo_performance[algo_performance['mean'] > 0.7].sort_values('mean', ascending=False)\n",
    "    \n",
    "    if not top_algorithms.empty:\n",
    "        best_algo = top_algorithms.index[0]\n",
    "        best_auc = top_algorithms.loc[best_algo, 'mean']\n",
    "        recommendations.append(f\"Primary recommendation: {best_algo} shows the best overall performance \"\n",
    "                             f\"with mean AUC of {best_auc:.3f} across tasks.\")\n",
    "    \n",
    "    # Identify most predictable outcomes\n",
    "    task_performance = results_df.groupby('Task')['AUC'].agg(['mean', 'std', 'count'])\n",
    "    easiest_tasks = task_performance[task_performance['mean'] > 0.75].sort_values('mean', ascending=False)\n",
    "    \n",
    "    if not easiest_tasks.empty:\n",
    "        recommendations.append(f\"Most predictable outcomes: {', '.join(easiest_tasks.index[:3])} show \"\n",
    "                             f\"consistently high prediction accuracy.\")\n",
    "    \n",
    "    # Identify challenging prediction tasks\n",
    "    difficult_tasks = task_performance[task_performance['mean'] < 0.65].sort_values('mean', ascending=True)\n",
    "    \n",
    "    if not difficult_tasks.empty:\n",
    "        recommendations.append(f\"Challenging predictions: {', '.join(difficult_tasks.index[:3])} may require \"\n",
    "                             f\"additional feature engineering or larger datasets.\")\n",
    "    \n",
    "    # Sample size recommendations\n",
    "    sample_size_analysis = results_df.groupby('Task').agg({'N_Samples': 'mean', 'AUC': 'mean'})\n",
    "    correlation = sample_size_analysis['N_Samples'].corr(sample_size_analysis['AUC'])\n",
    "    \n",
    "    if correlation > 0.3:\n",
    "        recommendations.append(\"Larger sample sizes appear to improve prediction performance. \"\n",
    "                             \"Consider expanding datasets for better results.\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "print(\"Results analysis and reporting functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Main Execution Cell\n",
    "\n",
    "def main_analysis(datasets_to_run: List[str] = None, save_results: bool = True, \n",
    "                 output_directory: str = \"./results\") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Main function to run the complete analysis pipeline\n",
    "    \n",
    "    Args:\n",
    "        datasets_to_run: List of dataset names to analyze (None for all available)\n",
    "        save_results: Whether to save results to files\n",
    "        output_directory: Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        Complete analysis results\n",
    "    \"\"\"\n",
    "    print(\"Starting Neurosurgical AI Analysis Pipeline...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Run comprehensive analysis\n",
    "    all_results = run_comprehensive_analysis(datasets_to_run)\n",
    "    \n",
    "    if not all_results:\n",
    "        logger.error(\"No results generated. Check dataset availability and configuration.\")\n",
    "        return {}\n",
    "    \n",
    "    # Step 2: Print summary report\n",
    "    print_summary_report(all_results)\n",
    "    \n",
    "    # Step 3: Generate additional analyses\n",
    "    print(\"\\nGENERATING DETAILED ANALYSES...\")\n",
    "    \n",
    "    # Algorithm strength analysis\n",
    "    algorithm_analysis = analyze_algorithm_strengths(all_results)\n",
    "    if algorithm_analysis:\n",
    "        print(\"\\nALGORITHM STRENGTH ANALYSIS:\")\n",
    "        for algorithm, analysis in algorithm_analysis.items():\n",
    "            print(f\"â¢ {algorithm}: Best at {analysis['best_task']} \"\n",
    "                  f\"(AUC: {analysis['best_task_auc']:.3f}), \"\n",
    "                  f\"Overall: {analysis['overall_mean_auc']:.3f}\")\n",
    "    \n",
    "    # Clinical recommendations\n",
    "    recommendations = generate_clinical_recommendations(all_results)\n",
    "    if recommendations:\n",
    "        print(\"\\nCLINICAL RECOMMENDATIONS:\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "    \n",
    "    # Step 4: Save results if requested\n",
    "    if save_results:\n",
    "        print(f\"\\nSAVING RESULTS TO: {output_directory}\")\n",
    "        save_results_to_files(all_results, output_directory)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def run_single_dataset_analysis(dataset_name: str, save_results: bool = True) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Analyze a single dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset to analyze\n",
    "        save_results: Whether to save results\n",
    "        \n",
    "    Returns:\n",
    "        Analysis results for the dataset\n",
    "    \"\"\"\n",
    "    if dataset_name not in config.datasets:\n",
    "        logger.error(f\"Dataset '{dataset_name}' not found in configuration\")\n",
    "        return {}\n",
    "    \n",
    "    if not file_status.get(dataset_name, False):\n",
    "        logger.error(f\"Dataset file for '{dataset_name}' not found\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Analyzing single dataset: {dataset_name}\")\n",
    "    results = main_analysis([dataset_name], save_results, f\"./results_{dataset_name}\")\n",
    "    \n",
    "    return results.get(dataset_name, {})\n",
    "\n",
    "def quick_test_analysis(n_samples: int = 100) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Run a quick test analysis with limited data for testing purposes\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to use for testing\n",
    "        \n",
    "    Returns:\n",
    "        Test analysis results\n",
    "    \"\"\"\n",
    "    print(f\"Running quick test analysis with {n_samples} samples...\")\n",
    "    \n",
    "    # Temporarily modify config for testing\n",
    "    original_min_sample_size = config.min_sample_size\n",
    "    config.min_sample_size = min(10, n_samples // 5)\n",
    "    \n",
    "    try:\n",
    "        # Get first available dataset\n",
    "        available_datasets = [ds for ds, status in file_status.items() if status]\n",
    "        if not available_datasets:\n",
    "            logger.error(\"No datasets available for testing\")\n",
    "            return {}\n",
    "        \n",
    "        test_dataset = available_datasets[0]\n",
    "        logger.info(f\"Using {test_dataset} for quick test\")\n",
    "        \n",
    "        # Load and sample data\n",
    "        file_path = config.datasets[test_dataset]\n",
    "        df = load_and_validate_dataset(file_path, test_dataset)\n",
    "        \n",
    "        if df is None or len(df) < n_samples:\n",
    "            logger.error(\"Insufficient data for testing\")\n",
    "            return {}\n",
    "        \n",
    "        # Sample data\n",
    "        df_sample = df.sample(n=min(n_samples, len(df)), random_state=42)\n",
    "        \n",
    "        # Temporarily save sample\n",
    "        sample_path = f\"./temp_sample_{test_dataset}.csv\"\n",
    "        df_sample.to_csv(sample_path, index=False)\n",
    "        \n",
    "        # Update config temporarily\n",
    "        original_path = config.datasets[test_dataset]\n",
    "        config.datasets[test_dataset] = sample_path\n",
    "        \n",
    "        # Run analysis\n",
    "        results = main_analysis([test_dataset], save_results=False)\n",
    "        \n",
    "        # Cleanup\n",
    "        config.datasets[test_dataset] = original_path\n",
    "        Path(sample_path).unlink(missing_ok=True)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    finally:\n",
    "        # Restore original config\n",
    "        config.min_sample_size = original_min_sample_size\n",
    "\n",
    "# Example usage functions\n",
    "def example_full_analysis():\n",
    "    \"\"\"Example: Run full analysis on all datasets\"\"\"\n",
    "    return main_analysis()\n",
    "\n",
    "def example_specific_datasets():\n",
    "    \"\"\"Example: Run analysis on specific datasets\"\"\"\n",
    "    datasets_to_analyze = ['ConvNext', 'ViT']  # Modify as needed\n",
    "    return main_analysis(datasets_to_analyze)\n",
    "\n",
    "def example_single_dataset():\n",
    "    \"\"\"Example: Analyze just one dataset\"\"\"\n",
    "    return run_single_dataset_analysis('ConvNext')  # Modify dataset name as needed\n",
    "\n",
    "def example_quick_test():\n",
    "    \"\"\"Example: Quick test with small sample\"\"\"\n",
    "    return quick_test_analysis(n_samples=50)\n",
    "\n",
    "print(\"Main execution functions defined successfully!\")\n",
    "print(\"\\nAvailable execution options:\")\n",
    "print(\"1. main_analysis() - Run full analysis\")\n",
    "print(\"2. run_single_dataset_analysis('dataset_name') - Analyze one dataset\")\n",
    "print(\"3. quick_test_analysis(n_samples=100) - Quick test\")\n",
    "print(\"4. example_* functions for common use cases\")\n",
    "print(\"\\nReady to run analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cells 1-8 first to define all functions\n",
    "# Run cell 9 to get execution options\n",
    "# Execute analysis using one of these approaches:\n",
    "\n",
    "# Option 1: Full analysis\n",
    "results = main_analysis()\n",
    "\n",
    "# Option 2: Specific datasets  \n",
    "results = main_analysis(['ConvNext', 'ViT'])\n",
    "\n",
    "# Option 3: Single dataset\n",
    "results = run_single_dataset_analysis('ConvNext')\n",
    "\n",
    "# Option 4: Quick test\n",
    "results = quick_test_analysis(n_samples=100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
