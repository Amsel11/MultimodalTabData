{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_executive_summary(self):\n",
    "        \"\"\"Generate executive summary\"\"\"\n",
    "        print(\"\\nEXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        total_tests = 0\n",
    "        excellent_tests = 0\n",
    "        good_tests = 0\n",
    "        \n",
    "        all_aucs = []\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    total_tests += 1\n",
    "                    auc = result['auc']\n",
    "                    all_aucs.append(auc)\n",
    "                    \n",
    "                    if auc >= 0.85:\n",
    "                        excellent_tests += 1\n",
    "                    elif auc >= 0.75:\n",
    "                        good_tests += 1\n",
    "        \n",
    "        if all_aucs:\n",
    "            mean_auc = np.mean(all_aucs)\n",
    "            max_auc = np.max(all_aucs)\n",
    "            \n",
    "            print(f\"PERFORMANCE OVERVIEW:\")\n",
    "            print(f\"   Total algorithm-task combinations: {total_tests}\")\n",
    "            print(f\"   Mean AUC across all tests: {mean_auc:.3f}\")\n",
    "            print(f\"   Best AUC achieved: {max_auc:.3f}\")\n",
    "            print(f\"   Excellent performance (AUC >= 0.85): {excellent_tests}/{total_tests} ({excellent_tests/total_tests*100:.1f}%)\")\n",
    "            print(f\"   Good+ performance (AUC >= 0.75): {good_tests+excellent_tests}/{total_tests} ({(good_tests+excellent_tests)/total_tests*100:.1f}%)\")\n",
    "            \n",
    "            # Clinical readiness assessment\n",
    "            if excellent_tests > 0:\n",
    "                print(f\"   CLINICAL DEPLOYMENT: {excellent_tests} combinations ready for validation\")\n",
    "            if max_auc >= 0.90:\n",
    "                print(f\"   PUBLICATION READY: Exceptional results achieved\")\n",
    "            elif max_auc >= 0.80:\n",
    "                print(f\"   PUBLICATION READY: Strong results achieved\")\n",
    "\n",
    "def _generate_detailed_results_table(self):\n",
    "        \"\"\"Generate detailed results table\"\"\"\n",
    "        print(f\"\\nDETAILED RESULTS TABLE\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Header\n",
    "        print(f\"{'CNN':<20} {'Task':<25} {'Algorithm':<15} {'AUC':<8} {'Acc':<8} {'Sens':<8} {'Spec':<8} {'Status':<15}\")\n",
    "        print(\"-\" * 120)\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                task_name = task_data['task_name']\n",
    "                \n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    auc = result['auc']\n",
    "                    acc = result['accuracy']\n",
    "                    sens = result['sensitivity']\n",
    "                    spec = result['specificity']\n",
    "                    \n",
    "                    # Status based on AUC\n",
    "                    if auc >= 0.85:\n",
    "                        status = \"EXCELLENT\"\n",
    "                    elif auc >= 0.75:\n",
    "                        status = \"STRONG\"\n",
    "                    elif auc >= 0.65:\n",
    "                        status = \"GOOD\"\n",
    "                    else:\n",
    "                        status = \"MODERATE\"\n",
    "                    \n",
    "                    print(f\"{cnn_name:<20} {task_name:<25} {alg_name:<15} {auc:<8.3f} {acc:<8.3f} {sens:<8.3f} {spec:<8.3f} {status:<15}\")\n",
    "\n",
    "def _generate_best_performers_analysis(self):\n",
    "        \"\"\"Generate best performers analysis\"\"\"\n",
    "        print(f\"\\nBEST PERFORMERS BY TASK\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Find best performer for each task across all CNNs\n",
    "        task_best = {}\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                task_name = task_data['task_name']\n",
    "                \n",
    "                if task_name not in task_best:\n",
    "                    task_best[task_name] = {'auc': 0, 'cnn': '', 'algorithm': '', 'result': None}\n",
    "                \n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    if result['auc'] > task_best[task_name]['auc']:\n",
    "                        task_best[task_name] = {\n",
    "                            'auc': result['auc'],\n",
    "                            'cnn': cnn_name,\n",
    "                            'algorithm': alg_name,\n",
    "                            'result': result\n",
    "                        }\n",
    "        \n",
    "        for task_name, best in task_best.items():\n",
    "            auc = best['auc']\n",
    "            status = \"DEPLOYMENT READY\" if auc >= 0.85 else \"PROMISING\" if auc >= 0.75 else \"NEEDS WORK\"\n",
    "            print(f\"{task_name:<30}: {best['cnn']} + {best['algorithm']} (AUC = {auc:.3f}) {status}\")\n",
    "\n",
    "def _generate_validation_summary(self):\n",
    "        \"\"\"Generate validation summary\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           accuracy_score, roc_curve, precision_recall_curve, auc)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for optional dependencies\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"⚠️ XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "    import torch\n",
    "    TABNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TABNET_AVAILABLE = False\n",
    "    print(\"⚠️ TabNet not available. Install with: pip install pytorch-tabnet torch\")\n",
    "\n",
    "class NeurosurgicalAIAnalyzer:\n",
    "    \"\"\"Comprehensive AI analysis system for neurosurgical outcome prediction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Updated paths to match your actual file names\n",
    "        self.datasets = {\n",
    "            'ConvNext': '/Users/joi263/Documents/MultimodalTabData/data/convnext_data/convnext_cleaned_patient_features_separate_256d.csv',\n",
    "            'ViT': '/Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_cleaned_patient_features_separate_256d.csv',\n",
    "            'ResNet50_Pretrained': '/Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_cleaned_patient_features_separate_256d.csv',\n",
    "            'ResNet50_ImageNet': '/Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_cleaned_patient_features_separate_256d.csv',\n",
    "            'EfficientNet': '/Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_cleaned_patient_features_separate_256d.csv'\n",
    "        }\n",
    "        self.results = {}\n",
    "        self.validation_results = {}\n",
    "        \n",
    "        # Print file paths for verification\n",
    "        print(\"CHECKING DATA FILE PATHS:\")\n",
    "        print(\"=\"*50)\n",
    "        import os\n",
    "        for cnn_name, file_path in self.datasets.items():\n",
    "            exists = os.path.exists(file_path)\n",
    "            status = \"EXISTS\" if exists else \"NOT FOUND\"\n",
    "            print(f\"{cnn_name:<20}: {status}\")\n",
    "            if not exists:\n",
    "                print(f\"  Expected: {file_path}\")\n",
    "        print(\"=\"*50)\n",
    "        print()\n",
    "        \n",
    "        # Count how many files exist\n",
    "        existing_files = sum(1 for path in self.datasets.values() if os.path.exists(path))\n",
    "        print(f\"Found {existing_files}/{len(self.datasets)} data files\")\n",
    "        \n",
    "        if existing_files == 0:\n",
    "            print(\"ERROR: No data files found!\")\n",
    "            print(\"Please verify the file paths match your actual file locations.\")\n",
    "        elif existing_files < len(self.datasets):\n",
    "            print(f\"WARNING: Only {existing_files} out of {len(self.datasets)} files found.\")\n",
    "            print(\"Analysis will proceed with available datasets.\")\n",
    "        else:\n",
    "            print(\"SUCCESS: All data files found!\")\n",
    "        print()\n",
    "        \n",
    "    def get_ml_algorithms(self):\n",
    "        \"\"\"Initialize all available ML algorithms with optimized parameters\"\"\"\n",
    "        algorithms = {}\n",
    "        \n",
    "        # 1. TabPFN (always available) - Optimized for small biomedical datasets\n",
    "        algorithms['TabPFN'] = {\n",
    "            'model': TabPFNClassifier(device='cpu'),  # Only use valid parameters\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Transformer-based Few-Shot Learning'\n",
    "        }\n",
    "        \n",
    "        # 2. XGBoost (if available) - Tuned for biomedical data\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            algorithms['XGBoost'] = {\n",
    "                'model': xgb.XGBClassifier(\n",
    "                    n_estimators=300,  # Increased for better performance\n",
    "                    max_depth=4,       # Reduced to prevent overfitting on small datasets\n",
    "                    learning_rate=0.05, # Lower for better generalization\n",
    "                    subsample=0.8,     # Add regularization\n",
    "                    colsample_bytree=0.8,\n",
    "                    min_child_weight=3, # Prevent overfitting\n",
    "                    reg_alpha=1,       # L1 regularization\n",
    "                    reg_lambda=1,      # L2 regularization\n",
    "                    random_state=42,\n",
    "                    eval_metric='logloss',\n",
    "                    use_label_encoder=False  # Suppress warnings\n",
    "                ),\n",
    "                'needs_scaling': False,\n",
    "                'description': 'Optimized Gradient Boosting'\n",
    "            }\n",
    "        \n",
    "        # 3. TabNet (if available) - Tuned for tabular biomedical data\n",
    "        if TABNET_AVAILABLE:\n",
    "            algorithms['TabNet'] = {\n",
    "                'model': TabNetClassifier(\n",
    "                    n_d=64, n_a=64,    # Increased capacity\n",
    "                    n_steps=5,         # More decision steps\n",
    "                    gamma=1.5,         # Stronger feature selection\n",
    "                    lambda_sparse=1e-4, # Lighter sparsity penalty\n",
    "                    optimizer_fn=torch.optim.Adam,\n",
    "                    optimizer_params=dict(lr=0.01, weight_decay=1e-5),\n",
    "                    mask_type=\"entmax\",\n",
    "                    scheduler_params={\"step_size\": 20, \"gamma\": 0.8},\n",
    "                    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                    verbose=0,\n",
    "                    seed=42\n",
    "                ),\n",
    "                'needs_scaling': True,  # TabNet benefits from scaling\n",
    "                'description': 'Optimized Attention-based Neural Network'\n",
    "            }\n",
    "        \n",
    "        # 4. Random Forest (always available) - Tuned for biomedical features\n",
    "        algorithms['RandomForest'] = {\n",
    "            'model': RandomForestClassifier(\n",
    "                n_estimators=500,   # Increased for stability\n",
    "                max_depth=8,        # Moderate depth to prevent overfitting\n",
    "                min_samples_split=10, # Higher to prevent overfitting\n",
    "                min_samples_leaf=5,   # Higher to ensure leaf reliability\n",
    "                max_features='sqrt',  # Good default for classification\n",
    "                bootstrap=True,\n",
    "                oob_score=True,     # Out-of-bag validation\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1           # Use all cores\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Optimized Ensemble Decision Trees'\n",
    "        }\n",
    "        \n",
    "        # 5. Logistic Regression (always available) - Tuned with regularization\n",
    "        algorithms['LogisticRegression'] = {\n",
    "            'model': LogisticRegression(\n",
    "                penalty='elasticnet',  # Combines L1 and L2 regularization\n",
    "                l1_ratio=0.5,         # Balance between L1 and L2\n",
    "                C=0.1,                # Strong regularization for small datasets\n",
    "                solver='saga',        # Supports elasticnet\n",
    "                max_iter=2000,        # More iterations for convergence\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'needs_scaling': True,  # CRITICAL for logistic regression\n",
    "            'description': 'Regularized Linear Model with ElasticNet'\n",
    "        }\n",
    "        \n",
    "        # 6. Support Vector Machine - Added as bonus strong performer\n",
    "        algorithms['SVM'] = {\n",
    "            'model': SVC(\n",
    "                kernel='rbf',\n",
    "                C=1.0,                # Balanced regularization\n",
    "                gamma='scale',        # Adaptive gamma\n",
    "                probability=True,     # Enable probability estimates\n",
    "                random_state=42,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'needs_scaling': True,    # CRITICAL for SVM\n",
    "            'description': 'Support Vector Machine with RBF Kernel'\n",
    "        }\n",
    "        \n",
    "        return algorithms\n",
    "\n",
    "    def create_all_targets(self, df):\n",
    "        \"\"\"Create all prediction targets: mortality, tumor classification, IDH, MGMT\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"CREATING ALL PREDICTION TARGETS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        targets_data = {}\n",
    "        \n",
    "        # ============================================================\n",
    "        # MORTALITY TARGETS\n",
    "        # ============================================================\n",
    "        print(\"MORTALITY TARGETS:\")\n",
    "        survival_data = df[df['survival'].notna() & df['patient_status'].notna()].copy()\n",
    "        \n",
    "        if len(survival_data) > 0:\n",
    "            survival_data['mortality_6mo'] = ((survival_data['patient_status'] == 2) & \n",
    "                                              (survival_data['survival'] <= 6)).astype(int)\n",
    "            survival_data['mortality_1yr'] = ((survival_data['patient_status'] == 2) & \n",
    "                                              (survival_data['survival'] <= 12)).astype(int)\n",
    "            survival_data['mortality_2yr'] = ((survival_data['patient_status'] == 2) & \n",
    "                                              (survival_data['survival'] <= 24)).astype(int)\n",
    "            \n",
    "            targets_data['mortality'] = {\n",
    "                'data': survival_data,\n",
    "                'targets': ['mortality_6mo', 'mortality_1yr', 'mortality_2yr'],\n",
    "                'descriptions': ['6-Month Mortality', '1-Year Mortality', '2-Year Mortality']\n",
    "            }\n",
    "            \n",
    "            print(f\"   Patients: {len(survival_data)}\")\n",
    "            print(f\"   6-month: {survival_data['mortality_6mo'].sum()}/{len(survival_data)} ({survival_data['mortality_6mo'].mean()*100:.1f}%)\")\n",
    "            print(f\"   1-year: {survival_data['mortality_1yr'].sum()}/{len(survival_data)} ({survival_data['mortality_1yr'].mean()*100:.1f}%)\")\n",
    "            print(f\"   2-year: {survival_data['mortality_2yr'].sum()}/{len(survival_data)} ({survival_data['mortality_2yr'].mean()*100:.1f}%)\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # TUMOR CLASSIFICATION TARGETS\n",
    "        # ============================================================\n",
    "        print(\"\\nTUMOR CLASSIFICATION TARGETS:\")\n",
    "        tumor_data = df[df['methylation_class'].notna()].copy()\n",
    "        \n",
    "        if len(tumor_data) > 0:\n",
    "            # Binary high-grade vs low-grade\n",
    "            high_grade_terms = ['glioblastoma', 'anaplastic', 'high grade', 'grade iv', 'grade 4', 'gbm']\n",
    "            tumor_data['high_grade'] = tumor_data['methylation_class'].str.lower().str.contains(\n",
    "                '|'.join(high_grade_terms), na=False\n",
    "            ).astype(int)\n",
    "            \n",
    "            targets_data['tumor'] = {\n",
    "                'data': tumor_data,\n",
    "                'targets': ['high_grade'],\n",
    "                'descriptions': ['High-Grade vs Low-Grade']\n",
    "            }\n",
    "            \n",
    "            print(f\"   Patients: {len(tumor_data)}\")\n",
    "            print(f\"   High-grade: {tumor_data['high_grade'].sum()}/{len(tumor_data)} ({tumor_data['high_grade'].mean()*100:.1f}%)\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # IDH MUTATION TARGETS\n",
    "        # ============================================================\n",
    "        print(\"\\nIDH MUTATION TARGETS:\")\n",
    "        idh_data = self._create_idh_targets(df)\n",
    "        \n",
    "        if idh_data is not None and len(idh_data) > 0:\n",
    "            targets_data['idh'] = {\n",
    "                'data': idh_data,\n",
    "                'targets': ['idh_binary'],\n",
    "                'descriptions': ['IDH Mutation Status']\n",
    "            }\n",
    "            \n",
    "            print(f\"   Patients: {len(idh_data)}\")\n",
    "            print(f\"   IDH Mutant: {idh_data['idh_binary'].sum()}/{len(idh_data)} ({idh_data['idh_binary'].mean()*100:.1f}%)\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # MGMT METHYLATION TARGETS\n",
    "        # ============================================================\n",
    "        print(\"\\nMGMT METHYLATION TARGETS:\")\n",
    "        mgmt_data = self._create_mgmt_targets(df)\n",
    "        \n",
    "        if mgmt_data is not None and len(mgmt_data) > 0:\n",
    "            targets_data['mgmt'] = {\n",
    "                'data': mgmt_data,\n",
    "                'targets': ['mgmt_binary'],\n",
    "                'descriptions': ['MGMT Promoter Methylation']\n",
    "            }\n",
    "            \n",
    "            print(f\"   Patients: {len(mgmt_data)}\")\n",
    "            print(f\"   MGMT Methylated: {mgmt_data['mgmt_binary'].sum()}/{len(mgmt_data)} ({mgmt_data['mgmt_binary'].mean()*100:.1f}%)\")\n",
    "        \n",
    "        return targets_data\n",
    "\n",
    "    def _create_idh_targets(self, df):\n",
    "        \"\"\"Create IDH mutation targets with proper decoding\"\"\"\n",
    "        if 'idh_1_r132h' not in df.columns:\n",
    "            return None\n",
    "            \n",
    "        idh_data = df.copy()\n",
    "        idh_data['idh_binary'] = np.nan\n",
    "        \n",
    "        # Cross-reference with text data if available\n",
    "        if 'idh1' in df.columns:\n",
    "            text_idh = df['idh1'].astype(str).str.lower()\n",
    "            mutant_patterns = ['r132h', 'r132s', 'arg132his', 'arg132ser', 'missense', 'p.arg132']\n",
    "            is_mutant_text = text_idh.str.contains('|'.join(mutant_patterns), na=False)\n",
    "            idh_data.loc[is_mutant_text, 'idh_binary'] = 1  # Mutant\n",
    "        \n",
    "        # Apply numerical encoding (2 = mutant based on cross-reference analysis)\n",
    "        remaining_mask = idh_data['idh_binary'].isna() & idh_data['idh_1_r132h'].notna()\n",
    "        idh_data.loc[remaining_mask & (idh_data['idh_1_r132h'] == 2), 'idh_binary'] = 1  # Mutant\n",
    "        idh_data.loc[remaining_mask & (idh_data['idh_1_r132h'] == 1), 'idh_binary'] = 0  # Wildtype\n",
    "        \n",
    "        # Exclude unknown cases\n",
    "        idh_data.loc[idh_data['idh_1_r132h'] == 3, 'idh_binary'] = np.nan\n",
    "        \n",
    "        return idh_data[idh_data['idh_binary'].notna()].copy()\n",
    "\n",
    "    def _create_mgmt_targets(self, df):\n",
    "        \"\"\"Create MGMT methylation targets with correct encoding\"\"\"\n",
    "        if 'mgmt' not in df.columns:\n",
    "            return None\n",
    "            \n",
    "        mgmt_data = df[df['mgmt'].notna()].copy()\n",
    "        \n",
    "        if len(mgmt_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Correct encoding based on data dictionary:\n",
    "        # 1 = Positive (methylated), 2 = Negative (unmethylated), 3 = Non-informative\n",
    "        mgmt_data['mgmt_binary'] = np.nan\n",
    "        \n",
    "        # Set methylated cases (value = 1)\n",
    "        mgmt_data.loc[mgmt_data['mgmt'] == 1, 'mgmt_binary'] = 1  # Methylated\n",
    "        \n",
    "        # Set unmethylated cases (value = 2) \n",
    "        mgmt_data.loc[mgmt_data['mgmt'] == 2, 'mgmt_binary'] = 0  # Unmethylated\n",
    "        \n",
    "        # Exclude non-informative cases (value = 3)\n",
    "        mgmt_data.loc[mgmt_data['mgmt'] == 3, 'mgmt_binary'] = np.nan\n",
    "        \n",
    "        # Return only cases with definitive results\n",
    "        return mgmt_data[mgmt_data['mgmt_binary'].notna()].copy()\n",
    "\n",
    "    def select_features(self, df):\n",
    "        \"\"\"Select comprehensive feature set\"\"\"\n",
    "        # Clinical features\n",
    "        clinical_features = ['age', 'sex', 'race', 'ethnicity', 'gtr']\n",
    "        \n",
    "        # Molecular features (exclude target variables to prevent leakage)\n",
    "        molecular_features = ['mgmt_pyro', 'atrx', 'p53', 'braf_v600', 'h3k27m', 'gfap', 'tumor', 'hg_glioma']\n",
    "        \n",
    "        # CNN-extracted imaging features\n",
    "        image_features = [col for col in df.columns if col.startswith('feature_')]\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = clinical_features + molecular_features + image_features\n",
    "        available_features = [f for f in all_features if f in df.columns]\n",
    "        \n",
    "        return available_features\n",
    "\n",
    "    def preprocess_data(self, df, features, target_col):\n",
    "        \"\"\"Advanced preprocessing for multiple ML algorithms\"\"\"\n",
    "        data = df[features + [target_col]].copy()\n",
    "        data = data[data[target_col].notna()]\n",
    "        \n",
    "        if len(data) < 15:  # Minimum viable sample size\n",
    "            return None, None, f\"Insufficient data: {len(data)} samples\"\n",
    "        \n",
    "        # Handle categorical features\n",
    "        categorical_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "        if target_col in categorical_features:\n",
    "            categorical_features.remove(target_col)\n",
    "        \n",
    "        for col in categorical_features:\n",
    "            if col in features:\n",
    "                le = LabelEncoder()\n",
    "                data[col] = data[col].astype(str)\n",
    "                data[col] = le.fit_transform(data[col])\n",
    "        \n",
    "        # Handle missing values\n",
    "        numerical_features = [f for f in features if f in data.select_dtypes(include=[np.number]).columns]\n",
    "        \n",
    "        for col in numerical_features:\n",
    "            if data[col].isnull().sum() > 0:\n",
    "                if col.startswith('feature_'):\n",
    "                    data[col] = data[col].fillna(data[col].mean())\n",
    "                else:\n",
    "                    data[col] = data[col].fillna(data[col].median())\n",
    "        \n",
    "        # Remove features with >50% missing\n",
    "        missing_pct = data[features].isnull().mean()\n",
    "        good_features = missing_pct[missing_pct <= 0.5].index.tolist()\n",
    "        \n",
    "        if len(good_features) < len(features):\n",
    "            features = good_features\n",
    "            data = data[features + [target_col]]\n",
    "        \n",
    "        # Feature selection for computational efficiency\n",
    "        X = data[features].values\n",
    "        y = data[target_col].values\n",
    "        \n",
    "        # Check class balance\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        min_class_size = min(class_counts)\n",
    "        \n",
    "        if min_class_size < 3:\n",
    "            return None, None, f\"Class too small: minimum class has {min_class_size} samples\"\n",
    "        \n",
    "        # Feature selection (limit to 100 for computational efficiency)\n",
    "        if X.shape[1] > 100:\n",
    "            selector = SelectKBest(score_func=f_classif, k=100)\n",
    "            X = selector.fit_transform(X, y)\n",
    "        \n",
    "        return X, y, None\n",
    "\n",
    "    def train_and_evaluate_algorithm(self, X_train, X_test, y_train, y_test, algorithm_name, algorithm_config):\n",
    "        \"\"\"Train and evaluate a single algorithm with optimized preprocessing\"\"\"\n",
    "        try:\n",
    "            model = algorithm_config['model']\n",
    "            needs_scaling = algorithm_config['needs_scaling']\n",
    "            \n",
    "            # Apply robust scaling if needed\n",
    "            if needs_scaling:\n",
    "                # Use RobustScaler for biomedical data (handles outliers better than StandardScaler)\n",
    "                from sklearn.preprocessing import RobustScaler\n",
    "                scaler = RobustScaler(quantile_range=(10.0, 90.0))  # Less sensitive to outliers\n",
    "                X_train_processed = scaler.fit_transform(X_train)\n",
    "                X_test_processed = scaler.transform(X_test)\n",
    "                \n",
    "                # Handle potential scaling issues\n",
    "                if np.any(np.isnan(X_train_processed)) or np.any(np.isnan(X_test_processed)):\n",
    "                    # Fallback to StandardScaler if RobustScaler fails\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_processed = scaler.fit_transform(X_train)\n",
    "                    X_test_processed = scaler.transform(X_test)\n",
    "            else:\n",
    "                X_train_processed = X_train\n",
    "                X_test_processed = X_test\n",
    "            \n",
    "            # Special handling for different algorithms\n",
    "            if algorithm_name == 'TabNet' and TABNET_AVAILABLE:\n",
    "                # TabNet needs special training procedure\n",
    "                model.fit(\n",
    "                    X_train_processed, y_train,\n",
    "                    eval_set=[(X_test_processed, y_test)],\n",
    "                    patience=20,        # Increased patience for better convergence\n",
    "                    max_epochs=100,     # More epochs for biomedical data\n",
    "                    eval_metric=['auc'],\n",
    "                    batch_size=min(256, len(X_train)//4)  # Adaptive batch size\n",
    "                )\n",
    "                y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "            elif algorithm_name == 'XGBoost' and XGBOOST_AVAILABLE:\n",
    "                # XGBoost with standard training (early stopping varies by version)\n",
    "                try:\n",
    "                    # Try with early stopping if supported\n",
    "                    eval_set = [(X_test_processed, y_test)]\n",
    "                    model.fit(\n",
    "                        X_train_processed, y_train,\n",
    "                        eval_set=eval_set,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                except TypeError:\n",
    "                    # Fallback to standard training if early stopping not supported\n",
    "                    model.fit(X_train_processed, y_train)\n",
    "                \n",
    "                y_pred = model.predict(X_test_processed)\n",
    "                y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "                \n",
    "            else:\n",
    "                # Standard scikit-learn interface\n",
    "                model.fit(X_train_processed, y_train)\n",
    "                y_pred = model.predict(X_test_processed)\n",
    "                \n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "                else:\n",
    "                    y_pred_proba = y_pred.astype(float)\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Robust AUC calculation\n",
    "            try:\n",
    "                auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            except ValueError:\n",
    "                # Handle edge cases (e.g., all one class in test set)\n",
    "                auc = 0.5\n",
    "            \n",
    "            # Confusion matrix and clinical metrics\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Clinical metrics for binary classification\n",
    "            if cm.shape == (2, 2):\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "                ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "            else:\n",
    "                sensitivity = specificity = ppv = npv = 0\n",
    "            \n",
    "            # Additional metrics for model comparison\n",
    "            balanced_accuracy = (sensitivity + specificity) / 2\n",
    "            f1_score = 2 * (ppv * sensitivity) / (ppv + sensitivity) if (ppv + sensitivity) > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'accuracy': accuracy,\n",
    "                'balanced_accuracy': balanced_accuracy,\n",
    "                'auc': auc,\n",
    "                'sensitivity': sensitivity,\n",
    "                'specificity': specificity,\n",
    "                'ppv': ppv,\n",
    "                'npv': npv,\n",
    "                'f1_score': f1_score,\n",
    "                'confusion_matrix': cm,\n",
    "                'n_test': len(y_test),\n",
    "                'scaling_used': needs_scaling\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ {algorithm_name} failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def run_prediction_task(self, X, y, task_name, cnn_name, algorithms):\n",
    "        \"\"\"Run prediction task with cross-validation and single holdout validation\"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{task_name} - {cnn_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Single holdout split for detailed analysis\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.25, random_state=42, stratify=y\n",
    "            )\n",
    "        except:\n",
    "            # If stratification fails, try without it\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.25, random_state=42\n",
    "            )\n",
    "        \n",
    "        print(f\"DATA SPLIT:\")\n",
    "        print(f\"   Training: {len(X_train)} samples\")\n",
    "        print(f\"   Testing: {len(X_test)} samples\")\n",
    "        print(f\"   Positive rate: {y_train.mean()*100:.1f}% (train), {y_test.mean()*100:.1f}% (test)\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Test each algorithm with both holdout and cross-validation\n",
    "        for alg_name, alg_config in algorithms.items():\n",
    "            print(f\"\\nTESTING {alg_name}...\")\n",
    "            \n",
    "            # Single holdout result (for detailed metrics)\n",
    "            holdout_result = self.train_and_evaluate_algorithm(X_train, X_test, y_train, y_test, alg_name, alg_config)\n",
    "            \n",
    "            if holdout_result is None:\n",
    "                print(f\"   ERROR {alg_name}: FAILED\")\n",
    "                continue\n",
    "            \n",
    "            # Cross-validation for robustness\n",
    "            cv_result = self.cross_validate_algorithm(X, y, alg_name, alg_config)\n",
    "            \n",
    "            if cv_result is None:\n",
    "                print(f\"   WARNING {alg_name}: Cross-validation failed, using holdout only\")\n",
    "                cv_result = {\n",
    "                    'cv_auc_mean': holdout_result['auc'],\n",
    "                    'cv_auc_std': 0.0,\n",
    "                    'cv_auc_ci_lower': holdout_result['auc'],\n",
    "                    'cv_auc_ci_upper': holdout_result['auc'],\n",
    "                    'cv_accuracy_mean': holdout_result['accuracy'],\n",
    "                    'cv_accuracy_std': 0.0,\n",
    "                    'cv_folds': 1,\n",
    "                    'cv_stability': 'SINGLE_SPLIT'\n",
    "                }\n",
    "            \n",
    "            # Combine holdout and CV results\n",
    "            combined_result = {**holdout_result, **cv_result}\n",
    "            results[alg_name] = combined_result\n",
    "            \n",
    "            # Enhanced reporting with confidence intervals\n",
    "            auc_mean = cv_result['cv_auc_mean']\n",
    "            auc_std = cv_result['cv_auc_std']\n",
    "            auc_ci_lower = cv_result['cv_auc_ci_lower']\n",
    "            auc_ci_upper = cv_result['cv_auc_ci_upper']\n",
    "            stability = cv_result['cv_stability']\n",
    "            \n",
    "            print(f\"   HOLDOUT: Accuracy={holdout_result['accuracy']:.3f}, AUC={holdout_result['auc']:.3f}\")\n",
    "            print(f\"   CROSS-VAL: AUC={auc_mean:.3f} (95% CI: {auc_ci_lower:.3f}-{auc_ci_upper:.3f})\")\n",
    "            print(f\"   STABILITY: {stability}\")\n",
    "            \n",
    "            # Clinical interpretation with confidence intervals\n",
    "            if auc_ci_lower >= 0.85:\n",
    "                print(f\"       EXCELLENT clinical performance (robust across CV)\")\n",
    "            elif auc_mean >= 0.85 and auc_ci_lower >= 0.75:\n",
    "                print(f\"       EXCELLENT clinical performance (some variability)\")\n",
    "            elif auc_ci_lower >= 0.75:\n",
    "                print(f\"       STRONG clinical performance (robust across CV)\")\n",
    "            elif auc_mean >= 0.75 and auc_ci_lower >= 0.65:\n",
    "                print(f\"       STRONG clinical performance (some variability)\")\n",
    "            elif auc_ci_lower >= 0.65:\n",
    "                print(f\"       GOOD performance (robust across CV)\")\n",
    "            else:\n",
    "                print(f\"       MODERATE performance (consider more data/optimization)\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def cross_validate_algorithm(self, X, y, algorithm_name, algorithm_config, cv_folds=5):\n",
    "        \"\"\"Perform stratified cross-validation with confidence intervals\"\"\"\n",
    "        try:\n",
    "            # Create stratified k-fold\n",
    "            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "            \n",
    "            # Storage for CV results\n",
    "            cv_aucs = []\n",
    "            cv_accuracies = []\n",
    "            cv_sensitivities = []\n",
    "            cv_specificities = []\n",
    "            \n",
    "            fold_num = 0\n",
    "            for train_idx, val_idx in cv.split(X, y):\n",
    "                fold_num += 1\n",
    "                X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "                y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Train and evaluate on this fold\n",
    "                fold_result = self.train_and_evaluate_algorithm(\n",
    "                    X_train_cv, X_val_cv, y_train_cv, y_val_cv, \n",
    "                    algorithm_name, algorithm_config\n",
    "                )\n",
    "                \n",
    "                if fold_result is not None:\n",
    "                    cv_aucs.append(fold_result['auc'])\n",
    "                    cv_accuracies.append(fold_result['accuracy'])\n",
    "                    cv_sensitivities.append(fold_result['sensitivity'])\n",
    "                    cv_specificities.append(fold_result['specificity'])\n",
    "                else:\n",
    "                    # If a fold fails, record it but continue\n",
    "                    cv_aucs.append(0.5)  # Random performance\n",
    "                    cv_accuracies.append(0.5)\n",
    "                    cv_sensitivities.append(0.5)\n",
    "                    cv_specificities.append(0.5)\n",
    "            \n",
    "            # Calculate CV statistics\n",
    "            cv_aucs = np.array(cv_aucs)\n",
    "            cv_accuracies = np.array(cv_accuracies)\n",
    "            \n",
    "            # Mean and standard deviation\n",
    "            auc_mean = np.mean(cv_aucs)\n",
    "            auc_std = np.std(cv_aucs)\n",
    "            acc_mean = np.mean(cv_accuracies)\n",
    "            acc_std = np.std(cv_accuracies)\n",
    "            \n",
    "            # 95% Confidence intervals (using t-distribution for small samples)\n",
    "            from scipy import stats\n",
    "            t_critical = stats.t.ppf(0.975, df=len(cv_aucs)-1)  # 95% CI\n",
    "            auc_margin = t_critical * (auc_std / np.sqrt(len(cv_aucs)))\n",
    "            \n",
    "            auc_ci_lower = max(0.0, auc_mean - auc_margin)\n",
    "            auc_ci_upper = min(1.0, auc_mean + auc_margin)\n",
    "            \n",
    "            # Stability assessment\n",
    "            cv_of_variation = auc_std / auc_mean if auc_mean > 0 else 1.0\n",
    "            \n",
    "            if cv_of_variation < 0.05:\n",
    "                stability = \"HIGHLY STABLE\"\n",
    "            elif cv_of_variation < 0.10:\n",
    "                stability = \"STABLE\"\n",
    "            elif cv_of_variation < 0.15:\n",
    "                stability = \"MODERATE VARIABILITY\"\n",
    "            else:\n",
    "                stability = \"HIGH VARIABILITY\"\n",
    "            \n",
    "            return {\n",
    "                'cv_auc_mean': auc_mean,\n",
    "                'cv_auc_std': auc_std,\n",
    "                'cv_auc_ci_lower': auc_ci_lower,\n",
    "                'cv_auc_ci_upper': auc_ci_upper,\n",
    "                'cv_accuracy_mean': acc_mean,\n",
    "                'cv_accuracy_std': acc_std,\n",
    "                'cv_sensitivity_mean': np.mean(cv_sensitivities),\n",
    "                'cv_specificity_mean': np.mean(cv_specificities),\n",
    "                'cv_folds': cv_folds,\n",
    "                'cv_stability': stability,\n",
    "                'cv_coefficient_variation': cv_of_variation,\n",
    "                'cv_individual_aucs': cv_aucs.tolist()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Cross-validation failed for {algorithm_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _check_feature_quality(self, df):\n",
    "        \"\"\"Check feature quality and completeness\"\"\"\n",
    "        try:\n",
    "            image_features = [col for col in df.columns if col.startswith('feature_')]\n",
    "            clinical_features = ['age', 'sex', 'race', 'ethnicity']\n",
    "            \n",
    "            image_quality = len(image_features) >= 50  # Sufficient image features\n",
    "            clinical_completeness = sum(col in df.columns for col in clinical_features) >= 2\n",
    "            \n",
    "            score = (image_quality + clinical_completeness) / 2\n",
    "            \n",
    "            return {\n",
    "                'status': 'PASS' if score >= 0.5 else 'WARN',\n",
    "                'score': score,\n",
    "                'details': f\"Image features: {len(image_features)}, Clinical completeness: {clinical_completeness}\"\n",
    "            }\n",
    "        except:\n",
    "            return {'status': 'FAIL', 'score': 0, 'details': 'Feature quality check failed'}\n",
    "\n",
    "    def run_validation_checks(self, cnn_name, file_path):\n",
    "        \"\"\"Run comprehensive validation checks\"\"\"\n",
    "        print(f\"\\n🔍 VALIDATION CHECKS FOR {cnn_name}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            validation = {\n",
    "                'data_integrity': self._check_data_integrity(df),\n",
    "                'class_balance': self._check_class_balance(df),\n",
    "                'feature_quality': self._check_feature_quality(df),\n",
    "                'sample_size': self._check_sample_size(df)\n",
    "            }\n",
    "            \n",
    "            # Overall assessment\n",
    "            passed_checks = sum(1 for check in validation.values() if check['status'] == 'PASS')\n",
    "            total_checks = len(validation)\n",
    "            \n",
    "            validation['overall'] = {\n",
    "                'status': 'PASS' if passed_checks >= 3 else 'WARN',\n",
    "                'score': passed_checks / total_checks,\n",
    "                'summary': f\"{passed_checks}/{total_checks} validation checks passed\"\n",
    "            }\n",
    "            \n",
    "            return validation\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "\n",
    "    def _check_data_integrity(self, df):\n",
    "        \"\"\"Check basic data integrity\"\"\"\n",
    "        try:\n",
    "            has_survival = df['survival'].notna().sum() > 10\n",
    "            has_molecular = any(col in df.columns for col in ['mgmt', 'idh_1_r132h', 'methylation_class'])\n",
    "            has_images = any(col.startswith('feature_') for col in df.columns)\n",
    "            \n",
    "            score = sum([has_survival, has_molecular, has_images]) / 3\n",
    "            \n",
    "            return {\n",
    "                'status': 'PASS' if score >= 0.67 else 'WARN',\n",
    "                'score': score,\n",
    "                'details': f\"Survival: {has_survival}, Molecular: {has_molecular}, Images: {has_images}\"\n",
    "            }\n",
    "        except:\n",
    "            return {'status': 'FAIL', 'score': 0, 'details': 'Data integrity check failed'}\n",
    "\n",
    "    def _check_class_balance(self, df):\n",
    "        \"\"\"Check class balance across targets\"\"\"\n",
    "        try:\n",
    "            balances = []\n",
    "            \n",
    "            # Check mortality balance\n",
    "            if 'survival' in df.columns and 'patient_status' in df.columns:\n",
    "                survival_data = df[df['survival'].notna() & df['patient_status'].notna()]\n",
    "                if len(survival_data) > 0:\n",
    "                    mortality_1yr = ((survival_data['patient_status'] == 2) & \n",
    "                                   (survival_data['survival'] <= 12)).mean()\n",
    "                    balances.append(min(mortality_1yr, 1-mortality_1yr))\n",
    "            \n",
    "            # Check tumor grade balance\n",
    "            if 'methylation_class' in df.columns:\n",
    "                tumor_data = df[df['methylation_class'].notna()]\n",
    "                if len(tumor_data) > 0:\n",
    "                    high_grade_terms = ['glioblastoma', 'anaplastic', 'high grade', 'grade iv', 'grade 4', 'gbm']\n",
    "                    high_grade_rate = tumor_data['methylation_class'].str.lower().str.contains(\n",
    "                        '|'.join(high_grade_terms), na=False\n",
    "                    ).mean()\n",
    "                    balances.append(min(high_grade_rate, 1-high_grade_rate))\n",
    "            \n",
    "            avg_balance = np.mean(balances) if balances else 0\n",
    "            \n",
    "            return {\n",
    "                'status': 'PASS' if avg_balance >= 0.15 else 'WARN',\n",
    "                'score': avg_balance,\n",
    "                'details': f\"Average minority class rate: {avg_balance:.3f}\"\n",
    "            }\n",
    "        except:\n",
    "            return {'status': 'FAIL', 'score': 0, 'details': 'Class balance check failed'}\n",
    "\n",
    "    def _check_confounding_factors(self, df):\n",
    "        \"\"\"Check for potential confounding factors in clinical predictions\"\"\"\n",
    "        try:\n",
    "            confounding_issues = []\n",
    "            severity_scores = []\n",
    "            \n",
    "            # Check for age-outcome confounding\n",
    "            age_confounding = self._check_age_confounding(df)\n",
    "            if age_confounding['severity'] > 0:\n",
    "                confounding_issues.append(age_confounding)\n",
    "                severity_scores.append(age_confounding['severity'])\n",
    "            \n",
    "            # Check for center/batch effects (if institutional data available)\n",
    "            batch_confounding = self._check_batch_effects(df)\n",
    "            if batch_confounding['severity'] > 0:\n",
    "                confounding_issues.append(batch_confounding)\n",
    "                severity_scores.append(batch_confounding['severity'])\n",
    "            \n",
    "            # Check for molecular marker interdependence\n",
    "            molecular_confounding = self._check_molecular_confounding(df)\n",
    "            if molecular_confounding['severity'] > 0:\n",
    "                confounding_issues.append(molecular_confounding)\n",
    "                severity_scores.append(molecular_confounding['severity'])\n",
    "            \n",
    "            # Check for survival bias in molecular markers\n",
    "            survival_bias = self._check_survival_bias(df)\n",
    "            if survival_bias['severity'] > 0:\n",
    "                confounding_issues.append(survival_bias) \n",
    "                severity_scores.append(survival_bias['severity'])\n",
    "            \n",
    "            # Overall assessment\n",
    "            if not severity_scores:\n",
    "                status = 'PASS'\n",
    "                score = 1.0\n",
    "                details = \"No major confounding factors detected\"\n",
    "            else:\n",
    "                max_severity = max(severity_scores)\n",
    "                if max_severity >= 0.8:\n",
    "                    status = 'FAIL'\n",
    "                    score = 0.2\n",
    "                    details = f\"Critical confounding detected: {len(confounding_issues)} issues\"\n",
    "                elif max_severity >= 0.5:\n",
    "                    status = 'WARN'\n",
    "                    score = 0.6\n",
    "                    details = f\"Moderate confounding detected: {len(confounding_issues)} issues\"\n",
    "                else:\n",
    "                    status = 'PASS'\n",
    "                    score = 0.8\n",
    "                    details = f\"Minor confounding detected: {len(confounding_issues)} issues\"\n",
    "            \n",
    "            return {\n",
    "                'status': status,\n",
    "                'score': score,\n",
    "                'details': details,\n",
    "                'confounding_issues': confounding_issues,\n",
    "                'n_issues': len(confounding_issues)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': 'WARN',\n",
    "                'score': 0.5,\n",
    "                'details': f'Confounding check incomplete: {str(e)}',\n",
    "                'confounding_issues': [],\n",
    "                'n_issues': 0\n",
    "            }\n",
    "\n",
    "    def _check_age_confounding(self, df):\n",
    "        \"\"\"Check if age is confounded with outcomes\"\"\"\n",
    "        try:\n",
    "            if 'age' not in df.columns:\n",
    "                return {'type': 'age', 'severity': 0, 'description': 'Age data not available'}\n",
    "            \n",
    "            issues = []\n",
    "            max_severity = 0\n",
    "            \n",
    "            # Check age-mortality confounding\n",
    "            if 'survival' in df.columns and 'patient_status' in df.columns:\n",
    "                survival_data = df[df['survival'].notna() & df['patient_status'].notna() & df['age'].notna()]\n",
    "                if len(survival_data) > 10:\n",
    "                    deceased = survival_data[survival_data['patient_status'] == 2]['age']\n",
    "                    alive = survival_data[survival_data['patient_status'] != 2]['age']\n",
    "                    \n",
    "                    if len(deceased) > 5 and len(alive) > 5:\n",
    "                        age_diff = abs(deceased.mean() - alive.mean())\n",
    "                        pooled_std = np.sqrt(((deceased.std()**2 + alive.std()**2) / 2))\n",
    "                        effect_size = age_diff / pooled_std if pooled_std > 0 else 0\n",
    "                        \n",
    "                        if effect_size > 0.8:  # Large effect\n",
    "                            severity = 0.9\n",
    "                            issues.append(f\"Large age difference between deceased ({deceased.mean():.1f}) and alive ({alive.mean():.1f})\")\n",
    "                        elif effect_size > 0.5:  # Medium effect\n",
    "                            severity = 0.6\n",
    "                            issues.append(f\"Moderate age difference between outcomes\")\n",
    "                        \n",
    "                        max_severity = max(max_severity, severity if 'severity' in locals() else 0)\n",
    "            \n",
    "            # Check age-tumor grade confounding  \n",
    "            if 'methylation_class' in df.columns:\n",
    "                tumor_data = df[df['methylation_class'].notna() & df['age'].notna()]\n",
    "                if len(tumor_data) > 10:\n",
    "                    high_grade_terms = ['glioblastoma', 'anaplastic', 'high grade', 'grade iv', 'grade 4', 'gbm']\n",
    "                    high_grade_mask = tumor_data['methylation_class'].str.lower().str.contains('|'.join(high_grade_terms), na=False)\n",
    "                    \n",
    "                    high_grade_ages = tumor_data[high_grade_mask]['age']\n",
    "                    low_grade_ages = tumor_data[~high_grade_mask]['age']\n",
    "                    \n",
    "                    if len(high_grade_ages) > 5 and len(low_grade_ages) > 5:\n",
    "                        age_diff = abs(high_grade_ages.mean() - low_grade_ages.mean())\n",
    "                        pooled_std = np.sqrt(((high_grade_ages.std()**2 + low_grade_ages.std()**2) / 2))\n",
    "                        effect_size = age_diff / pooled_std if pooled_std > 0 else 0\n",
    "                        \n",
    "                        if effect_size > 0.8:\n",
    "                            severity = 0.7  # Slightly less critical than mortality\n",
    "                            issues.append(f\"Age strongly associated with tumor grade\")\n",
    "                            max_severity = max(max_severity, severity)\n",
    "            \n",
    "            return {\n",
    "                'type': 'age_confounding',\n",
    "                'severity': max_severity,\n",
    "                'description': '; '.join(issues) if issues else 'No significant age confounding detected'\n",
    "            }\n",
    "            \n",
    "        except:\n",
    "            return {'type': 'age_confounding', 'severity': 0, 'description': 'Age confounding check failed'}\n",
    "\n",
    "    def _check_batch_effects(self, df):\n",
    "        \"\"\"Check for potential batch/center effects\"\"\"\n",
    "        try:\n",
    "            # Look for institutional or batch identifiers\n",
    "            batch_columns = [col for col in df.columns if any(term in col.lower() \n",
    "                           for term in ['institution', 'center', 'batch', 'site', 'hospital'])]\n",
    "            \n",
    "            if not batch_columns:\n",
    "                return {'type': 'batch_effects', 'severity': 0, 'description': 'No batch identifiers found'}\n",
    "            \n",
    "            # Check if outcomes vary significantly by batch\n",
    "            severity = 0\n",
    "            issues = []\n",
    "            \n",
    "            for batch_col in batch_columns:\n",
    "                unique_batches = df[batch_col].nunique()\n",
    "                if unique_batches > 1 and unique_batches < len(df) * 0.5:  # Reasonable number of batches\n",
    "                    # Check mortality rates by batch\n",
    "                    if 'survival' in df.columns and 'patient_status' in df.columns:\n",
    "                        batch_mortality = df.groupby(batch_col).apply(\n",
    "                            lambda x: ((x['patient_status'] == 2) & (x['survival'] <= 12)).mean()\n",
    "                        )\n",
    "                        if batch_mortality.std() > 0.15:  # >15% variation in mortality rates\n",
    "                            severity = max(severity, 0.6)\n",
    "                            issues.append(f\"Mortality rates vary by {batch_col}\")\n",
    "            \n",
    "            return {\n",
    "                'type': 'batch_effects',\n",
    "                'severity': severity,\n",
    "                'description': '; '.join(issues) if issues else 'No significant batch effects detected'\n",
    "            }\n",
    "            \n",
    "        except:\n",
    "            return {'type': 'batch_effects', 'severity': 0, 'description': 'Batch effects check failed'}\n",
    "\n",
    "    def _check_molecular_confounding(self, df):\n",
    "        \"\"\"Check for confounding between molecular markers\"\"\"\n",
    "        try:\n",
    "            molecular_cols = ['mgmt', 'idh_1_r132h', 'atrx', 'p53']\n",
    "            available_molecular = [col for col in molecular_cols if col in df.columns]\n",
    "            \n",
    "            if len(available_molecular) < 2:\n",
    "                return {'type': 'molecular_confounding', 'severity': 0, 'description': 'Insufficient molecular data'}\n",
    "            \n",
    "            issues = []\n",
    "            max_severity = 0\n",
    "            \n",
    "            # Check IDH-MGMT association (known biological confounding)\n",
    "            if 'idh_1_r132h' in df.columns and 'mgmt' in df.columns:\n",
    "                idh_mgmt_data = df[(df['idh_1_r132h'].isin([1, 2])) & (df['mgmt'].isin([1, 2]))]\n",
    "                \n",
    "                if len(idh_mgmt_data) > 20:\n",
    "                    # Create contingency table\n",
    "                    idh_mutant = (idh_mgmt_data['idh_1_r132h'] == 2)  # Assuming 2 = mutant\n",
    "                    mgmt_methylated = (idh_mgmt_data['mgmt'] == 1)  # 1 = methylated per data dictionary\n",
    "                    \n",
    "                    # Calculate association strength (Cramér's V)\n",
    "                    from scipy.stats import chi2_contingency\n",
    "                    try:\n",
    "                        contingency = pd.crosstab(idh_mutant, mgmt_methylated)\n",
    "                        chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "                        n = contingency.sum().sum()\n",
    "                        cramers_v = np.sqrt(chi2 / (n * (min(contingency.shape) - 1)))\n",
    "                        \n",
    "                        if cramers_v > 0.5 and p_value < 0.05:\n",
    "                            max_severity = 0.8\n",
    "                            issues.append(\"Strong IDH-MGMT association detected (biological confounding)\")\n",
    "                        elif cramers_v > 0.3 and p_value < 0.05:\n",
    "                            max_severity = 0.5\n",
    "                            issues.append(\"Moderate IDH-MGMT association detected\")\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            return {\n",
    "                'type': 'molecular_confounding',\n",
    "                'severity': max_severity,\n",
    "                'description': '; '.join(issues) if issues else 'No significant molecular confounding detected'\n",
    "            }\n",
    "            \n",
    "        except:\n",
    "            return {'type': 'molecular_confounding', 'severity': 0, 'description': 'Molecular confounding check failed'}\n",
    "\n",
    "    def _check_survival_bias(self, df):\n",
    "        \"\"\"Check for survival bias in molecular marker availability\"\"\"\n",
    "        try:\n",
    "            if not all(col in df.columns for col in ['survival', 'patient_status']):\n",
    "                return {'type': 'survival_bias', 'severity': 0, 'description': 'Survival data not available'}\n",
    "            \n",
    "            issues = []\n",
    "            max_severity = 0\n",
    "            \n",
    "            molecular_cols = ['mgmt', 'idh_1_r132h', 'atrx', 'p53']\n",
    "            \n",
    "            for mol_col in molecular_cols:\n",
    "                if mol_col in df.columns:\n",
    "                    # Compare survival times between patients with/without molecular data\n",
    "                    has_molecular = df[df[mol_col].notna() & df['survival'].notna()]\n",
    "                    no_molecular = df[df[mol_col].isna() & df['survival'].notna()]\n",
    "                    \n",
    "                    if len(has_molecular) > 10 and len(no_molecular) > 10:\n",
    "                        survival_diff = abs(has_molecular['survival'].mean() - no_molecular['survival'].mean())\n",
    "                        pooled_std = np.sqrt((has_molecular['survival'].std()**2 + no_molecular['survival'].std()**2) / 2)\n",
    "                        \n",
    "                        if pooled_std > 0:\n",
    "                            effect_size = survival_diff / pooled_std\n",
    "                            \n",
    "                            if effect_size > 0.5:  # Medium to large effect\n",
    "                                severity = 0.6\n",
    "                                issues.append(f\"Survival bias detected for {mol_col} availability\")\n",
    "                                max_severity = max(max_severity, severity)\n",
    "            \n",
    "            return {\n",
    "                'type': 'survival_bias',\n",
    "                'severity': max_severity,\n",
    "                'description': '; '.join(issues) if issues else 'No significant survival bias detected'\n",
    "            }\n",
    "            \n",
    "        except:\n",
    "            return {'type': 'survival_bias', 'severity': 0, 'description': 'Survival bias check failed'}\n",
    "        \"\"\"Check feature quality and completeness\"\"\"\n",
    "        try:\n",
    "            image_features = [col for col in df.columns if col.startswith('feature_')]\n",
    "            clinical_features = ['age', 'sex', 'race', 'ethnicity']\n",
    "            \n",
    "            image_quality = len(image_features) >= 50  # Sufficient image features\n",
    "            clinical_completeness = sum(col in df.columns for col in clinical_features) >= 2\n",
    "            \n",
    "            score = (image_quality + clinical_completeness) / 2\n",
    "            \n",
    "            return {\n",
    "                'status': 'PASS' if score >= 0.5 else 'WARN',\n",
    "                'score': score,\n",
    "                'details': f\"Image features: {len(image_features)}, Clinical completeness: {clinical_completeness}\"\n",
    "            }\n",
    "        except:\n",
    "            return {'status': 'FAIL', 'score': 0, 'details': 'Feature quality check failed'}\n",
    "\n",
    "    def _check_sample_size(self, df):\n",
    "        \"\"\"Check sample size adequacy\"\"\"\n",
    "        try:\n",
    "            total_samples = len(df)\n",
    "            \n",
    "            # Check samples for different tasks\n",
    "            survival_samples = df[df['survival'].notna() & df['patient_status'].notna()].shape[0]\n",
    "            tumor_samples = df[df['methylation_class'].notna()].shape[0]\n",
    "            \n",
    "            min_samples = min(survival_samples, tumor_samples) if tumor_samples > 0 else survival_samples\n",
    "            \n",
    "            if min_samples >= 50:\n",
    "                status = 'PASS'\n",
    "                score = 1.0\n",
    "            elif min_samples >= 30:\n",
    "                status = 'WARN'\n",
    "                score = 0.7\n",
    "            else:\n",
    "                status = 'FAIL'\n",
    "                score = 0.3\n",
    "            \n",
    "            return {\n",
    "                'status': status,\n",
    "                'score': score,\n",
    "                'details': f\"Min task samples: {min_samples}, Total: {total_samples}\"\n",
    "            }\n",
    "        except:\n",
    "            return {'status': 'FAIL', 'score': 0, 'details': 'Sample size check failed'}\n",
    "\n",
    "    def generate_publication_document(self):\n",
    "        \"\"\"Generate a comprehensive publication-ready document\"\"\"\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"No results available for document generation\")\n",
    "            return\n",
    "        \n",
    "        # Create comprehensive document content\n",
    "        doc_content = []\n",
    "        \n",
    "        # Title and Header\n",
    "        doc_content.append(\"COMPREHENSIVE NEUROSURGICAL AI ANALYSIS\")\n",
    "        doc_content.append(\"=\" * 80)\n",
    "        doc_content.append(\"\")\n",
    "        doc_content.append(\"EXECUTIVE SUMMARY\")\n",
    "        doc_content.append(\"-\" * 40)\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        total_tests = 0\n",
    "        excellent_tests = 0\n",
    "        good_tests = 0\n",
    "        all_aucs = []\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    total_tests += 1\n",
    "                    auc = result['auc']\n",
    "                    all_aucs.append(auc)\n",
    "                    \n",
    "                    if auc >= 0.85:\n",
    "                        excellent_tests += 1\n",
    "                    elif auc >= 0.75:\n",
    "                        good_tests += 1\n",
    "        \n",
    "        if all_aucs:\n",
    "            mean_auc = np.mean(all_aucs)\n",
    "            max_auc = np.max(all_aucs)\n",
    "            \n",
    "            doc_content.append(f\"Total algorithm-task combinations tested: {total_tests}\")\n",
    "            doc_content.append(f\"Mean AUC across all tests: {mean_auc:.3f}\")\n",
    "            doc_content.append(f\"Best AUC achieved: {max_auc:.3f}\")\n",
    "            doc_content.append(f\"Excellent performance (AUC >= 0.85): {excellent_tests}/{total_tests} ({excellent_tests/total_tests*100:.1f}%)\")\n",
    "            doc_content.append(f\"Good+ performance (AUC >= 0.75): {good_tests+excellent_tests}/{total_tests} ({(good_tests+excellent_tests)/total_tests*100:.1f}%)\")\n",
    "            doc_content.append(\"\")\n",
    "            \n",
    "            if excellent_tests > 0:\n",
    "                doc_content.append(f\"CLINICAL DEPLOYMENT: {excellent_tests} combinations ready for validation\")\n",
    "            if max_auc >= 0.90:\n",
    "                doc_content.append(\"PUBLICATION STATUS: Exceptional results achieved - ready for top-tier journals\")\n",
    "            elif max_auc >= 0.80:\n",
    "                doc_content.append(\"PUBLICATION STATUS: Strong results achieved - ready for clinical journals\")\n",
    "        \n",
    "        doc_content.append(\"\")\n",
    "        doc_content.append(\"\")\n",
    "        \n",
    "        # Detailed Results Table\n",
    "        doc_content.append(\"COMPREHENSIVE RESULTS TABLE\")\n",
    "        doc_content.append(\"-\" * 80)\n",
    "        doc_content.append(\"\")\n",
    "        \n",
    "        # Create detailed table\n",
    "        header = f\"{'CNN':<20} {'Task':<25} {'Algorithm':<15} {'AUC':<8} {'Accuracy':<9} {'Sensitivity':<11} {'Specificity':<11} {'Status':<15}\"\n",
    "        doc_content.append(header)\n",
    "        doc_content.append(\"-\" * len(header))\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                task_name = task_data['task_name']\n",
    "                \n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    auc = result['auc']\n",
    "                    acc = result['accuracy']\n",
    "                    sens = result['sensitivity']\n",
    "                    spec = result['specificity']\n",
    "                    \n",
    "                    # Status based on AUC without emojis\n",
    "                    if auc >= 0.85:\n",
    "                        status = \"EXCELLENT\"\n",
    "                    elif auc >= 0.75:\n",
    "                        status = \"STRONG\"\n",
    "                    elif auc >= 0.65:\n",
    "                        status = \"GOOD\"\n",
    "                    else:\n",
    "                        status = \"MODERATE\"\n",
    "                    \n",
    "                    row = f\"{cnn_name:<20} {task_name:<25} {alg_name:<15} {auc:<8.3f} {acc:<9.3f} {sens:<11.3f} {spec:<11.3f} {status:<15}\"\n",
    "                    doc_content.append(row)\n",
    "        \n",
    "        doc_content.append(\"\")\n",
    "        doc_content.append(\"\")\n",
    "        \n",
    "        # Best Performers Analysis\n",
    "        doc_content.append(\"BEST PERFORMERS BY CLINICAL TASK\")\n",
    "        doc_content.append(\"-\" * 40)\n",
    "        doc_content.append(\"\")\n",
    "        \n",
    "        # Find best performer for each task\n",
    "        task_best = {}\n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                task_name = task_data['task_name']\n",
    "                \n",
    "                if task_name not in task_best:\n",
    "                    task_best[task_name] = {'auc': 0, 'cnn': '', 'algorithm': '', 'result': None}\n",
    "                \n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    if result['auc'] > task_best[task_name]['auc']:\n",
    "                        task_best[task_name] = {\n",
    "                            'auc': result['auc'],\n",
    "                            'cnn': cnn_name,\n",
    "                            'algorithm': alg_name,\n",
    "                            'result': result\n",
    "                        }\n",
    "        \n",
    "        for task_name, best in task_best.items():\n",
    "            auc = best['auc']\n",
    "            acc = best['result']['accuracy']\n",
    "            sens = best['result']['sensitivity']\n",
    "            spec = best['result']['specificity']\n",
    "            \n",
    "            status = \"DEPLOYMENT READY\" if auc >= 0.85 else \"PROMISING\" if auc >= 0.75 else \"NEEDS OPTIMIZATION\"\n",
    "            \n",
    "            doc_content.append(f\"Task: {task_name}\")\n",
    "            doc_content.append(f\"  Best Combination: {best['cnn']} + {best['algorithm']}\")\n",
    "            doc_content.append(f\"  Performance: AUC = {auc:.3f}, Accuracy = {acc:.3f}\")\n",
    "            doc_content.append(f\"  Clinical Metrics: Sensitivity = {sens:.3f}, Specificity = {spec:.3f}\")\n",
    "            doc_content.append(f\"  Status: {status}\")\n",
    "            doc_content.append(\"\")\n",
    "        \n",
    "        # Algorithm Performance Ranking\n",
    "        doc_content.append(\"ALGORITHM PERFORMANCE RANKING\")\n",
    "        doc_content.append(\"-\" * 40)\n",
    "        doc_content.append(\"\")\n",
    "        \n",
    "        algorithm_stats = {}\n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    if alg_name not in algorithm_stats:\n",
    "                        algorithm_stats[alg_name] = []\n",
    "                    algorithm_stats[alg_name].append(result['auc'])\n",
    "        \n",
    "        if algorithm_stats:\n",
    "            sorted_algorithms = sorted(algorithm_stats.items(), key=lambda x: np.mean(x[1]), reverse=True)\n",
    "            \n",
    "            for i, (alg_name, aucs) in enumerate(sorted_algorithms, 1):\n",
    "                mean_auc = np.mean(aucs)\n",
    "                std_auc = np.std(aucs)\n",
    "                max_auc = np.max(aucs)\n",
    "                n_tests = len(aucs)\n",
    "                \n",
    "                doc_content.append(f\"{i}. {alg_name}\")\n",
    "                doc_content.append(f\"   Mean AUC: {mean_auc:.3f} (±{std_auc:.3f})\")\n",
    "                doc_content.append(f\"   Best AUC: {max_auc:.3f}\")\n",
    "                doc_content.append(f\"   Tests: {n_tests}\")\n",
    "                doc_content.append(\"\")\n",
    "        \n",
    "        # CNN Architecture Ranking\n",
    "        doc_content.append(\"CNN ARCHITECTURE RANKING\")\n",
    "        doc_content.append(\"-\" * 40)\n",
    "        doc_content.append(\"\")\n",
    "        \n",
    "        cnn_stats = {}\n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            aucs = []\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    aucs.append(result['auc'])\n",
    "            if aucs:\n",
    "                cnn_stats[cnn_name] = aucs\n",
    "        \n",
    "        if cnn_stats:\n",
    "            sorted_cnns = sorted(cnn_stats.items(), key=lambda x: np.mean(x[1]), reverse=True)\n",
    "            \n",
    "            for i, (cnn_name, aucs) in enumerate(sorted_cnns, 1):\n",
    "                mean_auc = np.mean(aucs)\n",
    "                std_auc = np.std(aucs)\n",
    "                max_auc = np.max(aucs)\n",
    "                n_tests = len(aucs)\n",
    "                \n",
    "                doc_content.append(f\"{i}. {cnn_name}\")\n",
    "                doc_content.append(f\"   Mean AUC: {mean_auc:.3f} (±{std_auc:.3f})\")\n",
    "                doc_content.append(f\"   Best AUC: {max_auc:.3f}\")\n",
    "                doc_content.append(f\"   Tests: {n_tests}\")\n",
    "                doc_content.append(\"\")\n",
    "        \n",
    "        # Clinical Recommendations\n",
    "        doc_content.append(\"CLINICAL IMPLEMENTATION RECOMMENDATIONS\")\n",
    "        doc_content.append(\"-\" * 40)\n",
    "        doc_content.append(\"\")\n",
    "        \n",
    "        # Find deployment-ready combinations\n",
    "        deployment_ready = []\n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                task_name = task_data['task_name']\n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    if result['auc'] >= 0.80:  # Clinical deployment threshold\n",
    "                        deployment_ready.append({\n",
    "                            'task': task_name,\n",
    "                            'cnn': cnn_name,\n",
    "                            'algorithm': alg_name,\n",
    "                            'auc': result['auc'],\n",
    "                            'accuracy': result['accuracy']\n",
    "                        })\n",
    "        \n",
    "        deployment_ready.sort(key=lambda x: x['auc'], reverse=True)\n",
    "        \n",
    "        if deployment_ready:\n",
    "            doc_content.append(f\"DEPLOYMENT-READY COMBINATIONS (AUC >= 0.80): {len(deployment_ready)}\")\n",
    "            doc_content.append(\"\")\n",
    "            \n",
    "            for i, combo in enumerate(deployment_ready[:10], 1):  # Top 10\n",
    "                doc_content.append(f\"{i}. {combo['task']}\")\n",
    "                doc_content.append(f\"   Model: {combo['cnn']} + {combo['algorithm']}\")\n",
    "                doc_content.append(f\"   Performance: {combo['auc']:.1%} AUC, {combo['accuracy']:.1%} Accuracy\")\n",
    "                doc_content.append(\"\")\n",
    "                \n",
    "            doc_content.append(\"PRIORITY IMPLEMENTATION:\")\n",
    "            top_combo = deployment_ready[0]\n",
    "            doc_content.append(f\"Task: {top_combo['task']}\")\n",
    "            doc_content.append(f\"Architecture: {top_combo['cnn']} + {top_combo['algorithm']}\")\n",
    "            doc_content.append(f\"Expected Clinical Performance: {top_combo['auc']:.1%} discrimination accuracy\")\n",
    "            doc_content.append(\"\")\n",
    "        else:\n",
    "            doc_content.append(\"No combinations reached clinical deployment threshold (AUC >= 0.80)\")\n",
    "            doc_content.append(\"Focus on methodology optimization for best performing approaches\")\n",
    "            doc_content.append(\"\")\n",
    "        \n",
    "        # Publication Strategy\n",
    "        doc_content.append(\"PUBLICATION STRATEGY\")\n",
    "        doc_content.append(\"-\" * 40)\n",
    "        doc_content.append(\"\")\n",
    "        \n",
    "        # Count publication-ready results\n",
    "        tier1_results = []  # AUC >= 0.85\n",
    "        tier2_results = []  # AUC >= 0.75\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                task_name = task_data['task_name']\n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    if result['auc'] >= 0.85:\n",
    "                        tier1_results.append((task_name, cnn_name, alg_name, result['auc']))\n",
    "                    elif result['auc'] >= 0.75:\n",
    "                        tier2_results.append((task_name, cnn_name, alg_name, result['auc']))\n",
    "        \n",
    "        doc_content.append(\"PUBLICATION READINESS ASSESSMENT:\")\n",
    "        doc_content.append(f\"Tier 1 Results (AUC >= 0.85): {len(tier1_results)} - Suitable for top-tier journals\")\n",
    "        doc_content.append(f\"Tier 2 Results (AUC >= 0.75): {len(tier2_results)} - Suitable for clinical journals\")\n",
    "        doc_content.append(\"\")\n",
    "        \n",
    "        if tier1_results:\n",
    "            doc_content.append(\"TOP-TIER JOURNAL STRATEGY:\")\n",
    "            doc_content.append(\"Target Journals: Nature Medicine, Lancet Digital Health, Nature Biomedical Engineering\")\n",
    "            best_result = max(tier1_results, key=lambda x: x[3])\n",
    "            doc_content.append(f\"Lead Finding: {best_result[0]} ({best_result[1]} + {best_result[2]}, AUC = {best_result[3]:.3f})\")\n",
    "            doc_content.append(\"Narrative: 'Deep Learning Achieves Clinical-Grade Performance in Neurosurgical Prediction'\")\n",
    "            doc_content.append(\"\")\n",
    "            \n",
    "        if tier2_results:\n",
    "            doc_content.append(\"CLINICAL JOURNAL STRATEGY:\")\n",
    "            doc_content.append(\"Target Journals: Neuro-Oncology, Journal of Neurosurgery, Academic Radiology\")\n",
    "            doc_content.append(\"Focus: Clinical validation studies and comparative effectiveness research\")\n",
    "            doc_content.append(\"\")\n",
    "        \n",
    "        doc_content.append(\"MANUSCRIPT DEVELOPMENT PRIORITIES:\")\n",
    "        doc_content.append(\"1. Primary Research Paper: Best performing clinical task for high-impact publication\")\n",
    "        doc_content.append(\"2. Methodology Paper: Comprehensive multi-architecture comparison study\")\n",
    "        doc_content.append(\"3. Clinical Implementation Paper: Validation study and cost-effectiveness analysis\")\n",
    "        doc_content.append(\"4. Technical Paper: Algorithm optimization and feature engineering methods\")\n",
    "        doc_content.append(\"\")\n",
    "        \n",
    "        # Validation Summary\n",
    "        if self.validation_results:\n",
    "            doc_content.append(\"DATA VALIDATION SUMMARY\")\n",
    "            doc_content.append(\"-\" * 40)\n",
    "            doc_content.append(\"\")\n",
    "            \n",
    "            validation_header = f\"{'CNN Architecture':<20} {'Overall Status':<15} {'Data Quality':<12} {'Class Balance':<12} {'Sample Size':<12}\"\n",
    "            doc_content.append(validation_header)\n",
    "            doc_content.append(\"-\" * len(validation_header))\n",
    "            \n",
    "            for cnn_name, validation in self.validation_results.items():\n",
    "                if 'error' in validation:\n",
    "                    doc_content.append(f\"{cnn_name:<20} {'ERROR':<15} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")\n",
    "                else:\n",
    "                    overall = validation.get('overall', {}).get('status', 'FAIL')\n",
    "                    data_quality = validation.get('data_integrity', {}).get('status', 'FAIL')\n",
    "                    class_balance = validation.get('class_balance', {}).get('status', 'FAIL')\n",
    "                    sample_size = validation.get('sample_size', {}).get('status', 'FAIL')\n",
    "                    \n",
    "                    doc_content.append(f\"{cnn_name:<20} {overall:<15} {data_quality:<12} {class_balance:<12} {sample_size:<12}\")\n",
    "            \n",
    "            doc_content.append(\"\")\n",
    "        \n",
    "        # Technical Specifications\n",
    "        doc_content.append(\"TECHNICAL SPECIFICATIONS\")\n",
    "        doc_content.append(\"-\" * 40)\n",
    "        doc_content.append(\"\")\n",
    "        doc_content.append(\"Machine Learning Algorithms Tested:\")\n",
    "        \n",
    "        algorithms = self.get_ml_algorithms()\n",
    "        for i, (alg_name, alg_config) in enumerate(algorithms.items(), 1):\n",
    "            doc_content.append(f\"{i}. {alg_name}: {alg_config['description']}\")\n",
    "            doc_content.append(f\"   Preprocessing: {'Robust Scaling Applied' if alg_config['needs_scaling'] else 'No Scaling Required'}\")\n",
    "        \n",
    "        doc_content.append(\"\")\n",
    "        doc_content.append(\"CNN Architectures Evaluated:\")\n",
    "        for i, cnn_name in enumerate(self.datasets.keys(), 1):\n",
    "            doc_content.append(f\"{i}. {cnn_name}\")\n",
    "        \n",
    "        doc_content.append(\"\")\n",
    "        doc_content.append(\"Clinical Tasks Assessed:\")\n",
    "        tasks = set()\n",
    "        for cnn_results in self.results.values():\n",
    "            for task_data in cnn_results.values():\n",
    "                tasks.add(task_data['task_name'])\n",
    "        \n",
    "        for i, task in enumerate(sorted(tasks), 1):\n",
    "            doc_content.append(f\"{i}. {task}\")\n",
    "        \n",
    "        doc_content.append(\"\")\n",
    "        doc_content.append(\"=\" * 80)\n",
    "        doc_content.append(\"ANALYSIS COMPLETE\")\n",
    "        doc_content.append(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        doc_content.append(\"=\" * 80)\n",
    "        \n",
    "        # Write to file\n",
    "        filename = f\"neurosurgical_ai_analysis_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "        \n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                for line in doc_content:\n",
    "                    f.write(line + '\\n')\n",
    "            \n",
    "            # Calculate file size properly\n",
    "            doc_text = '\\n'.join(doc_content)\n",
    "            file_size = len(doc_text)\n",
    "            \n",
    "            print(f\"\\nPublication document generated successfully!\")\n",
    "            print(f\"Filename: {filename}\")\n",
    "            print(f\"Lines written: {len(doc_content)}\")\n",
    "            print(f\"File size: {file_size} characters\")\n",
    "            \n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error writing document: {e}\")\n",
    "            return None\n",
    "\n",
    "    def run_comprehensive_analysis(self):\n",
    "        \"\"\"Run the complete comprehensive analysis\"\"\"\n",
    "        \n",
    "        print(\"COMPREHENSIVE NEUROSURGICAL AI ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Testing 5 CNNs × Multiple ML Algorithms × 6 Clinical Tasks\")\n",
    "        print(\"Target: Clinical-grade performance (AUC >= 0.80)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Initialize ML algorithms\n",
    "        algorithms = self.get_ml_algorithms()\n",
    "        \n",
    "        print(f\"\\nAVAILABLE ALGORITHMS ({len(algorithms)}):\")\n",
    "        for alg_name, alg_config in algorithms.items():\n",
    "            print(f\"   {alg_name}: {alg_config['description']}\")\n",
    "        \n",
    "        # Test each CNN dataset\n",
    "        for cnn_name, file_path in self.datasets.items():\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"ANALYZING {cnn_name} DATASET\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            try:\n",
    "                # Check if file exists before processing\n",
    "                import os\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"ERROR {cnn_name}: File not found - {file_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Run validation checks first\n",
    "                validation = self.run_validation_checks(cnn_name, file_path)\n",
    "                self.validation_results[cnn_name] = validation\n",
    "                \n",
    "                if 'error' in validation:\n",
    "                    print(f\"ERROR {cnn_name}: Validation failed - {validation['error']}\")\n",
    "                    continue\n",
    "                \n",
    "                overall_status = validation.get('overall', {}).get('status', 'FAIL')\n",
    "                if overall_status == 'FAIL':\n",
    "                    print(f\"ERROR {cnn_name}: Failed validation checks\")\n",
    "                    continue\n",
    "                \n",
    "                # Load and process data\n",
    "                print(f\"Loading data from: {file_path}\")\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"Dataset shape: {df.shape}\")\n",
    "                \n",
    "                targets_data = self.create_all_targets(df)\n",
    "                \n",
    "                if not targets_data:\n",
    "                    print(f\"ERROR {cnn_name}: No valid targets created\")\n",
    "                    continue\n",
    "                \n",
    "                # Feature selection\n",
    "                features = self.select_features(df)\n",
    "                print(f\"Available features: {len(features)}\")\n",
    "                \n",
    "                cnn_results = {}\n",
    "                \n",
    "                # Test each target category\n",
    "                for category, target_info in targets_data.items():\n",
    "                    category_data = target_info['data']\n",
    "                    \n",
    "                    for i, target_col in enumerate(target_info['targets']):\n",
    "                        task_name = target_info['descriptions'][i]\n",
    "                        \n",
    "                        print(f\"\\n{'-'*40}\")\n",
    "                        print(f\"TASK: {task_name}\")\n",
    "                        print(f\"{'-'*40}\")\n",
    "                        \n",
    "                        # Exclude target-related features to prevent leakage\n",
    "                        safe_features = self._get_safe_features(features, target_col)\n",
    "                        \n",
    "                        X, y, error = self.preprocess_data(category_data, safe_features, target_col)\n",
    "                        \n",
    "                        if X is None:\n",
    "                            print(f\"ERROR {task_name}: {error}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Run all algorithms for this task\n",
    "                        task_results = self.run_prediction_task(X, y, task_name, cnn_name, algorithms)\n",
    "                        \n",
    "                        if task_results:\n",
    "                            task_key = f\"{category}_{target_col}\"\n",
    "                            cnn_results[task_key] = {\n",
    "                                'task_name': task_name,\n",
    "                                'results': task_results,\n",
    "                                'n_samples': len(X),\n",
    "                                'n_features': X.shape[1]\n",
    "                            }\n",
    "                \n",
    "                if cnn_results:\n",
    "                    self.results[cnn_name] = cnn_results\n",
    "                    print(f\"\\nSUCCESS {cnn_name}: {len(cnn_results)} tasks completed successfully\")\n",
    "                else:\n",
    "                    print(f\"ERROR {cnn_name}: No tasks completed successfully\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"ERROR {cnn_name}: Complete failure - {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()  # This will help debug the specific error\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        self.generate_comprehensive_report()\n",
    "        \n",
    "        # Generate publication document\n",
    "        doc_filename = self.generate_publication_document()\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "    def _get_safe_features(self, features, target_col):\n",
    "        \"\"\"Get features safe from data leakage\"\"\"\n",
    "        # Remove features that might leak information about the target\n",
    "        unsafe_patterns = {\n",
    "            'idh_binary': ['idh'],\n",
    "            'mgmt_binary': ['mgmt'],\n",
    "            'high_grade': [],  # Tumor grade can use all molecular features\n",
    "            'mortality_6mo': [],\n",
    "            'mortality_1yr': [],\n",
    "            'mortality_2yr': []\n",
    "        }\n",
    "        \n",
    "        patterns_to_exclude = unsafe_patterns.get(target_col, [])\n",
    "        \n",
    "        safe_features = []\n",
    "        for feature in features:\n",
    "            is_safe = True\n",
    "            for pattern in patterns_to_exclude:\n",
    "                if pattern.lower() in feature.lower():\n",
    "                    is_safe = False\n",
    "                    break\n",
    "            if is_safe:\n",
    "                safe_features.append(feature)\n",
    "        \n",
    "        return safe_features\n",
    "\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate comprehensive analysis report\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"\\n❌ No results to report\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"📊 COMPREHENSIVE ANALYSIS REPORT\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # EXECUTIVE SUMMARY\n",
    "        # ============================================================\n",
    "        self._generate_executive_summary()\n",
    "        \n",
    "        # ============================================================\n",
    "        # DETAILED RESULTS TABLE\n",
    "        # ============================================================\n",
    "        self._generate_detailed_results_table()\n",
    "        \n",
    "        # ============================================================\n",
    "        # BEST PERFORMERS ANALYSIS\n",
    "        # ============================================================\n",
    "        self._generate_best_performers_analysis()\n",
    "        \n",
    "        # ============================================================\n",
    "        # VALIDATION SUMMARY\n",
    "        # ============================================================\n",
    "        self._generate_validation_summary()\n",
    "        \n",
    "        # ============================================================\n",
    "        # CLINICAL RECOMMENDATIONS\n",
    "        # ============================================================\n",
    "        self._generate_clinical_recommendations()\n",
    "        \n",
    "        # ============================================================\n",
    "        # PUBLICATION STRATEGY\n",
    "        # ============================================================\n",
    "        self._generate_publication_strategy()\n",
    "\n",
    "    def _generate_executive_summary(self):\n",
    "        \"\"\"Generate executive summary\"\"\"\n",
    "        print(\"\\n🎯 EXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        total_tests = 0\n",
    "        excellent_tests = 0\n",
    "        good_tests = 0\n",
    "        \n",
    "        all_aucs = []\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    total_tests += 1\n",
    "                    auc = result['auc']\n",
    "                    all_aucs.append(auc)\n",
    "                    \n",
    "                    if auc >= 0.85:\n",
    "                        excellent_tests += 1\n",
    "                    elif auc >= 0.75:\n",
    "                        good_tests += 1\n",
    "        \n",
    "        if all_aucs:\n",
    "            mean_auc = np.mean(all_aucs)\n",
    "            max_auc = np.max(all_aucs)\n",
    "            \n",
    "            print(f\" PERFORMANCE OVERVIEW:\")\n",
    "            print(f\"   Total algorithm-task combinations: {total_tests}\")\n",
    "            print(f\"   Mean AUC across all tests: {mean_auc:.3f}\")\n",
    "            print(f\"   Best AUC achieved: {max_auc:.3f}\")\n",
    "            print(f\"   Excellent performance (AUC ≥ 0.85): {excellent_tests}/{total_tests} ({excellent_tests/total_tests*100:.1f}%)\")\n",
    "            print(f\"   Good+ performance (AUC ≥ 0.75): {good_tests+excellent_tests}/{total_tests} ({(good_tests+excellent_tests)/total_tests*100:.1f}%)\")\n",
    "            \n",
    "            # Clinical readiness assessment\n",
    "            if excellent_tests > 0:\n",
    "                print(f\"   🚀 CLINICAL DEPLOYMENT: {excellent_tests} combinations ready for validation\")\n",
    "            if max_auc >= 0.90:\n",
    "                print(f\"   🏆 PUBLICATION READY: Exceptional results achieved\")\n",
    "            elif max_auc >= 0.80:\n",
    "                print(f\"   📝 PUBLICATION READY: Strong results achieved\")\n",
    "\n",
    "    def _generate_detailed_results_table(self):\n",
    "        \"\"\"Generate detailed results table\"\"\"\n",
    "        print(f\"\\n📋 DETAILED RESULTS TABLE\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Header\n",
    "        print(f\"{'CNN':<20} {'Task':<25} {'Algorithm':<15} {'AUC':<8} {'Acc':<8} {'Sens':<8} {'Spec':<8} {'Status':<15}\")\n",
    "        print(\"-\" * 120)\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                task_name = task_data['task_name']\n",
    "                \n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    auc = result['auc']\n",
    "                    acc = result['accuracy']\n",
    "                    sens = result['sensitivity']\n",
    "                    spec = result['specificity']\n",
    "                    \n",
    "                    # Status based on AUC\n",
    "                    if auc >= 0.85:\n",
    "                        status = \"🏆 EXCELLENT\"\n",
    "                    elif auc >= 0.75:\n",
    "                        status = \"✅ STRONG\"\n",
    "                    elif auc >= 0.65:\n",
    "                        status = \"📈 GOOD\"\n",
    "                    else:\n",
    "                        status = \"⚠️ MODERATE\"\n",
    "                    \n",
    "                    print(f\"{cnn_name:<20} {task_name:<25} {alg_name:<15} {auc:<8.3f} {acc:<8.3f} {sens:<8.3f} {spec:<8.3f} {status:<15}\")\n",
    "\n",
    "    def _generate_best_performers_analysis(self):\n",
    "        \"\"\"Generate best performers analysis\"\"\"\n",
    "        print(f\"\\n🏆 BEST PERFORMERS BY TASK\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Find best performer for each task across all CNNs\n",
    "        task_best = {}\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                task_name = task_data['task_name']\n",
    "                \n",
    "                if task_name not in task_best:\n",
    "                    task_best[task_name] = {'auc': 0, 'cnn': '', 'algorithm': '', 'result': None}\n",
    "                \n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    if result['auc'] > task_best[task_name]['auc']:\n",
    "                        task_best[task_name] = {\n",
    "                            'auc': result['auc'],\n",
    "                            'cnn': cnn_name,\n",
    "                            'algorithm': alg_name,\n",
    "                            'result': result\n",
    "                        }\n",
    "        \n",
    "        for task_name, best in task_best.items():\n",
    "            auc = best['auc']\n",
    "            status = \"🚀 DEPLOYMENT READY\" if auc >= 0.85 else \"📈 PROMISING\" if auc >= 0.75 else \"⚠️ NEEDS WORK\"\n",
    "            print(f\"{task_name:<30}: {best['cnn']} + {best['algorithm']} (AUC = {auc:.3f}) {status}\")\n",
    "\n",
    "    def _generate_validation_summary(self):\n",
    "        \"\"\"Generate validation summary\"\"\"\n",
    "        print(f\"\\nVALIDATION SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if not self.validation_results:\n",
    "            print(\"No validation results available\")\n",
    "            return\n",
    "        \n",
    "        print(f\"{'CNN':<20} {'Overall':<10} {'Data':<10} {'Balance':<10} {'Features':<10} {'Samples':<10}\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        for cnn_name, validation in self.validation_results.items():\n",
    "            if 'error' in validation:\n",
    "                print(f\"{cnn_name:<20} {'ERROR':<10} {'N/A':<10} {'N/A':<10} {'N/A':<10} {'N/A':<10}\")\n",
    "                continue\n",
    "            \n",
    "            overall = validation.get('overall', {}).get('status', 'FAIL')\n",
    "            data_integrity = validation.get('data_integrity', {}).get('status', 'FAIL')\n",
    "            class_balance = validation.get('class_balance', {}).get('status', 'FAIL')\n",
    "            feature_quality = validation.get('feature_quality', {}).get('status', 'FAIL')\n",
    "            sample_size = validation.get('sample_size', {}).get('status', 'FAIL')\n",
    "            \n",
    "            print(f\"{cnn_name:<20} {overall:<10} {data_integrity:<10} {class_balance:<10} {feature_quality:<10} {sample_size:<10}\")\n",
    "\n",
    "    def _generate_clinical_recommendations(self):\n",
    "        \"\"\"Generate clinical recommendations\"\"\"\n",
    "        print(f\"\\nCLINICAL RECOMMENDATIONS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Algorithm performance ranking\n",
    "        algorithm_stats = {}\n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    if alg_name not in algorithm_stats:\n",
    "                        algorithm_stats[alg_name] = []\n",
    "                    algorithm_stats[alg_name].append(result['auc'])\n",
    "        \n",
    "        print(\"ALGORITHM PERFORMANCE RANKING:\")\n",
    "        if algorithm_stats:\n",
    "            for alg_name, aucs in sorted(algorithm_stats.items(), key=lambda x: np.mean(x[1]), reverse=True):\n",
    "                mean_auc = np.mean(aucs)\n",
    "                max_auc = np.max(aucs)\n",
    "                n_tests = len(aucs)\n",
    "                print(f\"   {alg_name}: {mean_auc:.3f} mean AUC, {max_auc:.3f} max AUC ({n_tests} tests)\")\n",
    "        \n",
    "        # CNN performance ranking\n",
    "        cnn_stats = {}\n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            aucs = []\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    aucs.append(result['auc'])\n",
    "            if aucs:\n",
    "                cnn_stats[cnn_name] = aucs\n",
    "        \n",
    "        print(f\"\\nCNN ARCHITECTURE RANKING:\")\n",
    "        if cnn_stats:\n",
    "            for cnn_name, aucs in sorted(cnn_stats.items(), key=lambda x: np.mean(x[1]), reverse=True):\n",
    "                mean_auc = np.mean(aucs)\n",
    "                max_auc = np.max(aucs)\n",
    "                n_tests = len(aucs)\n",
    "                print(f\"   {cnn_name}: {mean_auc:.3f} mean AUC, {max_auc:.3f} max AUC ({n_tests} tests)\")\n",
    "        \n",
    "        # Implementation recommendations\n",
    "        print(f\"\\nIMPLEMENTATION RECOMMENDATIONS:\")\n",
    "        \n",
    "        best_combinations = []\n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for task_key, task_data in cnn_results.items():\n",
    "                task_name = task_data['task_name']\n",
    "                for alg_name, result in task_data['results'].items():\n",
    "                    if result['auc'] >= 0.80:\n",
    "                        best_combinations.append({\n",
    "                            'cnn': cnn_name,\n",
    "                            'task': task_name,\n",
    "                            'algorithm': alg_name,\n",
    "                            'auc': result['auc']\n",
    "                        })\n",
    "        \n",
    "        best_combinations.sort(key=lambda x: x['auc'], reverse=True)\n",
    "        \n",
    "        if best_combinations:\n",
    "            print(f\"   {len(best_combinations)} CNN-algorithm combinations ready for clinical validation\")\n",
    "            print(f\"   Priority implementation: {best_combinations[0]['task']} using {best_combinations[0]['cnn']} + {best_combinations[0]['algorithm']}\")\n",
    "            print(f\"   Expected performance: {best_combinations[0]['auc']:.1%} discrimination accuracy\")\n",
    "        else:\n",
    "            print(f\"   No combinations reached clinical deployment threshold (AUC >= 0.80)\")\n",
    "            print(f\"   Focus on methodology optimization for best performing approaches\")\n",
    "\n",
    "    #def _generate_publication_strategy(self):\n",
    "       #\"\"\"Generate publication strategy\"\"\"\n",
    "        #print(f\"\\nPUBLICATION STRATEGY\")\n",
    "        #print(\"=\"*50)\n",
    "        \n",
    "        # Count publication-ready results\n",
    "        #excellent_results = []\n",
    "        #good_results = []\n",
    "        \n",
    "        #for cnn_name, cnn_results in self.results.items():\n",
    "            #for task_key, task_data in cnn_results.items():\n",
    "                #task_name = task_data['task_name']\n",
    "                #for alg_name, result in task_data['results'].items():\n",
    "                    #if result['auc'] >= 0.85:\n",
    "                        #excellent_results.append((task_name, cnn_name, alg_name, result['auc']))\n",
    "                    #elif result['auc'] >= 0.75:\n",
    "                        #good_results.append((task_name, cnn_name, alg_name, result['auc']))\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\"COMPREHENSIVE NEUROSURGICAL AI ANALYSIS SYSTEM\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"GOAL: Comprehensive evaluation of CNN architectures and ML algorithms\")\n",
    "    print(\"SCOPE: 5 CNNs × Multiple Algorithms × 6 Clinical Tasks\")\n",
    "    print(\"OUTPUT: Clinical-ready recommendations for your team and PI\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = NeurosurgicalAIAnalyzer()\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    results = analyzer.run_comprehensive_analysis()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"COMPREHENSIVE ANALYSIS COMPLETE!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if results:\n",
    "        n_cnns = len(results)\n",
    "        total_tasks = sum(len(cnn_results) for cnn_results in results.values())\n",
    "        total_tests = sum(\n",
    "            len(task_data['results']) \n",
    "            for cnn_results in results.values() \n",
    "            for task_data in cnn_results.values()\n",
    "        )\n",
    "        \n",
    "        print(f\"ANALYSIS SUMMARY:\")\n",
    "        print(f\"   • {n_cnns} CNN architectures analyzed\")\n",
    "        print(f\"   • {total_tasks} clinical tasks evaluated\") \n",
    "        print(f\"   • {total_tests} algorithm-task combinations tested\")\n",
    "        print(f\"   • Comprehensive validation and recommendations generated\")\n",
    "        print(f\"   • Publication-ready document created\")\n",
    "    else:\n",
    "        print(\"No results generated. Check data file paths and formats.\")\n",
    "    \n",
    "    return analyzer\n",
    "\n",
    "# Execute the comprehensive analysis\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a425d",
   "metadata": {},
   "source": [
    "*working code 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee99794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKING DATA FILE PATHS:\n",
      "==================================================\n",
      "ConvNext            : EXISTS\n",
      "ViT                 : EXISTS\n",
      "ResNet50_Pretrained : EXISTS\n",
      "ResNet50_ImageNet   : EXISTS\n",
      "EfficientNet        : EXISTS\n",
      "==================================================\n",
      "Found 5/5 data files\n",
      "SUCCESS: All data files found!\n",
      "\n",
      "NEUROSURGICAL AI OUTCOME PREDICTION ANALYSIS\n",
      "================================================================================\n",
      "Available algorithms: ['TabPFN', 'XGBoost', 'TabNet', 'RandomForest', 'LogisticRegression', 'SVM']\n",
      "\n",
      "Loading ConvNext dataset...\n",
      "Loaded 532 patients with 356 features\n",
      "============================================================\n",
      "CREATING PREDICTION TARGETS\n",
      "============================================================\n",
      "MORTALITY TARGETS:\n",
      "   Patients: 86\n",
      "   6-month: 19/86 (22.1%)\n",
      "   1-year: 38/86 (44.2%)\n",
      "   2-year: 70/86 (81.4%)\n",
      "\n",
      "TUMOR CLASSIFICATION TARGETS:\n",
      "   Patients: 241\n",
      "   High-grade: 129/241 (53.5%)\n",
      "\n",
      "IDH MUTATION TARGETS:\n",
      "   Patients: 198\n",
      "   IDH Mutant: 174.0/198 (87.9%)\n",
      "\n",
      "MGMT METHYLATION TARGETS:\n",
      "   Patients: 212\n",
      "   MGMT Methylated: 84.0/212 (39.6%)\n",
      "Selected 13 features for analysis\n",
      "\n",
      "==================================================\n",
      "6-Month Mortality - ConvNext\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 19, np.int64(0): 67}\n",
      "   Minority class: 19 (22.1%)\n",
      "   Imbalance ratio: 3.53:1\n",
      "   Severity: MILD IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 21.9% (train), 22.7% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.694\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.706\n",
      "      Balanced Accuracy: 0.553\n",
      "      F1 Score: 0.333\n",
      "      MCC: 0.095\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_0_auc = 0.70588\n",
      "      AUC-ROC: 0.706\n",
      "      Balanced Accuracy: 0.588\n",
      "      F1 Score: 0.417\n",
      "      MCC: 0.215\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.741\n",
      "      Balanced Accuracy: 0.735\n",
      "      F1 Score: 0.526\n",
      "      MCC: 0.410\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.647\n",
      "      Balanced Accuracy: 0.582\n",
      "      F1 Score: 0.364\n",
      "      MCC: 0.155\n",
      "\n",
      "==================================================\n",
      "1-Year Mortality - ConvNext\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 38, np.int64(0): 48}\n",
      "   Minority class: 38 (44.2%)\n",
      "   Imbalance ratio: 1.26:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 43.8% (train), 45.5% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.542\n",
      "      Balanced Accuracy: 0.592\n",
      "      F1 Score: 0.571\n",
      "      MCC: 0.183\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.604\n",
      "      Balanced Accuracy: 0.550\n",
      "      F1 Score: 0.545\n",
      "      MCC: 0.100\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.59167\n",
      "      AUC-ROC: 0.592\n",
      "      Balanced Accuracy: 0.583\n",
      "      F1 Score: 0.667\n",
      "      MCC: 0.289\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.458\n",
      "      Balanced Accuracy: 0.508\n",
      "      F1 Score: 0.522\n",
      "      MCC: 0.017\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.425\n",
      "      Balanced Accuracy: 0.425\n",
      "      F1 Score: 0.480\n",
      "      MCC: -0.160\n",
      "\n",
      "==================================================\n",
      "2-Year Mortality - ConvNext\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 70, np.int64(0): 16}\n",
      "   Minority class: 16 (18.6%)\n",
      "   Imbalance ratio: 4.38:1\n",
      "   Severity: MILD IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 81.2% (train), 81.8% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.722\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.900\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.900\n",
      "      MCC: 0.000\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.73611\n",
      "      AUC-ROC: 0.736\n",
      "      Balanced Accuracy: 0.736\n",
      "      F1 Score: 0.813\n",
      "      MCC: 0.379\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.736\n",
      "      Balanced Accuracy: 0.750\n",
      "      F1 Score: 0.667\n",
      "      MCC: 0.392\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.708\n",
      "      Balanced Accuracy: 0.625\n",
      "      F1 Score: 0.643\n",
      "      MCC: 0.194\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.375\n",
      "      Balanced Accuracy: 0.653\n",
      "      F1 Score: 0.690\n",
      "      MCC: 0.236\n",
      "\n",
      "==================================================\n",
      "High-Grade vs Low-Grade - ConvNext\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 241\n",
      "   Class distribution: {np.int64(1): 129, np.int64(0): 112}\n",
      "   Minority class: 112 (46.5%)\n",
      "   Imbalance ratio: 1.15:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 180 samples\n",
      "   Testing: 61 samples\n",
      "   Positive rate: 53.3% (train), 54.1% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.943\n",
      "      Balanced Accuracy: 0.817\n",
      "      F1 Score: 0.836\n",
      "      MCC: 0.636\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.862\n",
      "      Balanced Accuracy: 0.799\n",
      "      F1 Score: 0.824\n",
      "      MCC: 0.603\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.95238\n",
      "      AUC-ROC: 0.952\n",
      "      Balanced Accuracy: 0.804\n",
      "      F1 Score: 0.857\n",
      "      MCC: 0.675\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.897\n",
      "      Balanced Accuracy: 0.799\n",
      "      F1 Score: 0.824\n",
      "      MCC: 0.603\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.899\n",
      "      Balanced Accuracy: 0.740\n",
      "      F1 Score: 0.800\n",
      "      MCC: 0.517\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.917\n",
      "      Balanced Accuracy: 0.781\n",
      "      F1 Score: 0.812\n",
      "      MCC: 0.570\n",
      "\n",
      "==================================================\n",
      "IDH Mutation Status - ConvNext\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 198\n",
      "   Class distribution: {np.float64(1.0): 174, np.float64(0.0): 24}\n",
      "   Minority class: 24 (12.1%)\n",
      "   Imbalance ratio: 7.25:1\n",
      "   Severity: MODERATE IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 148 samples\n",
      "   Testing: 50 samples\n",
      "   Positive rate: 87.8% (train), 88.0% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.909\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.936\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.750\n",
      "      Balanced Accuracy: 0.792\n",
      "      F1 Score: 0.846\n",
      "      MCC: 0.406\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.82197\n",
      "      AUC-ROC: 0.822\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.936\n",
      "      MCC: 0.000\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.886\n",
      "      Balanced Accuracy: 0.720\n",
      "      F1 Score: 0.850\n",
      "      MCC: 0.318\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.830\n",
      "      Balanced Accuracy: 0.625\n",
      "      F1 Score: 0.825\n",
      "      MCC: 0.181\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.727\n",
      "      Balanced Accuracy: 0.697\n",
      "      F1 Score: 0.821\n",
      "      MCC: 0.274\n",
      "\n",
      "==================================================\n",
      "MGMT Promoter Methylation - ConvNext\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 212\n",
      "   Class distribution: {np.float64(0.0): 128, np.float64(1.0): 84}\n",
      "   Minority class: 84 (39.6%)\n",
      "   Imbalance ratio: 1.52:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 159 samples\n",
      "   Testing: 53 samples\n",
      "   Positive rate: 39.6% (train), 39.6% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.619\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.687\n",
      "      Balanced Accuracy: 0.701\n",
      "      F1 Score: 0.652\n",
      "      MCC: 0.394\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.70685\n",
      "      AUC-ROC: 0.707\n",
      "      Balanced Accuracy: 0.605\n",
      "      F1 Score: 0.486\n",
      "      MCC: 0.224\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.684\n",
      "      Balanced Accuracy: 0.615\n",
      "      F1 Score: 0.571\n",
      "      MCC: 0.225\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.390\n",
      "      Balanced Accuracy: 0.646\n",
      "      F1 Score: 0.596\n",
      "      MCC: 0.285\n",
      "\n",
      "Loading ViT dataset...\n",
      "Loaded 532 patients with 356 features\n",
      "============================================================\n",
      "CREATING PREDICTION TARGETS\n",
      "============================================================\n",
      "MORTALITY TARGETS:\n",
      "   Patients: 86\n",
      "   6-month: 19/86 (22.1%)\n",
      "   1-year: 38/86 (44.2%)\n",
      "   2-year: 70/86 (81.4%)\n",
      "\n",
      "TUMOR CLASSIFICATION TARGETS:\n",
      "   Patients: 241\n",
      "   High-grade: 129/241 (53.5%)\n",
      "\n",
      "IDH MUTATION TARGETS:\n",
      "   Patients: 198\n",
      "   IDH Mutant: 174.0/198 (87.9%)\n",
      "\n",
      "MGMT METHYLATION TARGETS:\n",
      "   Patients: 212\n",
      "   MGMT Methylated: 84.0/212 (39.6%)\n",
      "Selected 13 features for analysis\n",
      "\n",
      "==================================================\n",
      "6-Month Mortality - ViT\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 19, np.int64(0): 67}\n",
      "   Minority class: 19 (22.1%)\n",
      "   Imbalance ratio: 3.53:1\n",
      "   Severity: MILD IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 21.9% (train), 22.7% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.694\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.706\n",
      "      Balanced Accuracy: 0.553\n",
      "      F1 Score: 0.333\n",
      "      MCC: 0.095\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_0_auc = 0.70588\n",
      "      AUC-ROC: 0.706\n",
      "      Balanced Accuracy: 0.588\n",
      "      F1 Score: 0.417\n",
      "      MCC: 0.215\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.741\n",
      "      Balanced Accuracy: 0.735\n",
      "      F1 Score: 0.526\n",
      "      MCC: 0.410\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.647\n",
      "      Balanced Accuracy: 0.582\n",
      "      F1 Score: 0.364\n",
      "      MCC: 0.155\n",
      "\n",
      "==================================================\n",
      "1-Year Mortality - ViT\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 38, np.int64(0): 48}\n",
      "   Minority class: 38 (44.2%)\n",
      "   Imbalance ratio: 1.26:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 43.8% (train), 45.5% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.542\n",
      "      Balanced Accuracy: 0.592\n",
      "      F1 Score: 0.571\n",
      "      MCC: 0.183\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.604\n",
      "      Balanced Accuracy: 0.550\n",
      "      F1 Score: 0.545\n",
      "      MCC: 0.100\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.59167\n",
      "      AUC-ROC: 0.592\n",
      "      Balanced Accuracy: 0.583\n",
      "      F1 Score: 0.667\n",
      "      MCC: 0.289\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.458\n",
      "      Balanced Accuracy: 0.508\n",
      "      F1 Score: 0.522\n",
      "      MCC: 0.017\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.425\n",
      "      Balanced Accuracy: 0.425\n",
      "      F1 Score: 0.480\n",
      "      MCC: -0.160\n",
      "\n",
      "==================================================\n",
      "2-Year Mortality - ViT\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 70, np.int64(0): 16}\n",
      "   Minority class: 16 (18.6%)\n",
      "   Imbalance ratio: 4.38:1\n",
      "   Severity: MILD IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 81.2% (train), 81.8% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.722\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.900\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.900\n",
      "      MCC: 0.000\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.73611\n",
      "      AUC-ROC: 0.736\n",
      "      Balanced Accuracy: 0.736\n",
      "      F1 Score: 0.813\n",
      "      MCC: 0.379\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.736\n",
      "      Balanced Accuracy: 0.750\n",
      "      F1 Score: 0.667\n",
      "      MCC: 0.392\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.708\n",
      "      Balanced Accuracy: 0.625\n",
      "      F1 Score: 0.643\n",
      "      MCC: 0.194\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.375\n",
      "      Balanced Accuracy: 0.653\n",
      "      F1 Score: 0.690\n",
      "      MCC: 0.236\n",
      "\n",
      "==================================================\n",
      "High-Grade vs Low-Grade - ViT\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 241\n",
      "   Class distribution: {np.int64(1): 129, np.int64(0): 112}\n",
      "   Minority class: 112 (46.5%)\n",
      "   Imbalance ratio: 1.15:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 180 samples\n",
      "   Testing: 61 samples\n",
      "   Positive rate: 53.3% (train), 54.1% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.943\n",
      "      Balanced Accuracy: 0.817\n",
      "      F1 Score: 0.836\n",
      "      MCC: 0.636\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.862\n",
      "      Balanced Accuracy: 0.799\n",
      "      F1 Score: 0.824\n",
      "      MCC: 0.603\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.95238\n",
      "      AUC-ROC: 0.952\n",
      "      Balanced Accuracy: 0.804\n",
      "      F1 Score: 0.857\n",
      "      MCC: 0.675\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.897\n",
      "      Balanced Accuracy: 0.799\n",
      "      F1 Score: 0.824\n",
      "      MCC: 0.603\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.899\n",
      "      Balanced Accuracy: 0.740\n",
      "      F1 Score: 0.800\n",
      "      MCC: 0.517\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.917\n",
      "      Balanced Accuracy: 0.781\n",
      "      F1 Score: 0.812\n",
      "      MCC: 0.570\n",
      "\n",
      "==================================================\n",
      "IDH Mutation Status - ViT\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 198\n",
      "   Class distribution: {np.float64(1.0): 174, np.float64(0.0): 24}\n",
      "   Minority class: 24 (12.1%)\n",
      "   Imbalance ratio: 7.25:1\n",
      "   Severity: MODERATE IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 148 samples\n",
      "   Testing: 50 samples\n",
      "   Positive rate: 87.8% (train), 88.0% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.909\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.936\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.750\n",
      "      Balanced Accuracy: 0.792\n",
      "      F1 Score: 0.846\n",
      "      MCC: 0.406\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.82197\n",
      "      AUC-ROC: 0.822\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.936\n",
      "      MCC: 0.000\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.886\n",
      "      Balanced Accuracy: 0.720\n",
      "      F1 Score: 0.850\n",
      "      MCC: 0.318\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.830\n",
      "      Balanced Accuracy: 0.625\n",
      "      F1 Score: 0.825\n",
      "      MCC: 0.181\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.727\n",
      "      Balanced Accuracy: 0.697\n",
      "      F1 Score: 0.821\n",
      "      MCC: 0.274\n",
      "\n",
      "==================================================\n",
      "MGMT Promoter Methylation - ViT\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 212\n",
      "   Class distribution: {np.float64(0.0): 128, np.float64(1.0): 84}\n",
      "   Minority class: 84 (39.6%)\n",
      "   Imbalance ratio: 1.52:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 159 samples\n",
      "   Testing: 53 samples\n",
      "   Positive rate: 39.6% (train), 39.6% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.619\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.687\n",
      "      Balanced Accuracy: 0.701\n",
      "      F1 Score: 0.652\n",
      "      MCC: 0.394\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.70685\n",
      "      AUC-ROC: 0.707\n",
      "      Balanced Accuracy: 0.605\n",
      "      F1 Score: 0.486\n",
      "      MCC: 0.224\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.684\n",
      "      Balanced Accuracy: 0.615\n",
      "      F1 Score: 0.571\n",
      "      MCC: 0.225\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.390\n",
      "      Balanced Accuracy: 0.646\n",
      "      F1 Score: 0.596\n",
      "      MCC: 0.285\n",
      "\n",
      "Loading ResNet50_Pretrained dataset...\n",
      "Loaded 532 patients with 356 features\n",
      "============================================================\n",
      "CREATING PREDICTION TARGETS\n",
      "============================================================\n",
      "MORTALITY TARGETS:\n",
      "   Patients: 86\n",
      "   6-month: 19/86 (22.1%)\n",
      "   1-year: 38/86 (44.2%)\n",
      "   2-year: 70/86 (81.4%)\n",
      "\n",
      "TUMOR CLASSIFICATION TARGETS:\n",
      "   Patients: 241\n",
      "   High-grade: 129/241 (53.5%)\n",
      "\n",
      "IDH MUTATION TARGETS:\n",
      "   Patients: 198\n",
      "   IDH Mutant: 174.0/198 (87.9%)\n",
      "\n",
      "MGMT METHYLATION TARGETS:\n",
      "   Patients: 212\n",
      "   MGMT Methylated: 84.0/212 (39.6%)\n",
      "Selected 13 features for analysis\n",
      "\n",
      "==================================================\n",
      "6-Month Mortality - ResNet50_Pretrained\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 19, np.int64(0): 67}\n",
      "   Minority class: 19 (22.1%)\n",
      "   Imbalance ratio: 3.53:1\n",
      "   Severity: MILD IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 21.9% (train), 22.7% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.694\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.706\n",
      "      Balanced Accuracy: 0.553\n",
      "      F1 Score: 0.333\n",
      "      MCC: 0.095\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_0_auc = 0.70588\n",
      "      AUC-ROC: 0.706\n",
      "      Balanced Accuracy: 0.588\n",
      "      F1 Score: 0.417\n",
      "      MCC: 0.215\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.741\n",
      "      Balanced Accuracy: 0.735\n",
      "      F1 Score: 0.526\n",
      "      MCC: 0.410\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.647\n",
      "      Balanced Accuracy: 0.582\n",
      "      F1 Score: 0.364\n",
      "      MCC: 0.155\n",
      "\n",
      "==================================================\n",
      "1-Year Mortality - ResNet50_Pretrained\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 38, np.int64(0): 48}\n",
      "   Minority class: 38 (44.2%)\n",
      "   Imbalance ratio: 1.26:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 43.8% (train), 45.5% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.542\n",
      "      Balanced Accuracy: 0.592\n",
      "      F1 Score: 0.571\n",
      "      MCC: 0.183\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.604\n",
      "      Balanced Accuracy: 0.550\n",
      "      F1 Score: 0.545\n",
      "      MCC: 0.100\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.59167\n",
      "      AUC-ROC: 0.592\n",
      "      Balanced Accuracy: 0.583\n",
      "      F1 Score: 0.667\n",
      "      MCC: 0.289\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.458\n",
      "      Balanced Accuracy: 0.508\n",
      "      F1 Score: 0.522\n",
      "      MCC: 0.017\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.425\n",
      "      Balanced Accuracy: 0.425\n",
      "      F1 Score: 0.480\n",
      "      MCC: -0.160\n",
      "\n",
      "==================================================\n",
      "2-Year Mortality - ResNet50_Pretrained\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 70, np.int64(0): 16}\n",
      "   Minority class: 16 (18.6%)\n",
      "   Imbalance ratio: 4.38:1\n",
      "   Severity: MILD IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 81.2% (train), 81.8% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.722\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.900\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.900\n",
      "      MCC: 0.000\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.73611\n",
      "      AUC-ROC: 0.736\n",
      "      Balanced Accuracy: 0.736\n",
      "      F1 Score: 0.813\n",
      "      MCC: 0.379\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.736\n",
      "      Balanced Accuracy: 0.750\n",
      "      F1 Score: 0.667\n",
      "      MCC: 0.392\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.708\n",
      "      Balanced Accuracy: 0.625\n",
      "      F1 Score: 0.643\n",
      "      MCC: 0.194\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.375\n",
      "      Balanced Accuracy: 0.653\n",
      "      F1 Score: 0.690\n",
      "      MCC: 0.236\n",
      "\n",
      "==================================================\n",
      "High-Grade vs Low-Grade - ResNet50_Pretrained\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 241\n",
      "   Class distribution: {np.int64(1): 129, np.int64(0): 112}\n",
      "   Minority class: 112 (46.5%)\n",
      "   Imbalance ratio: 1.15:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 180 samples\n",
      "   Testing: 61 samples\n",
      "   Positive rate: 53.3% (train), 54.1% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.943\n",
      "      Balanced Accuracy: 0.817\n",
      "      F1 Score: 0.836\n",
      "      MCC: 0.636\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.862\n",
      "      Balanced Accuracy: 0.799\n",
      "      F1 Score: 0.824\n",
      "      MCC: 0.603\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.95238\n",
      "      AUC-ROC: 0.952\n",
      "      Balanced Accuracy: 0.804\n",
      "      F1 Score: 0.857\n",
      "      MCC: 0.675\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.897\n",
      "      Balanced Accuracy: 0.799\n",
      "      F1 Score: 0.824\n",
      "      MCC: 0.603\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.899\n",
      "      Balanced Accuracy: 0.740\n",
      "      F1 Score: 0.800\n",
      "      MCC: 0.517\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.917\n",
      "      Balanced Accuracy: 0.781\n",
      "      F1 Score: 0.812\n",
      "      MCC: 0.570\n",
      "\n",
      "==================================================\n",
      "IDH Mutation Status - ResNet50_Pretrained\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 198\n",
      "   Class distribution: {np.float64(1.0): 174, np.float64(0.0): 24}\n",
      "   Minority class: 24 (12.1%)\n",
      "   Imbalance ratio: 7.25:1\n",
      "   Severity: MODERATE IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 148 samples\n",
      "   Testing: 50 samples\n",
      "   Positive rate: 87.8% (train), 88.0% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.909\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.936\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.750\n",
      "      Balanced Accuracy: 0.792\n",
      "      F1 Score: 0.846\n",
      "      MCC: 0.406\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.82197\n",
      "      AUC-ROC: 0.822\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.936\n",
      "      MCC: 0.000\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.886\n",
      "      Balanced Accuracy: 0.720\n",
      "      F1 Score: 0.850\n",
      "      MCC: 0.318\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.830\n",
      "      Balanced Accuracy: 0.625\n",
      "      F1 Score: 0.825\n",
      "      MCC: 0.181\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.727\n",
      "      Balanced Accuracy: 0.697\n",
      "      F1 Score: 0.821\n",
      "      MCC: 0.274\n",
      "\n",
      "==================================================\n",
      "MGMT Promoter Methylation - ResNet50_Pretrained\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 212\n",
      "   Class distribution: {np.float64(0.0): 128, np.float64(1.0): 84}\n",
      "   Minority class: 84 (39.6%)\n",
      "   Imbalance ratio: 1.52:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 159 samples\n",
      "   Testing: 53 samples\n",
      "   Positive rate: 39.6% (train), 39.6% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.619\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.687\n",
      "      Balanced Accuracy: 0.701\n",
      "      F1 Score: 0.652\n",
      "      MCC: 0.394\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.70685\n",
      "      AUC-ROC: 0.707\n",
      "      Balanced Accuracy: 0.605\n",
      "      F1 Score: 0.486\n",
      "      MCC: 0.224\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.684\n",
      "      Balanced Accuracy: 0.615\n",
      "      F1 Score: 0.571\n",
      "      MCC: 0.225\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.390\n",
      "      Balanced Accuracy: 0.646\n",
      "      F1 Score: 0.596\n",
      "      MCC: 0.285\n",
      "\n",
      "Loading ResNet50_ImageNet dataset...\n",
      "Loaded 532 patients with 356 features\n",
      "============================================================\n",
      "CREATING PREDICTION TARGETS\n",
      "============================================================\n",
      "MORTALITY TARGETS:\n",
      "   Patients: 86\n",
      "   6-month: 19/86 (22.1%)\n",
      "   1-year: 38/86 (44.2%)\n",
      "   2-year: 70/86 (81.4%)\n",
      "\n",
      "TUMOR CLASSIFICATION TARGETS:\n",
      "   Patients: 241\n",
      "   High-grade: 129/241 (53.5%)\n",
      "\n",
      "IDH MUTATION TARGETS:\n",
      "   Patients: 198\n",
      "   IDH Mutant: 174.0/198 (87.9%)\n",
      "\n",
      "MGMT METHYLATION TARGETS:\n",
      "   Patients: 212\n",
      "   MGMT Methylated: 84.0/212 (39.6%)\n",
      "Selected 13 features for analysis\n",
      "\n",
      "==================================================\n",
      "6-Month Mortality - ResNet50_ImageNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 19, np.int64(0): 67}\n",
      "   Minority class: 19 (22.1%)\n",
      "   Imbalance ratio: 3.53:1\n",
      "   Severity: MILD IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 21.9% (train), 22.7% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.694\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.706\n",
      "      Balanced Accuracy: 0.553\n",
      "      F1 Score: 0.333\n",
      "      MCC: 0.095\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_0_auc = 0.70588\n",
      "      AUC-ROC: 0.706\n",
      "      Balanced Accuracy: 0.588\n",
      "      F1 Score: 0.417\n",
      "      MCC: 0.215\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.741\n",
      "      Balanced Accuracy: 0.735\n",
      "      F1 Score: 0.526\n",
      "      MCC: 0.410\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.647\n",
      "      Balanced Accuracy: 0.582\n",
      "      F1 Score: 0.364\n",
      "      MCC: 0.155\n",
      "\n",
      "==================================================\n",
      "1-Year Mortality - ResNet50_ImageNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 38, np.int64(0): 48}\n",
      "   Minority class: 38 (44.2%)\n",
      "   Imbalance ratio: 1.26:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 43.8% (train), 45.5% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.542\n",
      "      Balanced Accuracy: 0.592\n",
      "      F1 Score: 0.571\n",
      "      MCC: 0.183\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.604\n",
      "      Balanced Accuracy: 0.550\n",
      "      F1 Score: 0.545\n",
      "      MCC: 0.100\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.59167\n",
      "      AUC-ROC: 0.592\n",
      "      Balanced Accuracy: 0.583\n",
      "      F1 Score: 0.667\n",
      "      MCC: 0.289\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.458\n",
      "      Balanced Accuracy: 0.508\n",
      "      F1 Score: 0.522\n",
      "      MCC: 0.017\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.425\n",
      "      Balanced Accuracy: 0.425\n",
      "      F1 Score: 0.480\n",
      "      MCC: -0.160\n",
      "\n",
      "==================================================\n",
      "2-Year Mortality - ResNet50_ImageNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 70, np.int64(0): 16}\n",
      "   Minority class: 16 (18.6%)\n",
      "   Imbalance ratio: 4.38:1\n",
      "   Severity: MILD IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 81.2% (train), 81.8% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.722\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.900\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.900\n",
      "      MCC: 0.000\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.73611\n",
      "      AUC-ROC: 0.736\n",
      "      Balanced Accuracy: 0.736\n",
      "      F1 Score: 0.813\n",
      "      MCC: 0.379\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.736\n",
      "      Balanced Accuracy: 0.750\n",
      "      F1 Score: 0.667\n",
      "      MCC: 0.392\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.708\n",
      "      Balanced Accuracy: 0.625\n",
      "      F1 Score: 0.643\n",
      "      MCC: 0.194\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.375\n",
      "      Balanced Accuracy: 0.653\n",
      "      F1 Score: 0.690\n",
      "      MCC: 0.236\n",
      "\n",
      "==================================================\n",
      "High-Grade vs Low-Grade - ResNet50_ImageNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 241\n",
      "   Class distribution: {np.int64(1): 129, np.int64(0): 112}\n",
      "   Minority class: 112 (46.5%)\n",
      "   Imbalance ratio: 1.15:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 180 samples\n",
      "   Testing: 61 samples\n",
      "   Positive rate: 53.3% (train), 54.1% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.943\n",
      "      Balanced Accuracy: 0.817\n",
      "      F1 Score: 0.836\n",
      "      MCC: 0.636\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.862\n",
      "      Balanced Accuracy: 0.799\n",
      "      F1 Score: 0.824\n",
      "      MCC: 0.603\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.95238\n",
      "      AUC-ROC: 0.952\n",
      "      Balanced Accuracy: 0.804\n",
      "      F1 Score: 0.857\n",
      "      MCC: 0.675\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.897\n",
      "      Balanced Accuracy: 0.799\n",
      "      F1 Score: 0.824\n",
      "      MCC: 0.603\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.899\n",
      "      Balanced Accuracy: 0.740\n",
      "      F1 Score: 0.800\n",
      "      MCC: 0.517\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.917\n",
      "      Balanced Accuracy: 0.781\n",
      "      F1 Score: 0.812\n",
      "      MCC: 0.570\n",
      "\n",
      "==================================================\n",
      "IDH Mutation Status - ResNet50_ImageNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 198\n",
      "   Class distribution: {np.float64(1.0): 174, np.float64(0.0): 24}\n",
      "   Minority class: 24 (12.1%)\n",
      "   Imbalance ratio: 7.25:1\n",
      "   Severity: MODERATE IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 148 samples\n",
      "   Testing: 50 samples\n",
      "   Positive rate: 87.8% (train), 88.0% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.909\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.936\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.750\n",
      "      Balanced Accuracy: 0.792\n",
      "      F1 Score: 0.846\n",
      "      MCC: 0.406\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.82197\n",
      "      AUC-ROC: 0.822\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.936\n",
      "      MCC: 0.000\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.886\n",
      "      Balanced Accuracy: 0.720\n",
      "      F1 Score: 0.850\n",
      "      MCC: 0.318\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.830\n",
      "      Balanced Accuracy: 0.625\n",
      "      F1 Score: 0.825\n",
      "      MCC: 0.181\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.727\n",
      "      Balanced Accuracy: 0.697\n",
      "      F1 Score: 0.821\n",
      "      MCC: 0.274\n",
      "\n",
      "==================================================\n",
      "MGMT Promoter Methylation - ResNet50_ImageNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 212\n",
      "   Class distribution: {np.float64(0.0): 128, np.float64(1.0): 84}\n",
      "   Minority class: 84 (39.6%)\n",
      "   Imbalance ratio: 1.52:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 159 samples\n",
      "   Testing: 53 samples\n",
      "   Positive rate: 39.6% (train), 39.6% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.619\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.687\n",
      "      Balanced Accuracy: 0.701\n",
      "      F1 Score: 0.652\n",
      "      MCC: 0.394\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.70685\n",
      "      AUC-ROC: 0.707\n",
      "      Balanced Accuracy: 0.605\n",
      "      F1 Score: 0.486\n",
      "      MCC: 0.224\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.684\n",
      "      Balanced Accuracy: 0.615\n",
      "      F1 Score: 0.571\n",
      "      MCC: 0.225\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.390\n",
      "      Balanced Accuracy: 0.646\n",
      "      F1 Score: 0.596\n",
      "      MCC: 0.285\n",
      "\n",
      "Loading EfficientNet dataset...\n",
      "Loaded 532 patients with 356 features\n",
      "============================================================\n",
      "CREATING PREDICTION TARGETS\n",
      "============================================================\n",
      "MORTALITY TARGETS:\n",
      "   Patients: 86\n",
      "   6-month: 19/86 (22.1%)\n",
      "   1-year: 38/86 (44.2%)\n",
      "   2-year: 70/86 (81.4%)\n",
      "\n",
      "TUMOR CLASSIFICATION TARGETS:\n",
      "   Patients: 241\n",
      "   High-grade: 129/241 (53.5%)\n",
      "\n",
      "IDH MUTATION TARGETS:\n",
      "   Patients: 198\n",
      "   IDH Mutant: 174.0/198 (87.9%)\n",
      "\n",
      "MGMT METHYLATION TARGETS:\n",
      "   Patients: 212\n",
      "   MGMT Methylated: 84.0/212 (39.6%)\n",
      "Selected 13 features for analysis\n",
      "\n",
      "==================================================\n",
      "6-Month Mortality - EfficientNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 19, np.int64(0): 67}\n",
      "   Minority class: 19 (22.1%)\n",
      "   Imbalance ratio: 3.53:1\n",
      "   Severity: MILD IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 21.9% (train), 22.7% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.694\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.706\n",
      "      Balanced Accuracy: 0.553\n",
      "      F1 Score: 0.333\n",
      "      MCC: 0.095\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_0_auc = 0.70588\n",
      "      AUC-ROC: 0.706\n",
      "      Balanced Accuracy: 0.588\n",
      "      F1 Score: 0.417\n",
      "      MCC: 0.215\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.741\n",
      "      Balanced Accuracy: 0.735\n",
      "      F1 Score: 0.526\n",
      "      MCC: 0.410\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.647\n",
      "      Balanced Accuracy: 0.582\n",
      "      F1 Score: 0.364\n",
      "      MCC: 0.155\n",
      "\n",
      "==================================================\n",
      "1-Year Mortality - EfficientNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 38, np.int64(0): 48}\n",
      "   Minority class: 38 (44.2%)\n",
      "   Imbalance ratio: 1.26:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 43.8% (train), 45.5% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.542\n",
      "      Balanced Accuracy: 0.592\n",
      "      F1 Score: 0.571\n",
      "      MCC: 0.183\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.604\n",
      "      Balanced Accuracy: 0.550\n",
      "      F1 Score: 0.545\n",
      "      MCC: 0.100\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.59167\n",
      "      AUC-ROC: 0.592\n",
      "      Balanced Accuracy: 0.583\n",
      "      F1 Score: 0.667\n",
      "      MCC: 0.289\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.458\n",
      "      Balanced Accuracy: 0.508\n",
      "      F1 Score: 0.522\n",
      "      MCC: 0.017\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.425\n",
      "      Balanced Accuracy: 0.425\n",
      "      F1 Score: 0.480\n",
      "      MCC: -0.160\n",
      "\n",
      "==================================================\n",
      "2-Year Mortality - EfficientNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 86\n",
      "   Class distribution: {np.int64(1): 70, np.int64(0): 16}\n",
      "   Minority class: 16 (18.6%)\n",
      "   Imbalance ratio: 4.38:1\n",
      "   Severity: MILD IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 64 samples\n",
      "   Testing: 22 samples\n",
      "   Positive rate: 81.2% (train), 81.8% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.722\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.900\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.900\n",
      "      MCC: 0.000\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_0_auc = 0.73611\n",
      "      AUC-ROC: 0.736\n",
      "      Balanced Accuracy: 0.736\n",
      "      F1 Score: 0.813\n",
      "      MCC: 0.379\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.736\n",
      "      Balanced Accuracy: 0.750\n",
      "      F1 Score: 0.667\n",
      "      MCC: 0.392\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.708\n",
      "      Balanced Accuracy: 0.625\n",
      "      F1 Score: 0.643\n",
      "      MCC: 0.194\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.375\n",
      "      Balanced Accuracy: 0.653\n",
      "      F1 Score: 0.690\n",
      "      MCC: 0.236\n",
      "\n",
      "==================================================\n",
      "High-Grade vs Low-Grade - EfficientNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 241\n",
      "   Class distribution: {np.int64(1): 129, np.int64(0): 112}\n",
      "   Minority class: 112 (46.5%)\n",
      "   Imbalance ratio: 1.15:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 180 samples\n",
      "   Testing: 61 samples\n",
      "   Positive rate: 53.3% (train), 54.1% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.943\n",
      "      Balanced Accuracy: 0.817\n",
      "      F1 Score: 0.836\n",
      "      MCC: 0.636\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.862\n",
      "      Balanced Accuracy: 0.799\n",
      "      F1 Score: 0.824\n",
      "      MCC: 0.603\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.95238\n",
      "      AUC-ROC: 0.952\n",
      "      Balanced Accuracy: 0.804\n",
      "      F1 Score: 0.857\n",
      "      MCC: 0.675\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.897\n",
      "      Balanced Accuracy: 0.799\n",
      "      F1 Score: 0.824\n",
      "      MCC: 0.603\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.899\n",
      "      Balanced Accuracy: 0.740\n",
      "      F1 Score: 0.800\n",
      "      MCC: 0.517\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.917\n",
      "      Balanced Accuracy: 0.781\n",
      "      F1 Score: 0.812\n",
      "      MCC: 0.570\n",
      "\n",
      "==================================================\n",
      "IDH Mutation Status - EfficientNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 198\n",
      "   Class distribution: {np.float64(1.0): 174, np.float64(0.0): 24}\n",
      "   Minority class: 24 (12.1%)\n",
      "   Imbalance ratio: 7.25:1\n",
      "   Severity: MODERATE IMBALANCE\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 148 samples\n",
      "   Testing: 50 samples\n",
      "   Positive rate: 87.8% (train), 88.0% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.909\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.936\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.750\n",
      "      Balanced Accuracy: 0.792\n",
      "      F1 Score: 0.846\n",
      "      MCC: 0.406\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.82197\n",
      "      AUC-ROC: 0.822\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.936\n",
      "      MCC: 0.000\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.886\n",
      "      Balanced Accuracy: 0.720\n",
      "      F1 Score: 0.850\n",
      "      MCC: 0.318\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.830\n",
      "      Balanced Accuracy: 0.625\n",
      "      F1 Score: 0.825\n",
      "      MCC: 0.181\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.727\n",
      "      Balanced Accuracy: 0.697\n",
      "      F1 Score: 0.821\n",
      "      MCC: 0.274\n",
      "\n",
      "==================================================\n",
      "MGMT Promoter Methylation - EfficientNet\n",
      "==================================================\n",
      "CLASS IMBALANCE ANALYSIS:\n",
      "   Total samples: 212\n",
      "   Class distribution: {np.float64(0.0): 128, np.float64(1.0): 84}\n",
      "   Minority class: 84 (39.6%)\n",
      "   Imbalance ratio: 1.52:1\n",
      "   Severity: BALANCED\n",
      "\n",
      "DATA SPLIT:\n",
      "   Training: 159 samples\n",
      "   Testing: 53 samples\n",
      "   Positive rate: 39.6% (train), 39.6% (test)\n",
      "\n",
      "ALGORITHM EVALUATION:\n",
      "\n",
      "   TabPFN:\n",
      "      AUC-ROC: 0.619\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   XGBoost:\n",
      "      AUC-ROC: 0.687\n",
      "      Balanced Accuracy: 0.701\n",
      "      F1 Score: 0.652\n",
      "      MCC: 0.394\n",
      "\n",
      "   TabNet:\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.70685\n",
      "      AUC-ROC: 0.707\n",
      "      Balanced Accuracy: 0.605\n",
      "      F1 Score: 0.486\n",
      "      MCC: 0.224\n",
      "\n",
      "   RandomForest:\n",
      "      AUC-ROC: 0.684\n",
      "      Balanced Accuracy: 0.615\n",
      "      F1 Score: 0.571\n",
      "      MCC: 0.225\n",
      "\n",
      "   LogisticRegression:\n",
      "      AUC-ROC: 0.500\n",
      "      Balanced Accuracy: 0.500\n",
      "      F1 Score: 0.000\n",
      "      MCC: 0.000\n",
      "\n",
      "   SVM:\n",
      "      AUC-ROC: 0.390\n",
      "      Balanced Accuracy: 0.646\n",
      "      F1 Score: 0.596\n",
      "      MCC: 0.285\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE ANALYSIS REPORT\n",
      "================================================================================\n",
      "\n",
      "EXECUTIVE SUMMARY\n",
      "==================================================\n",
      "PERFORMANCE OVERVIEW:\n",
      "   Total algorithm-task combinations: 180\n",
      "   Mean AUC across all tests: 0.691\n",
      "   Best AUC achieved: 0.952\n",
      "   Excellent performance (AUC >= 0.85): 40/180 (22.2%)\n",
      "   Good+ performance (AUC >= 0.75): 55/180 (30.6%)\n",
      "   CLINICAL DEPLOYMENT: 40 combinations ready for validation\n",
      "   OUTSTANDING: Exceptional results achieved\n",
      "\n",
      "DETAILED RESULTS TABLE\n",
      "==================================================\n",
      "CNN                  Task                      Algorithm       AUC      Acc      Sens     Spec     Status         \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "ConvNext             6-Month Mortality         TabPFN          0.694    0.773    0.000    1.000    GOOD           \n",
      "ConvNext             6-Month Mortality         XGBoost         0.706    0.636    0.400    0.706    GOOD           \n",
      "ConvNext             6-Month Mortality         TabNet          0.706    0.364    1.000    0.176    GOOD           \n",
      "ConvNext             6-Month Mortality         RandomForest    0.741    0.591    1.000    0.471    GOOD           \n",
      "ConvNext             6-Month Mortality         LogisticRegression 0.500    0.773    0.000    1.000    MODERATE       \n",
      "ConvNext             6-Month Mortality         SVM             0.647    0.682    0.400    0.765    MODERATE       \n",
      "ConvNext             1-Year Mortality          TabPFN          0.542    0.591    0.600    0.583    MODERATE       \n",
      "ConvNext             1-Year Mortality          XGBoost         0.604    0.545    0.600    0.500    MODERATE       \n",
      "ConvNext             1-Year Mortality          TabNet          0.592    0.545    1.000    0.167    MODERATE       \n",
      "ConvNext             1-Year Mortality          RandomForest    0.458    0.500    0.600    0.417    MODERATE       \n",
      "ConvNext             1-Year Mortality          LogisticRegression 0.500    0.545    0.000    1.000    MODERATE       \n",
      "ConvNext             1-Year Mortality          SVM             0.425    0.409    0.600    0.250    MODERATE       \n",
      "ConvNext             2-Year Mortality          TabPFN          0.722    0.818    1.000    0.000    GOOD           \n",
      "ConvNext             2-Year Mortality          XGBoost         0.500    0.818    1.000    0.000    MODERATE       \n",
      "ConvNext             2-Year Mortality          TabNet          0.736    0.727    0.722    0.750    GOOD           \n",
      "ConvNext             2-Year Mortality          RandomForest    0.736    0.591    0.500    1.000    GOOD           \n",
      "ConvNext             2-Year Mortality          LogisticRegression 0.708    0.545    0.500    0.750    GOOD           \n",
      "ConvNext             2-Year Mortality          SVM             0.375    0.591    0.556    0.750    MODERATE       \n",
      "ConvNext             High-Grade vs Low-Grade   TabPFN          0.943    0.820    0.848    0.786    EXCELLENT      \n",
      "ConvNext             High-Grade vs Low-Grade   XGBoost         0.862    0.803    0.848    0.750    EXCELLENT      \n",
      "ConvNext             High-Grade vs Low-Grade   TabNet          0.952    0.820    1.000    0.607    EXCELLENT      \n",
      "ConvNext             High-Grade vs Low-Grade   RandomForest    0.897    0.803    0.848    0.750    EXCELLENT      \n",
      "ConvNext             High-Grade vs Low-Grade   LogisticRegression 0.899    0.754    0.909    0.571    EXCELLENT      \n",
      "ConvNext             High-Grade vs Low-Grade   SVM             0.917    0.787    0.848    0.714    EXCELLENT      \n",
      "ConvNext             IDH Mutation Status       TabPFN          0.909    0.880    1.000    0.000    EXCELLENT      \n",
      "ConvNext             IDH Mutation Status       XGBoost         0.750    0.760    0.750    0.833    STRONG         \n",
      "ConvNext             IDH Mutation Status       TabNet          0.822    0.880    1.000    0.000    STRONG         \n",
      "ConvNext             IDH Mutation Status       RandomForest    0.886    0.760    0.773    0.667    EXCELLENT      \n",
      "ConvNext             IDH Mutation Status       LogisticRegression 0.830    0.720    0.750    0.500    STRONG         \n",
      "ConvNext             IDH Mutation Status       SVM             0.727    0.720    0.727    0.667    GOOD           \n",
      "ConvNext             MGMT Promoter Methylation TabPFN          0.619    0.604    0.000    1.000    MODERATE       \n",
      "ConvNext             MGMT Promoter Methylation XGBoost         0.687    0.698    0.714    0.688    GOOD           \n",
      "ConvNext             MGMT Promoter Methylation TabNet          0.707    0.642    0.429    0.781    GOOD           \n",
      "ConvNext             MGMT Promoter Methylation RandomForest    0.684    0.604    0.667    0.562    GOOD           \n",
      "ConvNext             MGMT Promoter Methylation LogisticRegression 0.500    0.604    0.000    1.000    MODERATE       \n",
      "ConvNext             MGMT Promoter Methylation SVM             0.390    0.642    0.667    0.625    MODERATE       \n",
      "ViT                  6-Month Mortality         TabPFN          0.694    0.773    0.000    1.000    GOOD           \n",
      "ViT                  6-Month Mortality         XGBoost         0.706    0.636    0.400    0.706    GOOD           \n",
      "ViT                  6-Month Mortality         TabNet          0.706    0.364    1.000    0.176    GOOD           \n",
      "ViT                  6-Month Mortality         RandomForest    0.741    0.591    1.000    0.471    GOOD           \n",
      "ViT                  6-Month Mortality         LogisticRegression 0.500    0.773    0.000    1.000    MODERATE       \n",
      "ViT                  6-Month Mortality         SVM             0.647    0.682    0.400    0.765    MODERATE       \n",
      "ViT                  1-Year Mortality          TabPFN          0.542    0.591    0.600    0.583    MODERATE       \n",
      "ViT                  1-Year Mortality          XGBoost         0.604    0.545    0.600    0.500    MODERATE       \n",
      "ViT                  1-Year Mortality          TabNet          0.592    0.545    1.000    0.167    MODERATE       \n",
      "ViT                  1-Year Mortality          RandomForest    0.458    0.500    0.600    0.417    MODERATE       \n",
      "ViT                  1-Year Mortality          LogisticRegression 0.500    0.545    0.000    1.000    MODERATE       \n",
      "ViT                  1-Year Mortality          SVM             0.425    0.409    0.600    0.250    MODERATE       \n",
      "ViT                  2-Year Mortality          TabPFN          0.722    0.818    1.000    0.000    GOOD           \n",
      "ViT                  2-Year Mortality          XGBoost         0.500    0.818    1.000    0.000    MODERATE       \n",
      "ViT                  2-Year Mortality          TabNet          0.736    0.727    0.722    0.750    GOOD           \n",
      "ViT                  2-Year Mortality          RandomForest    0.736    0.591    0.500    1.000    GOOD           \n",
      "ViT                  2-Year Mortality          LogisticRegression 0.708    0.545    0.500    0.750    GOOD           \n",
      "ViT                  2-Year Mortality          SVM             0.375    0.591    0.556    0.750    MODERATE       \n",
      "ViT                  High-Grade vs Low-Grade   TabPFN          0.943    0.820    0.848    0.786    EXCELLENT      \n",
      "ViT                  High-Grade vs Low-Grade   XGBoost         0.862    0.803    0.848    0.750    EXCELLENT      \n",
      "ViT                  High-Grade vs Low-Grade   TabNet          0.952    0.820    1.000    0.607    EXCELLENT      \n",
      "ViT                  High-Grade vs Low-Grade   RandomForest    0.897    0.803    0.848    0.750    EXCELLENT      \n",
      "ViT                  High-Grade vs Low-Grade   LogisticRegression 0.899    0.754    0.909    0.571    EXCELLENT      \n",
      "ViT                  High-Grade vs Low-Grade   SVM             0.917    0.787    0.848    0.714    EXCELLENT      \n",
      "ViT                  IDH Mutation Status       TabPFN          0.909    0.880    1.000    0.000    EXCELLENT      \n",
      "ViT                  IDH Mutation Status       XGBoost         0.750    0.760    0.750    0.833    STRONG         \n",
      "ViT                  IDH Mutation Status       TabNet          0.822    0.880    1.000    0.000    STRONG         \n",
      "ViT                  IDH Mutation Status       RandomForest    0.886    0.760    0.773    0.667    EXCELLENT      \n",
      "ViT                  IDH Mutation Status       LogisticRegression 0.830    0.720    0.750    0.500    STRONG         \n",
      "ViT                  IDH Mutation Status       SVM             0.727    0.720    0.727    0.667    GOOD           \n",
      "ViT                  MGMT Promoter Methylation TabPFN          0.619    0.604    0.000    1.000    MODERATE       \n",
      "ViT                  MGMT Promoter Methylation XGBoost         0.687    0.698    0.714    0.688    GOOD           \n",
      "ViT                  MGMT Promoter Methylation TabNet          0.707    0.642    0.429    0.781    GOOD           \n",
      "ViT                  MGMT Promoter Methylation RandomForest    0.684    0.604    0.667    0.562    GOOD           \n",
      "ViT                  MGMT Promoter Methylation LogisticRegression 0.500    0.604    0.000    1.000    MODERATE       \n",
      "ViT                  MGMT Promoter Methylation SVM             0.390    0.642    0.667    0.625    MODERATE       \n",
      "ResNet50_Pretrained  6-Month Mortality         TabPFN          0.694    0.773    0.000    1.000    GOOD           \n",
      "ResNet50_Pretrained  6-Month Mortality         XGBoost         0.706    0.636    0.400    0.706    GOOD           \n",
      "ResNet50_Pretrained  6-Month Mortality         TabNet          0.706    0.364    1.000    0.176    GOOD           \n",
      "ResNet50_Pretrained  6-Month Mortality         RandomForest    0.741    0.591    1.000    0.471    GOOD           \n",
      "ResNet50_Pretrained  6-Month Mortality         LogisticRegression 0.500    0.773    0.000    1.000    MODERATE       \n",
      "ResNet50_Pretrained  6-Month Mortality         SVM             0.647    0.682    0.400    0.765    MODERATE       \n",
      "ResNet50_Pretrained  1-Year Mortality          TabPFN          0.542    0.591    0.600    0.583    MODERATE       \n",
      "ResNet50_Pretrained  1-Year Mortality          XGBoost         0.604    0.545    0.600    0.500    MODERATE       \n",
      "ResNet50_Pretrained  1-Year Mortality          TabNet          0.592    0.545    1.000    0.167    MODERATE       \n",
      "ResNet50_Pretrained  1-Year Mortality          RandomForest    0.458    0.500    0.600    0.417    MODERATE       \n",
      "ResNet50_Pretrained  1-Year Mortality          LogisticRegression 0.500    0.545    0.000    1.000    MODERATE       \n",
      "ResNet50_Pretrained  1-Year Mortality          SVM             0.425    0.409    0.600    0.250    MODERATE       \n",
      "ResNet50_Pretrained  2-Year Mortality          TabPFN          0.722    0.818    1.000    0.000    GOOD           \n",
      "ResNet50_Pretrained  2-Year Mortality          XGBoost         0.500    0.818    1.000    0.000    MODERATE       \n",
      "ResNet50_Pretrained  2-Year Mortality          TabNet          0.736    0.727    0.722    0.750    GOOD           \n",
      "ResNet50_Pretrained  2-Year Mortality          RandomForest    0.736    0.591    0.500    1.000    GOOD           \n",
      "ResNet50_Pretrained  2-Year Mortality          LogisticRegression 0.708    0.545    0.500    0.750    GOOD           \n",
      "ResNet50_Pretrained  2-Year Mortality          SVM             0.375    0.591    0.556    0.750    MODERATE       \n",
      "ResNet50_Pretrained  High-Grade vs Low-Grade   TabPFN          0.943    0.820    0.848    0.786    EXCELLENT      \n",
      "ResNet50_Pretrained  High-Grade vs Low-Grade   XGBoost         0.862    0.803    0.848    0.750    EXCELLENT      \n",
      "ResNet50_Pretrained  High-Grade vs Low-Grade   TabNet          0.952    0.820    1.000    0.607    EXCELLENT      \n",
      "ResNet50_Pretrained  High-Grade vs Low-Grade   RandomForest    0.897    0.803    0.848    0.750    EXCELLENT      \n",
      "ResNet50_Pretrained  High-Grade vs Low-Grade   LogisticRegression 0.899    0.754    0.909    0.571    EXCELLENT      \n",
      "ResNet50_Pretrained  High-Grade vs Low-Grade   SVM             0.917    0.787    0.848    0.714    EXCELLENT      \n",
      "ResNet50_Pretrained  IDH Mutation Status       TabPFN          0.909    0.880    1.000    0.000    EXCELLENT      \n",
      "ResNet50_Pretrained  IDH Mutation Status       XGBoost         0.750    0.760    0.750    0.833    STRONG         \n",
      "ResNet50_Pretrained  IDH Mutation Status       TabNet          0.822    0.880    1.000    0.000    STRONG         \n",
      "ResNet50_Pretrained  IDH Mutation Status       RandomForest    0.886    0.760    0.773    0.667    EXCELLENT      \n",
      "ResNet50_Pretrained  IDH Mutation Status       LogisticRegression 0.830    0.720    0.750    0.500    STRONG         \n",
      "ResNet50_Pretrained  IDH Mutation Status       SVM             0.727    0.720    0.727    0.667    GOOD           \n",
      "ResNet50_Pretrained  MGMT Promoter Methylation TabPFN          0.619    0.604    0.000    1.000    MODERATE       \n",
      "ResNet50_Pretrained  MGMT Promoter Methylation XGBoost         0.687    0.698    0.714    0.688    GOOD           \n",
      "ResNet50_Pretrained  MGMT Promoter Methylation TabNet          0.707    0.642    0.429    0.781    GOOD           \n",
      "ResNet50_Pretrained  MGMT Promoter Methylation RandomForest    0.684    0.604    0.667    0.562    GOOD           \n",
      "ResNet50_Pretrained  MGMT Promoter Methylation LogisticRegression 0.500    0.604    0.000    1.000    MODERATE       \n",
      "ResNet50_Pretrained  MGMT Promoter Methylation SVM             0.390    0.642    0.667    0.625    MODERATE       \n",
      "ResNet50_ImageNet    6-Month Mortality         TabPFN          0.694    0.773    0.000    1.000    GOOD           \n",
      "ResNet50_ImageNet    6-Month Mortality         XGBoost         0.706    0.636    0.400    0.706    GOOD           \n",
      "ResNet50_ImageNet    6-Month Mortality         TabNet          0.706    0.364    1.000    0.176    GOOD           \n",
      "ResNet50_ImageNet    6-Month Mortality         RandomForest    0.741    0.591    1.000    0.471    GOOD           \n",
      "ResNet50_ImageNet    6-Month Mortality         LogisticRegression 0.500    0.773    0.000    1.000    MODERATE       \n",
      "ResNet50_ImageNet    6-Month Mortality         SVM             0.647    0.682    0.400    0.765    MODERATE       \n",
      "ResNet50_ImageNet    1-Year Mortality          TabPFN          0.542    0.591    0.600    0.583    MODERATE       \n",
      "ResNet50_ImageNet    1-Year Mortality          XGBoost         0.604    0.545    0.600    0.500    MODERATE       \n",
      "ResNet50_ImageNet    1-Year Mortality          TabNet          0.592    0.545    1.000    0.167    MODERATE       \n",
      "ResNet50_ImageNet    1-Year Mortality          RandomForest    0.458    0.500    0.600    0.417    MODERATE       \n",
      "ResNet50_ImageNet    1-Year Mortality          LogisticRegression 0.500    0.545    0.000    1.000    MODERATE       \n",
      "ResNet50_ImageNet    1-Year Mortality          SVM             0.425    0.409    0.600    0.250    MODERATE       \n",
      "ResNet50_ImageNet    2-Year Mortality          TabPFN          0.722    0.818    1.000    0.000    GOOD           \n",
      "ResNet50_ImageNet    2-Year Mortality          XGBoost         0.500    0.818    1.000    0.000    MODERATE       \n",
      "ResNet50_ImageNet    2-Year Mortality          TabNet          0.736    0.727    0.722    0.750    GOOD           \n",
      "ResNet50_ImageNet    2-Year Mortality          RandomForest    0.736    0.591    0.500    1.000    GOOD           \n",
      "ResNet50_ImageNet    2-Year Mortality          LogisticRegression 0.708    0.545    0.500    0.750    GOOD           \n",
      "ResNet50_ImageNet    2-Year Mortality          SVM             0.375    0.591    0.556    0.750    MODERATE       \n",
      "ResNet50_ImageNet    High-Grade vs Low-Grade   TabPFN          0.943    0.820    0.848    0.786    EXCELLENT      \n",
      "ResNet50_ImageNet    High-Grade vs Low-Grade   XGBoost         0.862    0.803    0.848    0.750    EXCELLENT      \n",
      "ResNet50_ImageNet    High-Grade vs Low-Grade   TabNet          0.952    0.820    1.000    0.607    EXCELLENT      \n",
      "ResNet50_ImageNet    High-Grade vs Low-Grade   RandomForest    0.897    0.803    0.848    0.750    EXCELLENT      \n",
      "ResNet50_ImageNet    High-Grade vs Low-Grade   LogisticRegression 0.899    0.754    0.909    0.571    EXCELLENT      \n",
      "ResNet50_ImageNet    High-Grade vs Low-Grade   SVM             0.917    0.787    0.848    0.714    EXCELLENT      \n",
      "ResNet50_ImageNet    IDH Mutation Status       TabPFN          0.909    0.880    1.000    0.000    EXCELLENT      \n",
      "ResNet50_ImageNet    IDH Mutation Status       XGBoost         0.750    0.760    0.750    0.833    STRONG         \n",
      "ResNet50_ImageNet    IDH Mutation Status       TabNet          0.822    0.880    1.000    0.000    STRONG         \n",
      "ResNet50_ImageNet    IDH Mutation Status       RandomForest    0.886    0.760    0.773    0.667    EXCELLENT      \n",
      "ResNet50_ImageNet    IDH Mutation Status       LogisticRegression 0.830    0.720    0.750    0.500    STRONG         \n",
      "ResNet50_ImageNet    IDH Mutation Status       SVM             0.727    0.720    0.727    0.667    GOOD           \n",
      "ResNet50_ImageNet    MGMT Promoter Methylation TabPFN          0.619    0.604    0.000    1.000    MODERATE       \n",
      "ResNet50_ImageNet    MGMT Promoter Methylation XGBoost         0.687    0.698    0.714    0.688    GOOD           \n",
      "ResNet50_ImageNet    MGMT Promoter Methylation TabNet          0.707    0.642    0.429    0.781    GOOD           \n",
      "ResNet50_ImageNet    MGMT Promoter Methylation RandomForest    0.684    0.604    0.667    0.562    GOOD           \n",
      "ResNet50_ImageNet    MGMT Promoter Methylation LogisticRegression 0.500    0.604    0.000    1.000    MODERATE       \n",
      "ResNet50_ImageNet    MGMT Promoter Methylation SVM             0.390    0.642    0.667    0.625    MODERATE       \n",
      "EfficientNet         6-Month Mortality         TabPFN          0.694    0.773    0.000    1.000    GOOD           \n",
      "EfficientNet         6-Month Mortality         XGBoost         0.706    0.636    0.400    0.706    GOOD           \n",
      "EfficientNet         6-Month Mortality         TabNet          0.706    0.364    1.000    0.176    GOOD           \n",
      "EfficientNet         6-Month Mortality         RandomForest    0.741    0.591    1.000    0.471    GOOD           \n",
      "EfficientNet         6-Month Mortality         LogisticRegression 0.500    0.773    0.000    1.000    MODERATE       \n",
      "EfficientNet         6-Month Mortality         SVM             0.647    0.682    0.400    0.765    MODERATE       \n",
      "EfficientNet         1-Year Mortality          TabPFN          0.542    0.591    0.600    0.583    MODERATE       \n",
      "EfficientNet         1-Year Mortality          XGBoost         0.604    0.545    0.600    0.500    MODERATE       \n",
      "EfficientNet         1-Year Mortality          TabNet          0.592    0.545    1.000    0.167    MODERATE       \n",
      "EfficientNet         1-Year Mortality          RandomForest    0.458    0.500    0.600    0.417    MODERATE       \n",
      "EfficientNet         1-Year Mortality          LogisticRegression 0.500    0.545    0.000    1.000    MODERATE       \n",
      "EfficientNet         1-Year Mortality          SVM             0.425    0.409    0.600    0.250    MODERATE       \n",
      "EfficientNet         2-Year Mortality          TabPFN          0.722    0.818    1.000    0.000    GOOD           \n",
      "EfficientNet         2-Year Mortality          XGBoost         0.500    0.818    1.000    0.000    MODERATE       \n",
      "EfficientNet         2-Year Mortality          TabNet          0.736    0.727    0.722    0.750    GOOD           \n",
      "EfficientNet         2-Year Mortality          RandomForest    0.736    0.591    0.500    1.000    GOOD           \n",
      "EfficientNet         2-Year Mortality          LogisticRegression 0.708    0.545    0.500    0.750    GOOD           \n",
      "EfficientNet         2-Year Mortality          SVM             0.375    0.591    0.556    0.750    MODERATE       \n",
      "EfficientNet         High-Grade vs Low-Grade   TabPFN          0.943    0.820    0.848    0.786    EXCELLENT      \n",
      "EfficientNet         High-Grade vs Low-Grade   XGBoost         0.862    0.803    0.848    0.750    EXCELLENT      \n",
      "EfficientNet         High-Grade vs Low-Grade   TabNet          0.952    0.820    1.000    0.607    EXCELLENT      \n",
      "EfficientNet         High-Grade vs Low-Grade   RandomForest    0.897    0.803    0.848    0.750    EXCELLENT      \n",
      "EfficientNet         High-Grade vs Low-Grade   LogisticRegression 0.899    0.754    0.909    0.571    EXCELLENT      \n",
      "EfficientNet         High-Grade vs Low-Grade   SVM             0.917    0.787    0.848    0.714    EXCELLENT      \n",
      "EfficientNet         IDH Mutation Status       TabPFN          0.909    0.880    1.000    0.000    EXCELLENT      \n",
      "EfficientNet         IDH Mutation Status       XGBoost         0.750    0.760    0.750    0.833    STRONG         \n",
      "EfficientNet         IDH Mutation Status       TabNet          0.822    0.880    1.000    0.000    STRONG         \n",
      "EfficientNet         IDH Mutation Status       RandomForest    0.886    0.760    0.773    0.667    EXCELLENT      \n",
      "EfficientNet         IDH Mutation Status       LogisticRegression 0.830    0.720    0.750    0.500    STRONG         \n",
      "EfficientNet         IDH Mutation Status       SVM             0.727    0.720    0.727    0.667    GOOD           \n",
      "EfficientNet         MGMT Promoter Methylation TabPFN          0.619    0.604    0.000    1.000    MODERATE       \n",
      "EfficientNet         MGMT Promoter Methylation XGBoost         0.687    0.698    0.714    0.688    GOOD           \n",
      "EfficientNet         MGMT Promoter Methylation TabNet          0.707    0.642    0.429    0.781    GOOD           \n",
      "EfficientNet         MGMT Promoter Methylation RandomForest    0.684    0.604    0.667    0.562    GOOD           \n",
      "EfficientNet         MGMT Promoter Methylation LogisticRegression 0.500    0.604    0.000    1.000    MODERATE       \n",
      "EfficientNet         MGMT Promoter Methylation SVM             0.390    0.642    0.667    0.625    MODERATE       \n",
      "\n",
      "BEST PERFORMERS BY TASK\n",
      "==================================================\n",
      "6-Month Mortality             : ConvNext + RandomForest (AUC = 0.741) NEEDS WORK\n",
      "1-Year Mortality              : ConvNext + XGBoost (AUC = 0.604) NEEDS WORK\n",
      "2-Year Mortality              : ConvNext + TabNet (AUC = 0.736) NEEDS WORK\n",
      "High-Grade vs Low-Grade       : ConvNext + TabNet (AUC = 0.952) DEPLOYMENT READY\n",
      "IDH Mutation Status           : ConvNext + TabPFN (AUC = 0.909) DEPLOYMENT READY\n",
      "MGMT Promoter Methylation     : ConvNext + TabNet (AUC = 0.707) NEEDS WORK\n",
      "\n",
      "CLINICAL RECOMMENDATIONS\n",
      "==================================================\n",
      "ALGORITHM PERFORMANCE RANKING:\n",
      "   TabNet: 0.752 mean AUC, 0.952 max AUC (30 tests)\n",
      "   TabPFN: 0.738 mean AUC, 0.943 max AUC (30 tests)\n",
      "   RandomForest: 0.734 mean AUC, 0.897 max AUC (30 tests)\n",
      "   XGBoost: 0.685 mean AUC, 0.862 max AUC (30 tests)\n",
      "   LogisticRegression: 0.656 mean AUC, 0.899 max AUC (30 tests)\n",
      "   SVM: 0.580 mean AUC, 0.917 max AUC (30 tests)\n",
      "\n",
      "CNN ARCHITECTURE RANKING:\n",
      "   ConvNext: 0.691 mean AUC, 0.952 max AUC (36 tests)\n",
      "   ViT: 0.691 mean AUC, 0.952 max AUC (36 tests)\n",
      "   ResNet50_Pretrained: 0.691 mean AUC, 0.952 max AUC (36 tests)\n",
      "   ResNet50_ImageNet: 0.691 mean AUC, 0.952 max AUC (36 tests)\n",
      "   EfficientNet: 0.691 mean AUC, 0.952 max AUC (36 tests)\n",
      "\n",
      "IMPLEMENTATION RECOMMENDATIONS:\n",
      "   50 CNN-algorithm combinations ready for clinical validation\n",
      "   Priority implementation: High-Grade vs Low-Grade using ConvNext + TabNet\n",
      "   Expected performance: 95.2% discrimination accuracy\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "CONVNEXT RESULTS:\n",
      "----------------------------------------\n",
      "\n",
      "MORTALITY PREDICTION:\n",
      "\n",
      "  6-Month Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      RandomForest    - AUC: 0.741, Bal.Acc: 0.735, F1: 0.526, MCC: 0.410\n",
      "      TabNet          - AUC: 0.706, Bal.Acc: 0.588, F1: 0.417, MCC: 0.215\n",
      "      XGBoost         - AUC: 0.706, Bal.Acc: 0.553, F1: 0.333, MCC: 0.095\n",
      "      TabPFN          - AUC: 0.694, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      SVM             - AUC: 0.647, Bal.Acc: 0.582, F1: 0.364, MCC: 0.155\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "\n",
      "  1-Year Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      XGBoost         - AUC: 0.604, Bal.Acc: 0.550, F1: 0.545, MCC: 0.100\n",
      "      TabNet          - AUC: 0.592, Bal.Acc: 0.583, F1: 0.667, MCC: 0.289\n",
      "      TabPFN          - AUC: 0.542, Bal.Acc: 0.592, F1: 0.571, MCC: 0.183\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      RandomForest    - AUC: 0.458, Bal.Acc: 0.508, F1: 0.522, MCC: 0.017\n",
      "      SVM             - AUC: 0.425, Bal.Acc: 0.425, F1: 0.480, MCC: -0.160\n",
      "\n",
      "  2-Year Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.736, Bal.Acc: 0.736, F1: 0.813, MCC: 0.379\n",
      "      RandomForest    - AUC: 0.736, Bal.Acc: 0.750, F1: 0.667, MCC: 0.392\n",
      "      TabPFN          - AUC: 0.722, Bal.Acc: 0.500, F1: 0.900, MCC: 0.000\n",
      "      LogisticRegression - AUC: 0.708, Bal.Acc: 0.625, F1: 0.643, MCC: 0.194\n",
      "      XGBoost         - AUC: 0.500, Bal.Acc: 0.500, F1: 0.900, MCC: 0.000\n",
      "      SVM             - AUC: 0.375, Bal.Acc: 0.653, F1: 0.690, MCC: 0.236\n",
      "\n",
      "TUMOR PREDICTION:\n",
      "\n",
      "  High-Grade vs Low-Grade:\n",
      "    Samples: 241, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.952, Bal.Acc: 0.804, F1: 0.857, MCC: 0.675\n",
      "      TabPFN          - AUC: 0.943, Bal.Acc: 0.817, F1: 0.836, MCC: 0.636\n",
      "      SVM             - AUC: 0.917, Bal.Acc: 0.781, F1: 0.812, MCC: 0.570\n",
      "      LogisticRegression - AUC: 0.899, Bal.Acc: 0.740, F1: 0.800, MCC: 0.517\n",
      "      RandomForest    - AUC: 0.897, Bal.Acc: 0.799, F1: 0.824, MCC: 0.603\n",
      "      XGBoost         - AUC: 0.862, Bal.Acc: 0.799, F1: 0.824, MCC: 0.603\n",
      "\n",
      "IDH PREDICTION:\n",
      "\n",
      "  IDH Mutation Status:\n",
      "    Samples: 198, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabPFN          - AUC: 0.909, Bal.Acc: 0.500, F1: 0.936, MCC: 0.000\n",
      "      RandomForest    - AUC: 0.886, Bal.Acc: 0.720, F1: 0.850, MCC: 0.318\n",
      "      LogisticRegression - AUC: 0.830, Bal.Acc: 0.625, F1: 0.825, MCC: 0.181\n",
      "      TabNet          - AUC: 0.822, Bal.Acc: 0.500, F1: 0.936, MCC: 0.000\n",
      "      XGBoost         - AUC: 0.750, Bal.Acc: 0.792, F1: 0.846, MCC: 0.406\n",
      "      SVM             - AUC: 0.727, Bal.Acc: 0.697, F1: 0.821, MCC: 0.274\n",
      "\n",
      "MGMT PREDICTION:\n",
      "\n",
      "  MGMT Promoter Methylation:\n",
      "    Samples: 212, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.707, Bal.Acc: 0.605, F1: 0.486, MCC: 0.224\n",
      "      XGBoost         - AUC: 0.687, Bal.Acc: 0.701, F1: 0.652, MCC: 0.394\n",
      "      RandomForest    - AUC: 0.684, Bal.Acc: 0.615, F1: 0.571, MCC: 0.225\n",
      "      TabPFN          - AUC: 0.619, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      SVM             - AUC: 0.390, Bal.Acc: 0.646, F1: 0.596, MCC: 0.285\n",
      "\n",
      "VIT RESULTS:\n",
      "----------------------------------------\n",
      "\n",
      "MORTALITY PREDICTION:\n",
      "\n",
      "  6-Month Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      RandomForest    - AUC: 0.741, Bal.Acc: 0.735, F1: 0.526, MCC: 0.410\n",
      "      TabNet          - AUC: 0.706, Bal.Acc: 0.588, F1: 0.417, MCC: 0.215\n",
      "      XGBoost         - AUC: 0.706, Bal.Acc: 0.553, F1: 0.333, MCC: 0.095\n",
      "      TabPFN          - AUC: 0.694, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      SVM             - AUC: 0.647, Bal.Acc: 0.582, F1: 0.364, MCC: 0.155\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "\n",
      "  1-Year Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      XGBoost         - AUC: 0.604, Bal.Acc: 0.550, F1: 0.545, MCC: 0.100\n",
      "      TabNet          - AUC: 0.592, Bal.Acc: 0.583, F1: 0.667, MCC: 0.289\n",
      "      TabPFN          - AUC: 0.542, Bal.Acc: 0.592, F1: 0.571, MCC: 0.183\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      RandomForest    - AUC: 0.458, Bal.Acc: 0.508, F1: 0.522, MCC: 0.017\n",
      "      SVM             - AUC: 0.425, Bal.Acc: 0.425, F1: 0.480, MCC: -0.160\n",
      "\n",
      "  2-Year Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.736, Bal.Acc: 0.736, F1: 0.813, MCC: 0.379\n",
      "      RandomForest    - AUC: 0.736, Bal.Acc: 0.750, F1: 0.667, MCC: 0.392\n",
      "      TabPFN          - AUC: 0.722, Bal.Acc: 0.500, F1: 0.900, MCC: 0.000\n",
      "      LogisticRegression - AUC: 0.708, Bal.Acc: 0.625, F1: 0.643, MCC: 0.194\n",
      "      XGBoost         - AUC: 0.500, Bal.Acc: 0.500, F1: 0.900, MCC: 0.000\n",
      "      SVM             - AUC: 0.375, Bal.Acc: 0.653, F1: 0.690, MCC: 0.236\n",
      "\n",
      "TUMOR PREDICTION:\n",
      "\n",
      "  High-Grade vs Low-Grade:\n",
      "    Samples: 241, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.952, Bal.Acc: 0.804, F1: 0.857, MCC: 0.675\n",
      "      TabPFN          - AUC: 0.943, Bal.Acc: 0.817, F1: 0.836, MCC: 0.636\n",
      "      SVM             - AUC: 0.917, Bal.Acc: 0.781, F1: 0.812, MCC: 0.570\n",
      "      LogisticRegression - AUC: 0.899, Bal.Acc: 0.740, F1: 0.800, MCC: 0.517\n",
      "      RandomForest    - AUC: 0.897, Bal.Acc: 0.799, F1: 0.824, MCC: 0.603\n",
      "      XGBoost         - AUC: 0.862, Bal.Acc: 0.799, F1: 0.824, MCC: 0.603\n",
      "\n",
      "IDH PREDICTION:\n",
      "\n",
      "  IDH Mutation Status:\n",
      "    Samples: 198, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabPFN          - AUC: 0.909, Bal.Acc: 0.500, F1: 0.936, MCC: 0.000\n",
      "      RandomForest    - AUC: 0.886, Bal.Acc: 0.720, F1: 0.850, MCC: 0.318\n",
      "      LogisticRegression - AUC: 0.830, Bal.Acc: 0.625, F1: 0.825, MCC: 0.181\n",
      "      TabNet          - AUC: 0.822, Bal.Acc: 0.500, F1: 0.936, MCC: 0.000\n",
      "      XGBoost         - AUC: 0.750, Bal.Acc: 0.792, F1: 0.846, MCC: 0.406\n",
      "      SVM             - AUC: 0.727, Bal.Acc: 0.697, F1: 0.821, MCC: 0.274\n",
      "\n",
      "MGMT PREDICTION:\n",
      "\n",
      "  MGMT Promoter Methylation:\n",
      "    Samples: 212, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.707, Bal.Acc: 0.605, F1: 0.486, MCC: 0.224\n",
      "      XGBoost         - AUC: 0.687, Bal.Acc: 0.701, F1: 0.652, MCC: 0.394\n",
      "      RandomForest    - AUC: 0.684, Bal.Acc: 0.615, F1: 0.571, MCC: 0.225\n",
      "      TabPFN          - AUC: 0.619, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      SVM             - AUC: 0.390, Bal.Acc: 0.646, F1: 0.596, MCC: 0.285\n",
      "\n",
      "RESNET50_PRETRAINED RESULTS:\n",
      "----------------------------------------\n",
      "\n",
      "MORTALITY PREDICTION:\n",
      "\n",
      "  6-Month Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      RandomForest    - AUC: 0.741, Bal.Acc: 0.735, F1: 0.526, MCC: 0.410\n",
      "      TabNet          - AUC: 0.706, Bal.Acc: 0.588, F1: 0.417, MCC: 0.215\n",
      "      XGBoost         - AUC: 0.706, Bal.Acc: 0.553, F1: 0.333, MCC: 0.095\n",
      "      TabPFN          - AUC: 0.694, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      SVM             - AUC: 0.647, Bal.Acc: 0.582, F1: 0.364, MCC: 0.155\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "\n",
      "  1-Year Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      XGBoost         - AUC: 0.604, Bal.Acc: 0.550, F1: 0.545, MCC: 0.100\n",
      "      TabNet          - AUC: 0.592, Bal.Acc: 0.583, F1: 0.667, MCC: 0.289\n",
      "      TabPFN          - AUC: 0.542, Bal.Acc: 0.592, F1: 0.571, MCC: 0.183\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      RandomForest    - AUC: 0.458, Bal.Acc: 0.508, F1: 0.522, MCC: 0.017\n",
      "      SVM             - AUC: 0.425, Bal.Acc: 0.425, F1: 0.480, MCC: -0.160\n",
      "\n",
      "  2-Year Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.736, Bal.Acc: 0.736, F1: 0.813, MCC: 0.379\n",
      "      RandomForest    - AUC: 0.736, Bal.Acc: 0.750, F1: 0.667, MCC: 0.392\n",
      "      TabPFN          - AUC: 0.722, Bal.Acc: 0.500, F1: 0.900, MCC: 0.000\n",
      "      LogisticRegression - AUC: 0.708, Bal.Acc: 0.625, F1: 0.643, MCC: 0.194\n",
      "      XGBoost         - AUC: 0.500, Bal.Acc: 0.500, F1: 0.900, MCC: 0.000\n",
      "      SVM             - AUC: 0.375, Bal.Acc: 0.653, F1: 0.690, MCC: 0.236\n",
      "\n",
      "TUMOR PREDICTION:\n",
      "\n",
      "  High-Grade vs Low-Grade:\n",
      "    Samples: 241, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.952, Bal.Acc: 0.804, F1: 0.857, MCC: 0.675\n",
      "      TabPFN          - AUC: 0.943, Bal.Acc: 0.817, F1: 0.836, MCC: 0.636\n",
      "      SVM             - AUC: 0.917, Bal.Acc: 0.781, F1: 0.812, MCC: 0.570\n",
      "      LogisticRegression - AUC: 0.899, Bal.Acc: 0.740, F1: 0.800, MCC: 0.517\n",
      "      RandomForest    - AUC: 0.897, Bal.Acc: 0.799, F1: 0.824, MCC: 0.603\n",
      "      XGBoost         - AUC: 0.862, Bal.Acc: 0.799, F1: 0.824, MCC: 0.603\n",
      "\n",
      "IDH PREDICTION:\n",
      "\n",
      "  IDH Mutation Status:\n",
      "    Samples: 198, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabPFN          - AUC: 0.909, Bal.Acc: 0.500, F1: 0.936, MCC: 0.000\n",
      "      RandomForest    - AUC: 0.886, Bal.Acc: 0.720, F1: 0.850, MCC: 0.318\n",
      "      LogisticRegression - AUC: 0.830, Bal.Acc: 0.625, F1: 0.825, MCC: 0.181\n",
      "      TabNet          - AUC: 0.822, Bal.Acc: 0.500, F1: 0.936, MCC: 0.000\n",
      "      XGBoost         - AUC: 0.750, Bal.Acc: 0.792, F1: 0.846, MCC: 0.406\n",
      "      SVM             - AUC: 0.727, Bal.Acc: 0.697, F1: 0.821, MCC: 0.274\n",
      "\n",
      "MGMT PREDICTION:\n",
      "\n",
      "  MGMT Promoter Methylation:\n",
      "    Samples: 212, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.707, Bal.Acc: 0.605, F1: 0.486, MCC: 0.224\n",
      "      XGBoost         - AUC: 0.687, Bal.Acc: 0.701, F1: 0.652, MCC: 0.394\n",
      "      RandomForest    - AUC: 0.684, Bal.Acc: 0.615, F1: 0.571, MCC: 0.225\n",
      "      TabPFN          - AUC: 0.619, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      SVM             - AUC: 0.390, Bal.Acc: 0.646, F1: 0.596, MCC: 0.285\n",
      "\n",
      "RESNET50_IMAGENET RESULTS:\n",
      "----------------------------------------\n",
      "\n",
      "MORTALITY PREDICTION:\n",
      "\n",
      "  6-Month Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      RandomForest    - AUC: 0.741, Bal.Acc: 0.735, F1: 0.526, MCC: 0.410\n",
      "      TabNet          - AUC: 0.706, Bal.Acc: 0.588, F1: 0.417, MCC: 0.215\n",
      "      XGBoost         - AUC: 0.706, Bal.Acc: 0.553, F1: 0.333, MCC: 0.095\n",
      "      TabPFN          - AUC: 0.694, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      SVM             - AUC: 0.647, Bal.Acc: 0.582, F1: 0.364, MCC: 0.155\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "\n",
      "  1-Year Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      XGBoost         - AUC: 0.604, Bal.Acc: 0.550, F1: 0.545, MCC: 0.100\n",
      "      TabNet          - AUC: 0.592, Bal.Acc: 0.583, F1: 0.667, MCC: 0.289\n",
      "      TabPFN          - AUC: 0.542, Bal.Acc: 0.592, F1: 0.571, MCC: 0.183\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      RandomForest    - AUC: 0.458, Bal.Acc: 0.508, F1: 0.522, MCC: 0.017\n",
      "      SVM             - AUC: 0.425, Bal.Acc: 0.425, F1: 0.480, MCC: -0.160\n",
      "\n",
      "  2-Year Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.736, Bal.Acc: 0.736, F1: 0.813, MCC: 0.379\n",
      "      RandomForest    - AUC: 0.736, Bal.Acc: 0.750, F1: 0.667, MCC: 0.392\n",
      "      TabPFN          - AUC: 0.722, Bal.Acc: 0.500, F1: 0.900, MCC: 0.000\n",
      "      LogisticRegression - AUC: 0.708, Bal.Acc: 0.625, F1: 0.643, MCC: 0.194\n",
      "      XGBoost         - AUC: 0.500, Bal.Acc: 0.500, F1: 0.900, MCC: 0.000\n",
      "      SVM             - AUC: 0.375, Bal.Acc: 0.653, F1: 0.690, MCC: 0.236\n",
      "\n",
      "TUMOR PREDICTION:\n",
      "\n",
      "  High-Grade vs Low-Grade:\n",
      "    Samples: 241, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.952, Bal.Acc: 0.804, F1: 0.857, MCC: 0.675\n",
      "      TabPFN          - AUC: 0.943, Bal.Acc: 0.817, F1: 0.836, MCC: 0.636\n",
      "      SVM             - AUC: 0.917, Bal.Acc: 0.781, F1: 0.812, MCC: 0.570\n",
      "      LogisticRegression - AUC: 0.899, Bal.Acc: 0.740, F1: 0.800, MCC: 0.517\n",
      "      RandomForest    - AUC: 0.897, Bal.Acc: 0.799, F1: 0.824, MCC: 0.603\n",
      "      XGBoost         - AUC: 0.862, Bal.Acc: 0.799, F1: 0.824, MCC: 0.603\n",
      "\n",
      "IDH PREDICTION:\n",
      "\n",
      "  IDH Mutation Status:\n",
      "    Samples: 198, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabPFN          - AUC: 0.909, Bal.Acc: 0.500, F1: 0.936, MCC: 0.000\n",
      "      RandomForest    - AUC: 0.886, Bal.Acc: 0.720, F1: 0.850, MCC: 0.318\n",
      "      LogisticRegression - AUC: 0.830, Bal.Acc: 0.625, F1: 0.825, MCC: 0.181\n",
      "      TabNet          - AUC: 0.822, Bal.Acc: 0.500, F1: 0.936, MCC: 0.000\n",
      "      XGBoost         - AUC: 0.750, Bal.Acc: 0.792, F1: 0.846, MCC: 0.406\n",
      "      SVM             - AUC: 0.727, Bal.Acc: 0.697, F1: 0.821, MCC: 0.274\n",
      "\n",
      "MGMT PREDICTION:\n",
      "\n",
      "  MGMT Promoter Methylation:\n",
      "    Samples: 212, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.707, Bal.Acc: 0.605, F1: 0.486, MCC: 0.224\n",
      "      XGBoost         - AUC: 0.687, Bal.Acc: 0.701, F1: 0.652, MCC: 0.394\n",
      "      RandomForest    - AUC: 0.684, Bal.Acc: 0.615, F1: 0.571, MCC: 0.225\n",
      "      TabPFN          - AUC: 0.619, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      SVM             - AUC: 0.390, Bal.Acc: 0.646, F1: 0.596, MCC: 0.285\n",
      "\n",
      "EFFICIENTNET RESULTS:\n",
      "----------------------------------------\n",
      "\n",
      "MORTALITY PREDICTION:\n",
      "\n",
      "  6-Month Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      RandomForest    - AUC: 0.741, Bal.Acc: 0.735, F1: 0.526, MCC: 0.410\n",
      "      TabNet          - AUC: 0.706, Bal.Acc: 0.588, F1: 0.417, MCC: 0.215\n",
      "      XGBoost         - AUC: 0.706, Bal.Acc: 0.553, F1: 0.333, MCC: 0.095\n",
      "      TabPFN          - AUC: 0.694, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      SVM             - AUC: 0.647, Bal.Acc: 0.582, F1: 0.364, MCC: 0.155\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "\n",
      "  1-Year Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      XGBoost         - AUC: 0.604, Bal.Acc: 0.550, F1: 0.545, MCC: 0.100\n",
      "      TabNet          - AUC: 0.592, Bal.Acc: 0.583, F1: 0.667, MCC: 0.289\n",
      "      TabPFN          - AUC: 0.542, Bal.Acc: 0.592, F1: 0.571, MCC: 0.183\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      RandomForest    - AUC: 0.458, Bal.Acc: 0.508, F1: 0.522, MCC: 0.017\n",
      "      SVM             - AUC: 0.425, Bal.Acc: 0.425, F1: 0.480, MCC: -0.160\n",
      "\n",
      "  2-Year Mortality:\n",
      "    Samples: 86, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.736, Bal.Acc: 0.736, F1: 0.813, MCC: 0.379\n",
      "      RandomForest    - AUC: 0.736, Bal.Acc: 0.750, F1: 0.667, MCC: 0.392\n",
      "      TabPFN          - AUC: 0.722, Bal.Acc: 0.500, F1: 0.900, MCC: 0.000\n",
      "      LogisticRegression - AUC: 0.708, Bal.Acc: 0.625, F1: 0.643, MCC: 0.194\n",
      "      XGBoost         - AUC: 0.500, Bal.Acc: 0.500, F1: 0.900, MCC: 0.000\n",
      "      SVM             - AUC: 0.375, Bal.Acc: 0.653, F1: 0.690, MCC: 0.236\n",
      "\n",
      "TUMOR PREDICTION:\n",
      "\n",
      "  High-Grade vs Low-Grade:\n",
      "    Samples: 241, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.952, Bal.Acc: 0.804, F1: 0.857, MCC: 0.675\n",
      "      TabPFN          - AUC: 0.943, Bal.Acc: 0.817, F1: 0.836, MCC: 0.636\n",
      "      SVM             - AUC: 0.917, Bal.Acc: 0.781, F1: 0.812, MCC: 0.570\n",
      "      LogisticRegression - AUC: 0.899, Bal.Acc: 0.740, F1: 0.800, MCC: 0.517\n",
      "      RandomForest    - AUC: 0.897, Bal.Acc: 0.799, F1: 0.824, MCC: 0.603\n",
      "      XGBoost         - AUC: 0.862, Bal.Acc: 0.799, F1: 0.824, MCC: 0.603\n",
      "\n",
      "IDH PREDICTION:\n",
      "\n",
      "  IDH Mutation Status:\n",
      "    Samples: 198, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabPFN          - AUC: 0.909, Bal.Acc: 0.500, F1: 0.936, MCC: 0.000\n",
      "      RandomForest    - AUC: 0.886, Bal.Acc: 0.720, F1: 0.850, MCC: 0.318\n",
      "      LogisticRegression - AUC: 0.830, Bal.Acc: 0.625, F1: 0.825, MCC: 0.181\n",
      "      TabNet          - AUC: 0.822, Bal.Acc: 0.500, F1: 0.936, MCC: 0.000\n",
      "      XGBoost         - AUC: 0.750, Bal.Acc: 0.792, F1: 0.846, MCC: 0.406\n",
      "      SVM             - AUC: 0.727, Bal.Acc: 0.697, F1: 0.821, MCC: 0.274\n",
      "\n",
      "MGMT PREDICTION:\n",
      "\n",
      "  MGMT Promoter Methylation:\n",
      "    Samples: 212, Features: 13\n",
      "    Algorithm Performance (sorted by AUC-ROC):\n",
      "      TabNet          - AUC: 0.707, Bal.Acc: 0.605, F1: 0.486, MCC: 0.224\n",
      "      XGBoost         - AUC: 0.687, Bal.Acc: 0.701, F1: 0.652, MCC: 0.394\n",
      "      RandomForest    - AUC: 0.684, Bal.Acc: 0.615, F1: 0.571, MCC: 0.225\n",
      "      TabPFN          - AUC: 0.619, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      LogisticRegression - AUC: 0.500, Bal.Acc: 0.500, F1: 0.000, MCC: 0.000\n",
      "      SVM             - AUC: 0.390, Bal.Acc: 0.646, F1: 0.596, MCC: 0.285\n",
      "Results exported to neurosurgical_ai_results_256_separate.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           accuracy_score, roc_curve, precision_recall_curve, auc,\n",
    "                           balanced_accuracy_score, matthews_corrcoef, \n",
    "                           average_precision_score, cohen_kappa_score)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for optional dependencies\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "    import torch\n",
    "    TABNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TABNET_AVAILABLE = False\n",
    "    print(\"TabNet not available. Install with: pip install pytorch-tabnet torch\")\n",
    "\n",
    "class NeurosurgicalAIAnalyzer:\n",
    "    \"\"\"AI analysis system for neurosurgical outcome prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, data_paths=None):\n",
    "        # Default paths - update these to match your file locations\n",
    "        if data_paths is None:\n",
    "            self.datasets = {\n",
    "            'ConvNext': '/Users/joi263/Documents/MultimodalTabData/data/convnext_data/convnext_cleaned_patient_features_separate_256d.csv',\n",
    "            'ViT': '/Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_cleaned_patient_features_separate_256d.csv',\n",
    "            'ResNet50_Pretrained': '/Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_cleaned_patient_features_separate_256d.csv',\n",
    "            'ResNet50_ImageNet': '/Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_cleaned_patient_features_separate_256d.csv',\n",
    "            'EfficientNet': '/Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_cleaned_patient_features_separate_256d.csv'\n",
    "        }\n",
    "        else:\n",
    "            self.datasets = data_paths\n",
    "            \n",
    "        self.results = {}\n",
    "        self.validation_results = {}\n",
    "        \n",
    "        # Verify file paths\n",
    "        self._verify_data_files()\n",
    "        \n",
    "    def _verify_data_files(self):\n",
    "        \"\"\"Check which data files are available\"\"\"\n",
    "        import os\n",
    "        print(\"CHECKING DATA FILE PATHS:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        existing_files = 0\n",
    "        for cnn_name, file_path in self.datasets.items():\n",
    "            exists = os.path.exists(file_path)\n",
    "            status = \"EXISTS\" if exists else \"NOT FOUND\"\n",
    "            print(f\"{cnn_name:<20}: {status}\")\n",
    "            if exists:\n",
    "                existing_files += 1\n",
    "            else:\n",
    "                print(f\"  Expected: {file_path}\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(f\"Found {existing_files}/{len(self.datasets)} data files\")\n",
    "        \n",
    "        if existing_files == 0:\n",
    "            print(\"ERROR: No data files found!\")\n",
    "            print(\"Please update the file paths to match your actual file locations.\")\n",
    "        elif existing_files < len(self.datasets):\n",
    "            print(f\"WARNING: Only {existing_files} out of {len(self.datasets)} files found.\")\n",
    "        else:\n",
    "            print(\"SUCCESS: All data files found!\")\n",
    "        print()\n",
    "        \n",
    "    def get_ml_algorithms(self):\n",
    "        \"\"\"Initialize ML algorithms with class imbalance handling\"\"\"\n",
    "        algorithms = {}\n",
    "        \n",
    "        # TabPFN - Transformer-based Few-Shot Learning\n",
    "        algorithms['TabPFN'] = {\n",
    "            'model': TabPFNClassifier(device='cpu'),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Transformer-based Few-Shot Learning'\n",
    "        }\n",
    "        \n",
    "        # XGBoost with enhanced imbalance handling\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            algorithms['XGBoost'] = {\n",
    "                'model': xgb.XGBClassifier(\n",
    "                    n_estimators=300,\n",
    "                    max_depth=4,\n",
    "                    learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    min_child_weight=3,\n",
    "                    reg_alpha=1,\n",
    "                    reg_lambda=1,\n",
    "                    random_state=42,\n",
    "                    eval_metric='logloss',\n",
    "                    use_label_encoder=False,\n",
    "                    scale_pos_weight=None,  # Will be calculated dynamically\n",
    "                    objective='binary:logistic'\n",
    "                ),\n",
    "                'needs_scaling': False,\n",
    "                'description': 'Gradient Boosting with Auto-Balancing'\n",
    "            }\n",
    "        \n",
    "        # TabNet with enhanced imbalance handling\n",
    "        if TABNET_AVAILABLE:\n",
    "            algorithms['TabNet'] = {\n",
    "                'model': TabNetClassifier(\n",
    "                    n_d=64, n_a=64,\n",
    "                    n_steps=5,\n",
    "                    gamma=1.5,\n",
    "                    lambda_sparse=1e-4,\n",
    "                    optimizer_fn=torch.optim.Adam,\n",
    "                    optimizer_params=dict(lr=0.01, weight_decay=1e-5),\n",
    "                    mask_type=\"entmax\",\n",
    "                    scheduler_params={\"step_size\": 20, \"gamma\": 0.8},\n",
    "                    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                    verbose=0,\n",
    "                    seed=42\n",
    "                    # Note: Class weights will be handled during training\n",
    "                ),\n",
    "                'needs_scaling': True,\n",
    "                'description': 'Attention-based Neural Network with Class Balancing'\n",
    "            }\n",
    "        \n",
    "        # Random Forest with strong imbalance handling\n",
    "        algorithms['RandomForest'] = {\n",
    "            'model': RandomForestClassifier(\n",
    "                n_estimators=500,\n",
    "                max_depth=8,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                max_features='sqrt',\n",
    "                bootstrap=True,\n",
    "                oob_score=True,\n",
    "                random_state=42,\n",
    "                class_weight='balanced_subsample',\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'description': 'Ensemble Trees with Balanced Subsampling'\n",
    "        }\n",
    "        \n",
    "        # Logistic Regression with enhanced balancing\n",
    "        algorithms['LogisticRegression'] = {\n",
    "            'model': LogisticRegression(\n",
    "                penalty='elasticnet',\n",
    "                l1_ratio=0.5,\n",
    "                C=0.1,\n",
    "                solver='saga',\n",
    "                max_iter=2000,\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'needs_scaling': True,\n",
    "            'description': 'Regularized Linear Model with Auto-Balancing'\n",
    "        }\n",
    "        \n",
    "        # SVM with class balancing\n",
    "        algorithms['SVM'] = {\n",
    "            'model': SVC(\n",
    "                kernel='rbf',\n",
    "                C=1.0,\n",
    "                gamma='scale',\n",
    "                probability=True,\n",
    "                random_state=42,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'needs_scaling': True,\n",
    "            'description': 'Support Vector Machine with Auto-Balancing'\n",
    "        }\n",
    "        \n",
    "        return algorithms\n",
    "\n",
    "    def calculate_class_imbalance_metrics(self, y):\n",
    "        \"\"\"Calculate class imbalance metrics\"\"\"\n",
    "        from collections import Counter\n",
    "        \n",
    "        class_counts = Counter(y)\n",
    "        total_samples = len(y)\n",
    "        \n",
    "        if len(class_counts) != 2:\n",
    "            return None  # Skip non-binary tasks\n",
    "        \n",
    "        # Get minority and majority class counts\n",
    "        sorted_counts = sorted(class_counts.values())\n",
    "        minority_count = sorted_counts[0]\n",
    "        majority_count = sorted_counts[1]\n",
    "        \n",
    "        # Calculate imbalance metrics\n",
    "        imbalance_ratio = majority_count / minority_count\n",
    "        minority_percentage = minority_count / total_samples * 100\n",
    "        \n",
    "        # Imbalance severity classification\n",
    "        if imbalance_ratio <= 2:\n",
    "            severity = \"BALANCED\"\n",
    "        elif imbalance_ratio <= 5:\n",
    "            severity = \"MILD IMBALANCE\"\n",
    "        elif imbalance_ratio <= 10:\n",
    "            severity = \"MODERATE IMBALANCE\"\n",
    "        elif imbalance_ratio <= 20:\n",
    "            severity = \"HIGH IMBALANCE\"\n",
    "        else:\n",
    "            severity = \"SEVERE IMBALANCE\"\n",
    "        \n",
    "        return {\n",
    "            'minority_count': minority_count,\n",
    "            'majority_count': majority_count,\n",
    "            'total_samples': total_samples,\n",
    "            'imbalance_ratio': imbalance_ratio,\n",
    "            'minority_percentage': minority_percentage,\n",
    "            'severity': severity,\n",
    "            'class_distribution': dict(class_counts)\n",
    "        }\n",
    "\n",
    "    def calculate_balanced_metrics(self, y_true, y_pred, y_pred_proba):\n",
    "        \"\"\"Calculate comprehensive metrics for imbalanced data\"\"\"\n",
    "        \n",
    "        # Basic metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # AUC metrics\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        except:\n",
    "            roc_auc = 0.5\n",
    "            \n",
    "        try:\n",
    "            pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "        except:\n",
    "            pr_auc = 0.5\n",
    "        \n",
    "        # Confusion matrix metrics\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            \n",
    "            # Basic metrics\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "            \n",
    "            # Advanced metrics for imbalanced data\n",
    "            youden_j = sensitivity + specificity - 1\n",
    "            f1_score = 2 * (ppv * sensitivity) / (ppv + sensitivity) if (ppv + sensitivity) > 0 else 0\n",
    "            mcc = matthews_corrcoef(y_true, y_pred)\n",
    "            kappa = cohen_kappa_score(y_true, y_pred)\n",
    "            geometric_mean = np.sqrt(sensitivity * specificity)\n",
    "            \n",
    "        else:\n",
    "            sensitivity = specificity = ppv = npv = 0\n",
    "            youden_j = f1_score = mcc = kappa = geometric_mean = 0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'auc_roc': roc_auc,\n",
    "            'auc_pr': pr_auc,\n",
    "            'auc': roc_auc,  # For backward compatibility\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'ppv': ppv,\n",
    "            'npv': npv,\n",
    "            'youden_j': youden_j,\n",
    "            'f1_score': f1_score,\n",
    "            'mcc': mcc,\n",
    "            'kappa': kappa,\n",
    "            'geometric_mean': geometric_mean,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "    def create_prediction_targets(self, df):\n",
    "        \"\"\"Create prediction targets: mortality, tumor classification, IDH, MGMT\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"CREATING PREDICTION TARGETS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        targets_data = {}\n",
    "        \n",
    "        # MORTALITY TARGETS\n",
    "        print(\"MORTALITY TARGETS:\")\n",
    "        survival_data = df[df['survival'].notna() & df['patient_status'].notna()].copy()\n",
    "        \n",
    "        if len(survival_data) > 0:\n",
    "            survival_data['mortality_6mo'] = ((survival_data['patient_status'] == 2) & \n",
    "                                              (survival_data['survival'] <= 6)).astype(int)\n",
    "            survival_data['mortality_1yr'] = ((survival_data['patient_status'] == 2) & \n",
    "                                              (survival_data['survival'] <= 12)).astype(int)\n",
    "            survival_data['mortality_2yr'] = ((survival_data['patient_status'] == 2) & \n",
    "                                              (survival_data['survival'] <= 24)).astype(int)\n",
    "            \n",
    "            targets_data['mortality'] = {\n",
    "                'data': survival_data,\n",
    "                'targets': ['mortality_6mo', 'mortality_1yr', 'mortality_2yr'],\n",
    "                'descriptions': ['6-Month Mortality', '1-Year Mortality', '2-Year Mortality']\n",
    "            }\n",
    "            \n",
    "            print(f\"   Patients: {len(survival_data)}\")\n",
    "            for target, desc in zip(['mortality_6mo', 'mortality_1yr', 'mortality_2yr'], \n",
    "                                   ['6-month', '1-year', '2-year']):\n",
    "                count = survival_data[target].sum()\n",
    "                pct = survival_data[target].mean() * 100\n",
    "                print(f\"   {desc}: {count}/{len(survival_data)} ({pct:.1f}%)\")\n",
    "        \n",
    "        # TUMOR CLASSIFICATION TARGETS\n",
    "        print(\"\\nTUMOR CLASSIFICATION TARGETS:\")\n",
    "        tumor_data = df[df['methylation_class'].notna()].copy()\n",
    "        \n",
    "        if len(tumor_data) > 0:\n",
    "            high_grade_terms = ['glioblastoma', 'anaplastic', 'high grade', 'grade iv', 'grade 4', 'gbm']\n",
    "            tumor_data['high_grade'] = tumor_data['methylation_class'].str.lower().str.contains(\n",
    "                '|'.join(high_grade_terms), na=False\n",
    "            ).astype(int)\n",
    "            \n",
    "            targets_data['tumor'] = {\n",
    "                'data': tumor_data,\n",
    "                'targets': ['high_grade'],\n",
    "                'descriptions': ['High-Grade vs Low-Grade']\n",
    "            }\n",
    "            \n",
    "            print(f\"   Patients: {len(tumor_data)}\")\n",
    "            count = tumor_data['high_grade'].sum()\n",
    "            pct = tumor_data['high_grade'].mean() * 100\n",
    "            print(f\"   High-grade: {count}/{len(tumor_data)} ({pct:.1f}%)\")\n",
    "        \n",
    "        # IDH MUTATION TARGETS\n",
    "        print(\"\\nIDH MUTATION TARGETS:\")\n",
    "        idh_data = self._create_idh_targets(df)\n",
    "        \n",
    "        if idh_data is not None and len(idh_data) > 0:\n",
    "            targets_data['idh'] = {\n",
    "                'data': idh_data,\n",
    "                'targets': ['idh_binary'],\n",
    "                'descriptions': ['IDH Mutation Status']\n",
    "            }\n",
    "            \n",
    "            print(f\"   Patients: {len(idh_data)}\")\n",
    "            count = idh_data['idh_binary'].sum()\n",
    "            pct = idh_data['idh_binary'].mean() * 100\n",
    "            print(f\"   IDH Mutant: {count}/{len(idh_data)} ({pct:.1f}%)\")\n",
    "        \n",
    "        # MGMT METHYLATION TARGETS\n",
    "        print(\"\\nMGMT METHYLATION TARGETS:\")\n",
    "        mgmt_data = self._create_mgmt_targets(df)\n",
    "        \n",
    "        if mgmt_data is not None and len(mgmt_data) > 0:\n",
    "            targets_data['mgmt'] = {\n",
    "                'data': mgmt_data,\n",
    "                'targets': ['mgmt_binary'],\n",
    "                'descriptions': ['MGMT Promoter Methylation']\n",
    "            }\n",
    "            \n",
    "            print(f\"   Patients: {len(mgmt_data)}\")\n",
    "            count = mgmt_data['mgmt_binary'].sum()\n",
    "            pct = mgmt_data['mgmt_binary'].mean() * 100\n",
    "            print(f\"   MGMT Methylated: {count}/{len(mgmt_data)} ({pct:.1f}%)\")\n",
    "        \n",
    "        return targets_data\n",
    "\n",
    "    def _create_idh_targets(self, df):\n",
    "        \"\"\"Create IDH mutation targets with proper decoding\"\"\"\n",
    "        if 'idh_1_r132h' not in df.columns:\n",
    "            return None\n",
    "            \n",
    "        idh_data = df.copy()\n",
    "        idh_data['idh_binary'] = np.nan\n",
    "        \n",
    "        # Cross-reference with text data if available\n",
    "        if 'idh1' in df.columns:\n",
    "            text_idh = df['idh1'].astype(str).str.lower()\n",
    "            mutant_patterns = ['r132h', 'r132s', 'arg132his', 'arg132ser', 'missense', 'p.arg132']\n",
    "            is_mutant_text = text_idh.str.contains('|'.join(mutant_patterns), na=False)\n",
    "            idh_data.loc[is_mutant_text, 'idh_binary'] = 1  # Mutant\n",
    "        \n",
    "        # Apply numerical encoding (2 = mutant based on cross-reference analysis)\n",
    "        remaining_mask = idh_data['idh_binary'].isna() & idh_data['idh_1_r132h'].notna()\n",
    "        idh_data.loc[remaining_mask & (idh_data['idh_1_r132h'] == 2), 'idh_binary'] = 1  # Mutant\n",
    "        idh_data.loc[remaining_mask & (idh_data['idh_1_r132h'] == 1), 'idh_binary'] = 0  # Wildtype\n",
    "        \n",
    "        # Exclude unknown cases\n",
    "        idh_data.loc[idh_data['idh_1_r132h'] == 3, 'idh_binary'] = np.nan\n",
    "        \n",
    "        return idh_data[idh_data['idh_binary'].notna()].copy()\n",
    "\n",
    "    def _create_mgmt_targets(self, df):\n",
    "        \"\"\"Create MGMT methylation targets with correct encoding\"\"\"\n",
    "        if 'mgmt' not in df.columns:\n",
    "            return None\n",
    "            \n",
    "        mgmt_data = df[df['mgmt'].notna()].copy()\n",
    "        \n",
    "        if len(mgmt_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Correct encoding: 1 = Positive (methylated), 2 = Negative (unmethylated), 3 = Non-informative\n",
    "        mgmt_data['mgmt_binary'] = np.nan\n",
    "        mgmt_data.loc[mgmt_data['mgmt'] == 1, 'mgmt_binary'] = 1  # Methylated\n",
    "        mgmt_data.loc[mgmt_data['mgmt'] == 2, 'mgmt_binary'] = 0  # Unmethylated\n",
    "        mgmt_data.loc[mgmt_data['mgmt'] == 3, 'mgmt_binary'] = np.nan  # Exclude non-informative\n",
    "        \n",
    "        return mgmt_data[mgmt_data['mgmt_binary'].notna()].copy()\n",
    "\n",
    "    def select_features(self, df):\n",
    "        \"\"\"Select comprehensive feature set\"\"\"\n",
    "        # Clinical features\n",
    "        clinical_features = ['age', 'sex', 'race', 'ethnicity', 'gtr']\n",
    "        \n",
    "        # Molecular features (exclude target variables to prevent leakage)\n",
    "        molecular_features = ['mgmt_pyro', 'atrx', 'p53', 'braf_v600', 'h3k27m', 'gfap', 'tumor', 'hg_glioma']\n",
    "        \n",
    "        # CNN-extracted imaging features\n",
    "        image_features = [col for col in df.columns if col.startswith('feature_')]\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = clinical_features + molecular_features + image_features\n",
    "        available_features = [f for f in all_features if f in df.columns]\n",
    "        \n",
    "        return available_features\n",
    "\n",
    "    def preprocess_data(self, df, features, target_col):\n",
    "        \"\"\"Advanced preprocessing for multiple ML algorithms\"\"\"\n",
    "        data = df[features + [target_col]].copy()\n",
    "        data = data[data[target_col].notna()]\n",
    "        \n",
    "        if len(data) < 15:  # Minimum viable sample size\n",
    "            return None, None, f\"Insufficient data: {len(data)} samples\"\n",
    "        \n",
    "        # Handle categorical features\n",
    "        categorical_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "        if target_col in categorical_features:\n",
    "            categorical_features.remove(target_col)\n",
    "        \n",
    "        for col in categorical_features:\n",
    "            if col in features:\n",
    "                le = LabelEncoder()\n",
    "                data[col] = data[col].astype(str)\n",
    "                data[col] = le.fit_transform(data[col])\n",
    "        \n",
    "        # Handle missing values\n",
    "        numerical_features = [f for f in features if f in data.select_dtypes(include=[np.number]).columns]\n",
    "        \n",
    "        for col in numerical_features:\n",
    "            if data[col].isnull().sum() > 0:\n",
    "                if col.startswith('feature_'):\n",
    "                    data[col] = data[col].fillna(data[col].mean())\n",
    "                else:\n",
    "                    data[col] = data[col].fillna(data[col].median())\n",
    "        \n",
    "        # Remove features with >50% missing\n",
    "        missing_pct = data[features].isnull().mean()\n",
    "        good_features = missing_pct[missing_pct <= 0.5].index.tolist()\n",
    "        \n",
    "        if len(good_features) < len(features):\n",
    "            features = good_features\n",
    "            data = data[features + [target_col]]\n",
    "        \n",
    "        # Feature selection for computational efficiency\n",
    "        X = data[features].values\n",
    "        y = data[target_col].values\n",
    "        \n",
    "        # Check class balance\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        min_class_size = min(class_counts)\n",
    "        \n",
    "        if min_class_size < 3:\n",
    "            return None, None, f\"Class too small: minimum class has {min_class_size} samples\"\n",
    "        \n",
    "        # Feature selection (limit to 100 for computational efficiency)\n",
    "        if X.shape[1] > 100:\n",
    "            selector = SelectKBest(score_func=f_classif, k=100)\n",
    "            X = selector.fit_transform(X, y)\n",
    "        \n",
    "        return X, y, None\n",
    "\n",
    "    def train_and_evaluate_algorithm(self, X_train, X_test, y_train, y_test, algorithm_name, algorithm_config):\n",
    "        \"\"\"Train and evaluate a single algorithm with enhanced imbalance handling\"\"\"\n",
    "        try:\n",
    "            model = algorithm_config['model']\n",
    "            needs_scaling = algorithm_config['needs_scaling']\n",
    "            \n",
    "            # Calculate class imbalance and adjust model if needed\n",
    "            imbalance_info = self.calculate_class_imbalance_metrics(y_train)\n",
    "            \n",
    "            # Dynamic class balancing for specific algorithms\n",
    "            if algorithm_name == 'XGBoost' and XGBOOST_AVAILABLE and imbalance_info:\n",
    "                neg_count = imbalance_info['class_distribution'][0]\n",
    "                pos_count = imbalance_info['class_distribution'][1]\n",
    "                scale_pos_weight = neg_count / pos_count\n",
    "                model.set_params(scale_pos_weight=scale_pos_weight)\n",
    "            \n",
    "            elif algorithm_name == 'TabNet' and TABNET_AVAILABLE and imbalance_info:\n",
    "                from sklearn.utils.class_weight import compute_class_weight\n",
    "                classes = np.unique(y_train)\n",
    "                class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "            \n",
    "            # Apply robust scaling if needed\n",
    "            if needs_scaling:\n",
    "                scaler = RobustScaler(quantile_range=(10.0, 90.0))\n",
    "                X_train_processed = scaler.fit_transform(X_train)\n",
    "                X_test_processed = scaler.transform(X_test)\n",
    "                \n",
    "                # Handle potential scaling issues\n",
    "                if np.any(np.isnan(X_train_processed)) or np.any(np.isnan(X_test_processed)):\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_processed = scaler.fit_transform(X_train)\n",
    "                    X_test_processed = scaler.transform(X_test)\n",
    "            else:\n",
    "                X_train_processed = X_train\n",
    "                X_test_processed = X_test\n",
    "            \n",
    "            # Train model based on algorithm type\n",
    "            if algorithm_name == 'TabNet' and TABNET_AVAILABLE:\n",
    "                try:\n",
    "                    model.fit(\n",
    "                        X_train_processed, y_train,\n",
    "                        eval_set=[(X_test_processed, y_test)],\n",
    "                        patience=20,\n",
    "                        max_epochs=100,\n",
    "                        eval_metric=['auc'],\n",
    "                        batch_size=min(256, len(X_train)//4)\n",
    "                    )\n",
    "                    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "                    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                except Exception:\n",
    "                    model.fit(\n",
    "                        X_train_processed, y_train,\n",
    "                        eval_set=[(X_test_processed, y_test)],\n",
    "                        patience=20,\n",
    "                        max_epochs=100,\n",
    "                        eval_metric=['auc']\n",
    "                    )\n",
    "                    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "                    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "            elif algorithm_name == 'XGBoost' and XGBOOST_AVAILABLE:\n",
    "                try:\n",
    "                    eval_set = [(X_test_processed, y_test)]\n",
    "                    model.fit(\n",
    "                        X_train_processed, y_train,\n",
    "                        eval_set=eval_set,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                except TypeError:\n",
    "                    model.fit(X_train_processed, y_train)\n",
    "                \n",
    "                y_pred = model.predict(X_test_processed)\n",
    "                y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "                \n",
    "            else:\n",
    "                # Standard scikit-learn interface\n",
    "                model.fit(X_train_processed, y_train)\n",
    "                y_pred = model.predict(X_test_processed)\n",
    "                \n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "                else:\n",
    "                    y_pred_proba = y_pred.astype(float)\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            metrics = self.calculate_balanced_metrics(y_test, y_pred, y_pred_proba)\n",
    "            \n",
    "            # Add additional information\n",
    "            test_imbalance = self.calculate_class_imbalance_metrics(y_test)\n",
    "            \n",
    "            result = {\n",
    "                **metrics,\n",
    "                'n_test': len(y_test),\n",
    "                'scaling_used': needs_scaling,\n",
    "                'train_imbalance': imbalance_info,\n",
    "                'test_imbalance': test_imbalance\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR {algorithm_name} failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def run_prediction_task(self, X, y, task_name, cnn_name, algorithms):\n",
    "        \"\"\"Run prediction task with cross-validation and imbalance reporting\"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{task_name} - {cnn_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Analyze class imbalance\n",
    "        imbalance_info = self.calculate_class_imbalance_metrics(y)\n",
    "        if imbalance_info:\n",
    "            print(f\"CLASS IMBALANCE ANALYSIS:\")\n",
    "            print(f\"   Total samples: {imbalance_info['total_samples']}\")\n",
    "            print(f\"   Class distribution: {imbalance_info['class_distribution']}\")\n",
    "            print(f\"   Minority class: {imbalance_info['minority_count']} ({imbalance_info['minority_percentage']:.1f}%)\")\n",
    "            print(f\"   Imbalance ratio: {imbalance_info['imbalance_ratio']:.2f}:1\")\n",
    "            print(f\"   Severity: {imbalance_info['severity']}\")\n",
    "        \n",
    "        # Train-test split\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.25, random_state=42, stratify=y\n",
    "            )\n",
    "        except:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.25, random_state=42\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nDATA SPLIT:\")\n",
    "        print(f\"   Training: {len(X_train)} samples\")\n",
    "        print(f\"   Testing: {len(X_test)} samples\")\n",
    "        print(f\"   Positive rate: {y_train.mean()*100:.1f}% (train), {y_test.mean()*100:.1f}% (test)\")\n",
    "        \n",
    "        # Train and evaluate each algorithm\n",
    "        task_results = {}\n",
    "        print(f\"\\nALGORITHM EVALUATION:\")\n",
    "        \n",
    "        for algo_name, algo_config in algorithms.items():\n",
    "            print(f\"\\n   {algo_name}:\")\n",
    "            result = self.train_and_evaluate_algorithm(\n",
    "                X_train, X_test, y_train, y_test, algo_name, algo_config\n",
    "            )\n",
    "            if result:\n",
    "                task_results[algo_name] = result\n",
    "                print(f\"      AUC-ROC: {result['auc_roc']:.3f}\")\n",
    "                print(f\"      Balanced Accuracy: {result['balanced_accuracy']:.3f}\")\n",
    "                print(f\"      F1 Score: {result['f1_score']:.3f}\")\n",
    "                print(f\"      MCC: {result['mcc']:.3f}\")\n",
    "        \n",
    "        return task_results\n",
    "\n",
    "    def run_analysis(self):\n",
    "        \"\"\"Run complete analysis pipeline\"\"\"\n",
    "        print(\"NEUROSURGICAL AI OUTCOME PREDICTION ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Get available algorithms\n",
    "        algorithms = self.get_ml_algorithms()\n",
    "        print(f\"Available algorithms: {list(algorithms.keys())}\")\n",
    "        \n",
    "        # Process each CNN dataset\n",
    "        for cnn_name, file_path in self.datasets.items():\n",
    "            try:\n",
    "                # Load data\n",
    "                print(f\"\\nLoading {cnn_name} dataset...\")\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"Loaded {len(df)} patients with {len(df.columns)} features\")\n",
    "                \n",
    "                # Create targets\n",
    "                targets_data = self.create_prediction_targets(df)\n",
    "                \n",
    "                # Select features\n",
    "                features = self.select_features(df)\n",
    "                print(f\"Selected {len(features)} features for analysis\")\n",
    "                \n",
    "                # Run analysis for each target category\n",
    "                cnn_results = {}\n",
    "                \n",
    "                for target_category, target_info in targets_data.items():\n",
    "                    target_data = target_info['data']\n",
    "                    target_names = target_info['targets']\n",
    "                    target_descriptions = target_info['descriptions']\n",
    "                    \n",
    "                    category_results = {}\n",
    "                    \n",
    "                    for target_name, description in zip(target_names, target_descriptions):\n",
    "                        # Preprocess data for this target\n",
    "                        X, y, error = self.preprocess_data(target_data, features, target_name)\n",
    "                        \n",
    "                        if X is None:\n",
    "                            print(f\"Skipping {description}: {error}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Run prediction task\n",
    "                        task_results = self.run_prediction_task(\n",
    "                            X, y, description, cnn_name, algorithms\n",
    "                        )\n",
    "                        \n",
    "                        if task_results:\n",
    "                            category_results[target_name] = {\n",
    "                                'task_name': description,\n",
    "                                'description': description,\n",
    "                                'results': task_results,\n",
    "                                'n_samples': len(y),\n",
    "                                'n_features': X.shape[1]\n",
    "                            }\n",
    "                    \n",
    "                    if category_results:\n",
    "                        cnn_results[target_category] = category_results\n",
    "                \n",
    "                # Store results for this CNN\n",
    "                if cnn_results:\n",
    "                    self.results[cnn_name] = cnn_results\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {cnn_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "    def print_summary_results(self):\n",
    "        \"\"\"Print a summary of all results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to display. Run analysis first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            print(f\"\\n{cnn_name.upper()} RESULTS:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for target_category, category_results in cnn_results.items():\n",
    "                print(f\"\\n{target_category.upper()} PREDICTION:\")\n",
    "                \n",
    "                for target_name, target_info in category_results.items():\n",
    "                    description = target_info['description']\n",
    "                    results = target_info['results']\n",
    "                    n_samples = target_info['n_samples']\n",
    "                    n_features = target_info['n_features']\n",
    "                    \n",
    "                    print(f\"\\n  {description}:\")\n",
    "                    print(f\"    Samples: {n_samples}, Features: {n_features}\")\n",
    "                    \n",
    "                    # Sort algorithms by AUC-ROC\n",
    "                    sorted_results = sorted(results.items(), \n",
    "                                          key=lambda x: x[1]['auc_roc'], \n",
    "                                          reverse=True)\n",
    "                    \n",
    "                    print(\"    Algorithm Performance (sorted by AUC-ROC):\")\n",
    "                    for algo_name, metrics in sorted_results:\n",
    "                        print(f\"      {algo_name:15} - AUC: {metrics['auc_roc']:.3f}, \"\n",
    "                              f\"Bal.Acc: {metrics['balanced_accuracy']:.3f}, \"\n",
    "                              f\"F1: {metrics['f1_score']:.3f}, \"\n",
    "                              f\"MCC: {metrics['mcc']:.3f}\")\n",
    "\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate comprehensive analysis report\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"\\nNo results to report\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"COMPREHENSIVE ANALYSIS REPORT\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # EXECUTIVE SUMMARY\n",
    "        self._generate_executive_summary()\n",
    "        \n",
    "        # DETAILED RESULTS TABLE\n",
    "        self._generate_detailed_results_table()\n",
    "        \n",
    "        # BEST PERFORMERS ANALYSIS\n",
    "        self._generate_best_performers_analysis()\n",
    "        \n",
    "        # CLINICAL RECOMMENDATIONS\n",
    "        self._generate_clinical_recommendations()\n",
    "\n",
    "    def _generate_executive_summary(self):\n",
    "        \"\"\"Generate executive summary\"\"\"\n",
    "        print(\"\\nEXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        total_tests = 0\n",
    "        excellent_tests = 0\n",
    "        good_tests = 0\n",
    "        \n",
    "        all_aucs = []\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for target_category, category_results in cnn_results.items():\n",
    "                for target_name, target_info in category_results.items():\n",
    "                    for alg_name, result in target_info['results'].items():\n",
    "                        total_tests += 1\n",
    "                        auc = result['auc_roc']\n",
    "                        all_aucs.append(auc)\n",
    "                        \n",
    "                        if auc >= 0.85:\n",
    "                            excellent_tests += 1\n",
    "                        elif auc >= 0.75:\n",
    "                            good_tests += 1\n",
    "        \n",
    "        if all_aucs:\n",
    "            mean_auc = np.mean(all_aucs)\n",
    "            max_auc = np.max(all_aucs)\n",
    "            \n",
    "            print(f\"PERFORMANCE OVERVIEW:\")\n",
    "            print(f\"   Total algorithm-task combinations: {total_tests}\")\n",
    "            print(f\"   Mean AUC across all tests: {mean_auc:.3f}\")\n",
    "            print(f\"   Best AUC achieved: {max_auc:.3f}\")\n",
    "            print(f\"   Excellent performance (AUC >= 0.85): {excellent_tests}/{total_tests} ({excellent_tests/total_tests*100:.1f}%)\")\n",
    "            print(f\"   Good+ performance (AUC >= 0.75): {good_tests+excellent_tests}/{total_tests} ({(good_tests+excellent_tests)/total_tests*100:.1f}%)\")\n",
    "            \n",
    "            # Clinical readiness assessment\n",
    "            if excellent_tests > 0:\n",
    "                print(f\"   CLINICAL DEPLOYMENT: {excellent_tests} combinations ready for validation\")\n",
    "            if max_auc >= 0.90:\n",
    "                print(f\"   OUTSTANDING: Exceptional results achieved\")\n",
    "            elif max_auc >= 0.80:\n",
    "                print(f\"   STRONG: Clinical-grade results achieved\")\n",
    "\n",
    "    def _generate_detailed_results_table(self):\n",
    "        \"\"\"Generate detailed results table\"\"\"\n",
    "        print(f\"\\nDETAILED RESULTS TABLE\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Header\n",
    "        print(f\"{'CNN':<20} {'Task':<25} {'Algorithm':<15} {'AUC':<8} {'Acc':<8} {'Sens':<8} {'Spec':<8} {'Status':<15}\")\n",
    "        print(\"-\" * 120)\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for target_category, category_results in cnn_results.items():\n",
    "                for target_name, target_info in category_results.items():\n",
    "                    task_name = target_info['task_name']\n",
    "                    \n",
    "                    for alg_name, result in target_info['results'].items():\n",
    "                        auc = result['auc_roc']\n",
    "                        acc = result['accuracy']\n",
    "                        sens = result['sensitivity']\n",
    "                        spec = result['specificity']\n",
    "                        \n",
    "                        # Status based on AUC\n",
    "                        if auc >= 0.85:\n",
    "                            status = \"EXCELLENT\"\n",
    "                        elif auc >= 0.75:\n",
    "                            status = \"STRONG\"\n",
    "                        elif auc >= 0.65:\n",
    "                            status = \"GOOD\"\n",
    "                        else:\n",
    "                            status = \"MODERATE\"\n",
    "                        \n",
    "                        print(f\"{cnn_name:<20} {task_name:<25} {alg_name:<15} {auc:<8.3f} {acc:<8.3f} {sens:<8.3f} {spec:<8.3f} {status:<15}\")\n",
    "\n",
    "    def _generate_best_performers_analysis(self):\n",
    "        \"\"\"Generate best performers analysis\"\"\"\n",
    "        print(f\"\\nBEST PERFORMERS BY TASK\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Find best performer for each task across all CNNs\n",
    "        task_best = {}\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for target_category, category_results in cnn_results.items():\n",
    "                for target_name, target_info in category_results.items():\n",
    "                    task_name = target_info['task_name']\n",
    "                    \n",
    "                    if task_name not in task_best:\n",
    "                        task_best[task_name] = {'auc': 0, 'cnn': '', 'algorithm': '', 'result': None}\n",
    "                    \n",
    "                    for alg_name, result in target_info['results'].items():\n",
    "                        if result['auc_roc'] > task_best[task_name]['auc']:\n",
    "                            task_best[task_name] = {\n",
    "                                'auc': result['auc_roc'],\n",
    "                                'cnn': cnn_name,\n",
    "                                'algorithm': alg_name,\n",
    "                                'result': result\n",
    "                            }\n",
    "        \n",
    "        for task_name, best in task_best.items():\n",
    "            auc = best['auc']\n",
    "            status = \"DEPLOYMENT READY\" if auc >= 0.85 else \"PROMISING\" if auc >= 0.75 else \"NEEDS WORK\"\n",
    "            print(f\"{task_name:<30}: {best['cnn']} + {best['algorithm']} (AUC = {auc:.3f}) {status}\")\n",
    "\n",
    "    def _generate_clinical_recommendations(self):\n",
    "        \"\"\"Generate clinical recommendations\"\"\"\n",
    "        print(f\"\\nCLINICAL RECOMMENDATIONS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Algorithm performance ranking\n",
    "        algorithm_stats = {}\n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for target_category, category_results in cnn_results.items():\n",
    "                for target_name, target_info in category_results.items():\n",
    "                    for alg_name, result in target_info['results'].items():\n",
    "                        if alg_name not in algorithm_stats:\n",
    "                            algorithm_stats[alg_name] = []\n",
    "                        algorithm_stats[alg_name].append(result['auc_roc'])\n",
    "        \n",
    "        print(\"ALGORITHM PERFORMANCE RANKING:\")\n",
    "        if algorithm_stats:\n",
    "            for alg_name, aucs in sorted(algorithm_stats.items(), key=lambda x: np.mean(x[1]), reverse=True):\n",
    "                mean_auc = np.mean(aucs)\n",
    "                max_auc = np.max(aucs)\n",
    "                n_tests = len(aucs)\n",
    "                print(f\"   {alg_name}: {mean_auc:.3f} mean AUC, {max_auc:.3f} max AUC ({n_tests} tests)\")\n",
    "        \n",
    "        # CNN performance ranking\n",
    "        cnn_stats = {}\n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            aucs = []\n",
    "            for target_category, category_results in cnn_results.items():\n",
    "                for target_name, target_info in category_results.items():\n",
    "                    for alg_name, result in target_info['results'].items():\n",
    "                        aucs.append(result['auc_roc'])\n",
    "            if aucs:\n",
    "                cnn_stats[cnn_name] = aucs\n",
    "        \n",
    "        print(f\"\\nCNN ARCHITECTURE RANKING:\")\n",
    "        if cnn_stats:\n",
    "            for cnn_name, aucs in sorted(cnn_stats.items(), key=lambda x: np.mean(x[1]), reverse=True):\n",
    "                mean_auc = np.mean(aucs)\n",
    "                max_auc = np.max(aucs)\n",
    "                n_tests = len(aucs)\n",
    "                print(f\"   {cnn_name}: {mean_auc:.3f} mean AUC, {max_auc:.3f} max AUC ({n_tests} tests)\")\n",
    "        \n",
    "        # Implementation recommendations\n",
    "        print(f\"\\nIMPLEMENTATION RECOMMENDATIONS:\")\n",
    "        \n",
    "        best_combinations = []\n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for target_category, category_results in cnn_results.items():\n",
    "                for target_name, target_info in category_results.items():\n",
    "                    task_name = target_info['task_name']\n",
    "                    for alg_name, result in target_info['results'].items():\n",
    "                        if result['auc_roc'] >= 0.80:\n",
    "                            best_combinations.append({\n",
    "                                'cnn': cnn_name,\n",
    "                                'task': task_name,\n",
    "                                'algorithm': alg_name,\n",
    "                                'auc': result['auc_roc']\n",
    "                            })\n",
    "        \n",
    "        best_combinations.sort(key=lambda x: x['auc'], reverse=True)\n",
    "        \n",
    "        if best_combinations:\n",
    "            print(f\"   {len(best_combinations)} CNN-algorithm combinations ready for clinical validation\")\n",
    "            print(f\"   Priority implementation: {best_combinations[0]['task']} using {best_combinations[0]['cnn']} + {best_combinations[0]['algorithm']}\")\n",
    "            print(f\"   Expected performance: {best_combinations[0]['auc']:.1%} discrimination accuracy\")\n",
    "        else:\n",
    "            print(f\"   No combinations reached clinical deployment threshold (AUC >= 0.80)\")\n",
    "            print(f\"   Focus on methodology optimization for best performing approaches\")\n",
    "\n",
    "    def export_results_to_csv(self, filename='neurosurgical_ai_results_256_separate.csv'):\n",
    "        \"\"\"Export results to CSV format\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to export. Run analysis first.\")\n",
    "            return\n",
    "        \n",
    "        rows = []\n",
    "        \n",
    "        for cnn_name, cnn_results in self.results.items():\n",
    "            for target_category, category_results in cnn_results.items():\n",
    "                for target_name, target_info in category_results.items():\n",
    "                    description = target_info['description']\n",
    "                    results = target_info['results']\n",
    "                    n_samples = target_info['n_samples']\n",
    "                    n_features = target_info['n_features']\n",
    "                    \n",
    "                    for algo_name, metrics in results.items():\n",
    "                        row = {\n",
    "                            'CNN_Model': cnn_name,\n",
    "                            'Target_Category': target_category,\n",
    "                            'Target_Name': target_name,\n",
    "                            'Description': description,\n",
    "                            'Algorithm': algo_name,\n",
    "                            'N_Samples': n_samples,\n",
    "                            'N_Features': n_features,\n",
    "                            'AUC_ROC': metrics['auc_roc'],\n",
    "                            'AUC_PR': metrics['auc_pr'],\n",
    "                            'Accuracy': metrics['accuracy'],\n",
    "                            'Balanced_Accuracy': metrics['balanced_accuracy'],\n",
    "                            'Sensitivity': metrics['sensitivity'],\n",
    "                            'Specificity': metrics['specificity'],\n",
    "                            'PPV': metrics['ppv'],\n",
    "                            'NPV': metrics['npv'],\n",
    "                            'F1_Score': metrics['f1_score'],\n",
    "                            'MCC': metrics['mcc'],\n",
    "                            'Kappa': metrics['kappa'],\n",
    "                            'Geometric_Mean': metrics['geometric_mean'],\n",
    "                            'Youden_J': metrics['youden_j']\n",
    "                        }\n",
    "                        \n",
    "                        # Add imbalance information if available\n",
    "                        if metrics.get('test_imbalance'):\n",
    "                            imb = metrics['test_imbalance']\n",
    "                            row.update({\n",
    "                                'Imbalance_Ratio': imb['imbalance_ratio'],\n",
    "                                'Minority_Percentage': imb['minority_percentage'],\n",
    "                                'Imbalance_Severity': imb['severity']\n",
    "                            })\n",
    "                        \n",
    "                        rows.append(row)\n",
    "        \n",
    "        # Create DataFrame and export\n",
    "        results_df = pd.DataFrame(rows)\n",
    "        results_df.to_csv(filename, index=False)\n",
    "        print(f\"Results exported to {filename}\")\n",
    "        return results_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer with custom paths\n",
    "    custom_paths = {\n",
    "        'ConvNext': '/Users/joi263/Documents/MultimodalTabData/data/convnext_data/convnext_cleaned_patient_features_separate_256d.csv',\n",
    "        'ViT': '/Users/joi263/Documents/MultimodalTabData/data/vit_base_data/vit_base_cleaned_patient_features_separate_256d.csv',\n",
    "        'ResNet50_Pretrained': '/Users/joi263/Documents/MultimodalTabData/data/pretrained_resnet50_data/pretrained_resnet50_cleaned_patient_features_separate_256d.csv',\n",
    "        'ResNet50_ImageNet': '/Users/joi263/Documents/MultimodalTabData/data/imagenet_resnet50_data/imagenet_resnet50_cleaned_patient_features_separate_256d.csv',\n",
    "        'EfficientNet': '/Users/joi263/Documents/MultimodalTabData/data/efficientnet_data/efficientnet_cleaned_patient_features_separate_256d.csv'\n",
    "        }\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = NeurosurgicalAIAnalyzer(data_paths=custom_paths)\n",
    "    \n",
    "    # Run analysis\n",
    "    results = analyzer.run_analysis()\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    analyzer.generate_comprehensive_report()\n",
    "    \n",
    "    # Print summary\n",
    "    analyzer.print_summary_results()\n",
    "    \n",
    "    # Export results\n",
    "    analyzer.export_results_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161a256",
   "metadata": {},
   "source": [
    "*working code 3*\n",
    "\n",
    "added cox regression and txt file generation\n",
    "also made the code save the txt and csv files into their appropriate folders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurosurgery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
